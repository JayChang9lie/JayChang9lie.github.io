<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Batch Normalization Layer or Self-Normalization Network</title>
    <url>/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/</url>
    <content><![CDATA[<p>❓ 使用 <strong>自归一化网络</strong> or <strong>Batch Normalization</strong>?</p>
<ol>
<li><p>对比自归一化网络与使用 BN 层的网络在性能上的差异.</p>
</li>
<li><p>对比 BN 层添加在 activation 前 / 后的区别.</p>
<span id="more"></span>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>🔺 针对 Fashion MNIST 数据集, 开展下面的测试.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集 (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist 中数字标签对应的类别名称</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 展示部分训练集实例</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m 行 n 列</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/output_6_0.png" alt="png"></p>
<h1 id="Selu-lecun-normal-构建自归一化网络"><a href="#Selu-lecun-normal-构建自归一化网络" class="headerlink" title="Selu + lecun_normal 构建自归一化网络"></a>Selu + lecun_normal 构建自归一化网络</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型构建</span></span><br><span class="line">model_self_norm = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_self_norm.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_self_norm.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_self_norm.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入特征标准化 (μ=0, σ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_self_norm = model_self_norm.fit(x_train_scaled, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                        validation_data=(x_valid_scaled, y_valid), </span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 12s 6ms/step - loss: 0.5500 - accuracy: 0.8066 - val_loss: 0.4003 - val_accuracy: 0.8604
Epoch 2/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.4220 - accuracy: 0.8532 - val_loss: 0.4499 - val_accuracy: 0.8514
Epoch 3/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3881 - accuracy: 0.8645 - val_loss: 0.3875 - val_accuracy: 0.8640
Epoch 4/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3569 - accuracy: 0.8762 - val_loss: 0.3643 - val_accuracy: 0.8748
Epoch 5/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3400 - accuracy: 0.8797 - val_loss: 0.3468 - val_accuracy: 0.8820
Epoch 6/100
1719/1719 [==============================] - 9s 5ms/step - loss: 0.3196 - accuracy: 0.8894 - val_loss: 0.3662 - val_accuracy: 0.8776
Epoch 7/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3066 - accuracy: 0.8928 - val_loss: 0.3506 - val_accuracy: 0.8782
Epoch 8/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2969 - accuracy: 0.8979 - val_loss: 0.3716 - val_accuracy: 0.8692
Epoch 9/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2764 - accuracy: 0.9018 - val_loss: 0.3561 - val_accuracy: 0.8790
Epoch 10/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2659 - accuracy: 0.9062 - val_loss: 0.3311 - val_accuracy: 0.8884
Epoch 11/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2591 - accuracy: 0.9072 - val_loss: 0.3548 - val_accuracy: 0.8880
Epoch 12/100
1719/1719 [==============================] - 9s 5ms/step - loss: 0.2515 - accuracy: 0.9108 - val_loss: 0.3629 - val_accuracy: 0.8888
Epoch 13/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2436 - accuracy: 0.9137 - val_loss: 0.3941 - val_accuracy: 0.8850
Epoch 14/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2332 - accuracy: 0.9179 - val_loss: 0.3579 - val_accuracy: 0.8834
Epoch 15/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3577 - accuracy: 0.9130 - val_loss: 0.3526 - val_accuracy: 0.8842
Epoch 16/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2254 - accuracy: 0.9193 - val_loss: 0.3466 - val_accuracy: 0.8890
Epoch 17/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2018 - accuracy: 0.9274 - val_loss: 0.3625 - val_accuracy: 0.8858
Epoch 18/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.1970 - accuracy: 0.9298 - val_loss: 0.3882 - val_accuracy: 0.8884
Epoch 19/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2006 - accuracy: 0.9291 - val_loss: 0.3566 - val_accuracy: 0.8986
Epoch 20/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.1947 - accuracy: 0.9314 - val_loss: 0.3617 - val_accuracy: 0.8892
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_self_norm.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_self_norm.evaluate(x_test_scaled, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/output_11_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 3ms/step - loss: 0.3794 - accuracy: 0.8743

[0.3793765604496002, 0.8743000030517578]
</code></pre><h1 id="在模型中使用-Batch-Normalization-Layer"><a href="#在模型中使用-Batch-Normalization-Layer" class="headerlink" title="在模型中使用 Batch Normalization Layer"></a>在模型中使用 Batch Normalization Layer</h1><h2 id="Bach-Normalization-before-Activation"><a href="#Bach-Normalization-before-Activation" class="headerlink" title="Bach Normalization before Activation"></a>Bach Normalization before Activation</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型构建</span></span><br><span class="line">model_with_BN_1 = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, use_bias=<span class="literal">False</span>),    <span class="comment"># 注意这个 use_bias</span></span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    keras.layers.Activation(<span class="string">&#x27;elu&#x27;</span>),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, use_bias=<span class="literal">False</span>),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    keras.layers.Activation(<span class="string">&#x27;elu&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_BN_1.add(keras.layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, use_bias=<span class="literal">False</span>))</span><br><span class="line">    model_with_BN_1.add(keras.layers.BatchNormalization())</span><br><span class="line">    model_with_BN_1.add(keras.layers.Activation(<span class="string">&#x27;elu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model_with_BN_1.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_BN_1.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_with_BN_1 = model_with_BN_1.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                        validation_data=(x_valid, y_valid), </span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 30s 16ms/step - loss: 0.5438 - accuracy: 0.8053 - val_loss: 0.3970 - val_accuracy: 0.8510
Epoch 2/100
1719/1719 [==============================] - 28s 16ms/step - loss: 0.4381 - accuracy: 0.8407 - val_loss: 0.4060 - val_accuracy: 0.8542
Epoch 3/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.4067 - accuracy: 0.8514 - val_loss: 0.3232 - val_accuracy: 0.8804
Epoch 4/100
1719/1719 [==============================] - 28s 16ms/step - loss: 0.3749 - accuracy: 0.8631 - val_loss: 0.3409 - val_accuracy: 0.8736
Epoch 5/100
1719/1719 [==============================] - 27s 16ms/step - loss: 0.3516 - accuracy: 0.8704 - val_loss: 0.3216 - val_accuracy: 0.8800
Epoch 6/100
1719/1719 [==============================] - 27s 16ms/step - loss: 0.3404 - accuracy: 0.8761 - val_loss: 0.3147 - val_accuracy: 0.8888
Epoch 7/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.3288 - accuracy: 0.8798 - val_loss: 0.3098 - val_accuracy: 0.8834
Epoch 8/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.3136 - accuracy: 0.8834 - val_loss: 0.3211 - val_accuracy: 0.8826
Epoch 9/100
1719/1719 [==============================] - 30s 17ms/step - loss: 0.3023 - accuracy: 0.8878 - val_loss: 0.2975 - val_accuracy: 0.8892
Epoch 10/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.2896 - accuracy: 0.8927 - val_loss: 0.2917 - val_accuracy: 0.8922
Epoch 11/100
1719/1719 [==============================] - 30s 18ms/step - loss: 0.2821 - accuracy: 0.8952 - val_loss: 0.2928 - val_accuracy: 0.8924
Epoch 12/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.2743 - accuracy: 0.8987 - val_loss: 0.3111 - val_accuracy: 0.8838
Epoch 13/100
1719/1719 [==============================] - 30s 17ms/step - loss: 0.2668 - accuracy: 0.9009 - val_loss: 0.3185 - val_accuracy: 0.8824
Epoch 14/100
1719/1719 [==============================] - 30s 18ms/step - loss: 0.2566 - accuracy: 0.9049 - val_loss: 0.2922 - val_accuracy: 0.8916
Epoch 15/100
1719/1719 [==============================] - 28s 16ms/step - loss: 0.2510 - accuracy: 0.9069 - val_loss: 0.3094 - val_accuracy: 0.8888
Epoch 16/100
1719/1719 [==============================] - 28s 16ms/step - loss: 0.2467 - accuracy: 0.9097 - val_loss: 0.3134 - val_accuracy: 0.8882
Epoch 17/100
1719/1719 [==============================] - 28s 17ms/step - loss: 0.2394 - accuracy: 0.9103 - val_loss: 0.2938 - val_accuracy: 0.8966
Epoch 18/100
1719/1719 [==============================] - 30s 17ms/step - loss: 0.2302 - accuracy: 0.9138 - val_loss: 0.3070 - val_accuracy: 0.8932
Epoch 19/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.2260 - accuracy: 0.9156 - val_loss: 0.3005 - val_accuracy: 0.8936
Epoch 20/100
1719/1719 [==============================] - 30s 17ms/step - loss: 0.2203 - accuracy: 0.9171 - val_loss: 0.3001 - val_accuracy: 0.8912
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_with_BN_1.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_BN_1.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/output_16_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 6ms/step - loss: 0.3210 - accuracy: 0.8871

[0.3209632933139801, 0.8870999813079834]
</code></pre><h2 id="Bach-Normalization-after-Activation"><a href="#Bach-Normalization-after-Activation" class="headerlink" title="Bach Normalization after Activation"></a>Bach Normalization after Activation</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型构建</span></span><br><span class="line">model_with_BN_2 = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_BN_2.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_with_BN_2.add(keras.layers.BatchNormalization())</span><br><span class="line"></span><br><span class="line">model_with_BN_2.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_BN_2.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_with_BN_2 = model_with_BN_2.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                        validation_data=(x_valid, y_valid), </span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 33s 18ms/step - loss: 0.5615 - accuracy: 0.8028 - val_loss: 0.3802 - val_accuracy: 0.8562
Epoch 2/100
1719/1719 [==============================] - 33s 19ms/step - loss: 0.4428 - accuracy: 0.8419 - val_loss: 0.3715 - val_accuracy: 0.8642
Epoch 3/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.4041 - accuracy: 0.8523 - val_loss: 0.3343 - val_accuracy: 0.8738
Epoch 4/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.3682 - accuracy: 0.8662 - val_loss: 0.3374 - val_accuracy: 0.8800
Epoch 5/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.3444 - accuracy: 0.8755 - val_loss: 0.3160 - val_accuracy: 0.8816
Epoch 6/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.3294 - accuracy: 0.8813 - val_loss: 0.3069 - val_accuracy: 0.8904
Epoch 7/100
1719/1719 [==============================] - 33s 19ms/step - loss: 0.3173 - accuracy: 0.8849 - val_loss: 0.3125 - val_accuracy: 0.8866
Epoch 8/100
1719/1719 [==============================] - 32s 18ms/step - loss: 0.3008 - accuracy: 0.8905 - val_loss: 0.3078 - val_accuracy: 0.8880
Epoch 9/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.2882 - accuracy: 0.8941 - val_loss: 0.3044 - val_accuracy: 0.8936
Epoch 10/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2722 - accuracy: 0.8997 - val_loss: 0.2909 - val_accuracy: 0.8932
Epoch 11/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2666 - accuracy: 0.9024 - val_loss: 0.2970 - val_accuracy: 0.8962
Epoch 12/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2571 - accuracy: 0.9056 - val_loss: 0.2990 - val_accuracy: 0.8880
Epoch 13/100
1719/1719 [==============================] - 33s 19ms/step - loss: 0.2473 - accuracy: 0.9099 - val_loss: 0.2929 - val_accuracy: 0.8948
Epoch 14/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.2359 - accuracy: 0.9130 - val_loss: 0.3205 - val_accuracy: 0.8880
Epoch 15/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2314 - accuracy: 0.9144 - val_loss: 0.3113 - val_accuracy: 0.8898
Epoch 16/100
1719/1719 [==============================] - 33s 19ms/step - loss: 0.2254 - accuracy: 0.9165 - val_loss: 0.2950 - val_accuracy: 0.8924
Epoch 17/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2210 - accuracy: 0.9190 - val_loss: 0.2970 - val_accuracy: 0.8972s - loss: 0.2189 - accu
Epoch 18/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2137 - accuracy: 0.9211 - val_loss: 0.3025 - val_accuracy: 0.8932
Epoch 19/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2077 - accuracy: 0.9241 - val_loss: 0.3028 - val_accuracy: 0.8970
Epoch 20/100
1719/1719 [==============================] - 32s 18ms/step - loss: 0.2007 - accuracy: 0.9259 - val_loss: 0.3103 - val_accuracy: 0.8912
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_with_BN_2.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_BN_2.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/output_20_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 7ms/step - loss: 0.3176 - accuracy: 0.8904

[0.317624568939209, 0.8903999924659729]
</code></pre><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><div class="table-container">
<table>
<thead>
<tr>
<th>网络类型</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Evaluation on x_test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Selu + lecun normal 构建の自归一化网络</td>
<td>10s/epoch</td>
<td>10 epochs</td>
<td>accuracy=0.8743</td>
</tr>
<tr>
<td>ELU + he normal (BN 层在激活函数前)</td>
<td>29s/epoch</td>
<td>10 epochs</td>
<td>accuracy=0.8871</td>
</tr>
<tr>
<td>ELU + he normal (BN 层在激活函数后)</td>
<td>32s/epoch</td>
<td>10 epochs</td>
<td>accuracy=0.8904</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>自归一化网络, <strong>训练速度很快</strong>, 收敛速度很快, 模型性能比使用 BN 层的模型稍差.</li>
<li>使用 BN 层的模型, <strong>训练速度显著降低</strong>, 收敛速度很快, 获得的<strong>模型略好于自归一化网络模型</strong>.</li>
<li>BN 层添加在激活函数前 / 后, 区别不太明显.</li>
</ol>
<p>🐒 待改进的点: 可以尝试使用不同的 learning rate</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Better Plots With pairplot and PairGrid</title>
    <url>/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/</url>
    <content><![CDATA[<p>使用 seaborn 绘制美观的 pairplot  和 PairGrid ~</p>
<p>原帖地址: <a href="https://www.pythonheidong.com/blog/article/493964/5fb6bf7e2dd6e667a6a3/">https://www.pythonheidong.com/blog/article/493964/5fb6bf7e2dd6e667a6a3/</a></p>
<p>修改了其中的错误, 并根据理解添加了些许注释.</p>
<span id="more"></span>
<h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 0.common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.加载鸢尾花数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">x, y = iris.data, iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用类别名称作为标签</span></span><br><span class="line">y_1 = np.array([<span class="string">&#x27;setosa&#x27;</span> <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;versicolor&#x27;</span> <span class="keyword">if</span> i==<span class="number">1</span> <span class="keyword">else</span> <span class="string">&#x27;virginica&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> y])</span><br><span class="line"></span><br><span class="line">column_names = [</span><br><span class="line">    <span class="string">&#x27;sepal length(cm)&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;sepal width(cm)&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;petal length(cm)&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;petal width(cm)&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;class&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将特征和标签整理至 DataFrame 中</span></span><br><span class="line">pd_iris = pd.DataFrame(np.hstack([x, y_1.reshape(<span class="number">150</span>, <span class="number">1</span>)]), columns=column_names)</span><br><span class="line"></span><br><span class="line">display(pd_iris.head())</span><br><span class="line">display(pd_iris.info())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改数值特征的 Dtype</span></span><br><span class="line"><span class="keyword">for</span> feature_name <span class="keyword">in</span> column_names[:<span class="number">4</span>]:</span><br><span class="line">    pd_iris[feature_name] = pd_iris[feature_name].astype(np.float64)</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length(cm)</th>
      <th>sepal width(cm)</th>
      <th>petal length(cm)</th>
      <th>petal width(cm)</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>

</div>


<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 150 entries, 0 to 149
Data columns (total 5 columns):
 #   Column            Non-Null Count  Dtype 
---  ------            --------------  ----- 
 0   sepal length(cm)  150 non-null    object
 1   sepal width(cm)   150 non-null    object
 2   petal length(cm)  150 non-null    object
 3   petal width(cm)   150 non-null    object
 4   class             150 non-null    object
dtypes: object(5)
memory usage: 6.0+ KB



None
</code></pre><h1 id="seaborn-pairplot-方法"><a href="#seaborn-pairplot-方法" class="headerlink" title="seaborn.pairplot 方法"></a>seaborn.pairplot 方法</h1><h2 id="基础用法"><a href="#基础用法" class="headerlink" title="基础用法"></a>基础用法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris)    <span class="comment"># 类似 pandas 中的 scatter_matrix()</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)    <span class="comment"># 设置画布大小</span></span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)    <span class="comment"># 网格 + 字体缩放尺度</span></span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_6_0.png" alt="png"></p>
<h2 id="按类别设置数据色调"><a href="#按类别设置数据色调" class="headerlink" title="按类别设置数据色调"></a>按类别设置数据色调</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, </span><br><span class="line">                 hue=<span class="string">&#x27;class&#x27;</span>)    <span class="comment"># 根据 dataframe 中的 class 字段设置 hue(色调)</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_8_0.png" alt="png"></p>
<h2 id="修改调色盘"><a href="#修改调色盘" class="headerlink" title="修改调色盘"></a>修改调色盘</h2><p>可使用 Matplotlib, seaborn, 颜色号 list 等色盘.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># !pip3 install palettable</span></span><br><span class="line"><span class="keyword">import</span> palettable </span><br><span class="line"></span><br><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, </span><br><span class="line">                palette=palettable.cartocolors.qualitative.Bold_9.mpl_colors[<span class="number">3</span>:<span class="number">6</span>])    <span class="comment"># palettable 颜色盘</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_10_0.png" alt="png"></p>
<p><strong>Remark</strong>: 在 pairplot() 中设置 hue 参数后, palette 需要指定同样数量的色彩, 例如上面 hue=’class’, 由于 class 中只有三个类别, 因此 palette=palettable.cartocolors.qualitative.Bold_9.mpl_colors[3:6] 应使用切片选取三种颜色. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>)    <span class="comment"># Matplotlib 颜色</span></span><br><span class="line">                </span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_12_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, </span><br><span class="line">                 palette=[<span class="string">&#x27;#dc2624&#x27;</span>, <span class="string">&#x27;#2b4750&#x27;</span>, <span class="string">&#x27;#45a0a2&#x27;</span>])    <span class="comment"># 使用指定颜色列表</span></span><br><span class="line">                </span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_13_0.png" alt="png"></p>
<h2 id="x-y-轴上指定相同特征进行绘图"><a href="#x-y-轴上指定相同特征进行绘图" class="headerlink" title="x, y 轴上指定相同特征进行绘图"></a>x, y 轴上指定相同特征进行绘图</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 <span class="built_in">vars</span>=[<span class="string">&#x27;sepal length(cm)&#x27;</span>,<span class="string">&#x27;sepal width(cm)&#x27;</span>])    <span class="comment"># 指定要使用的特征</span></span><br><span class="line">                </span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">6</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_15_0.png" alt="png"></p>
<h2 id="x-y-轴上指定不同特征进行绘图"><a href="#x-y-轴上指定不同特征进行绘图" class="headerlink" title="x, y 轴上指定不同特征进行绘图"></a>x, y 轴上指定不同特征进行绘图</h2><p>注意此时没有一个特征对自己的散点图了, 因此对角线上没有 kde 曲线了.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 x_vars=[<span class="string">&#x27;sepal length(cm)&#x27;</span>, <span class="string">&#x27;sepal width(cm)&#x27;</span>],    <span class="comment"># 指定 x 轴上要使用的特征 </span></span><br><span class="line">                 y_vars=[<span class="string">&#x27;petal length(cm)&#x27;</span>, <span class="string">&#x27;petal width(cm)&#x27;</span>])    <span class="comment"># 指定 y 轴上要使用的特征</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">6</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_17_0.png" alt="png"></p>
<h2 id="散点图-回归直线"><a href="#散点图-回归直线" class="headerlink" title="散点图 + 回归直线"></a>散点图 + 回归直线</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 kind=<span class="string">&#x27;reg&#x27;</span>)    <span class="comment"># 在原本的散点图上绘制回归直线                 </span></span><br><span class="line">                </span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_19_0.png" alt="png"></p>
<h2 id="设置对角线上图表类型"><a href="#设置对角线上图表类型" class="headerlink" title="设置对角线上图表类型"></a>设置对角线上图表类型</h2><p>可选参数为: ‘auto’, ‘hist’(默认), ‘kde’, None.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 diag_kind=<span class="string">&#x27;hist&#x27;</span>)    <span class="comment"># 对角线上绘制图表类型: 直方图               </span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_21_0.png" alt="png"></p>
<h2 id="只显示对角线以下的散点图"><a href="#只显示对角线以下的散点图" class="headerlink" title="只显示对角线以下的散点图"></a>只显示对角线以下的散点图</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 corner=<span class="literal">True</span>)    <span class="comment"># 只保留对角线以下的散点图</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_23_0.png" alt="png"></p>
<h2 id="图形外观设置"><a href="#图形外观设置" class="headerlink" title="图形外观设置"></a>图形外观设置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(</span><br><span class="line">    pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">    markers=[<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;h&#x27;</span>],     <span class="comment"># 散点图 (填充型) marker</span></span><br><span class="line">    plot_kws=<span class="built_in">dict</span>(s=<span class="number">50</span>, edgecolor=<span class="string">&quot;k&quot;</span>, linewidth=<span class="number">0.6</span>),    <span class="comment"># 散点图 marker 大小, 外框颜色, 线宽</span></span><br><span class="line">    diag_kws=<span class="built_in">dict</span>(shade=<span class="literal">True</span>)    <span class="comment"># kde 图是否填充</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_25_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(</span><br><span class="line">    pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">    markers=[<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;+&#x27;</span>, <span class="string">&#x27;x&#x27;</span>],     <span class="comment"># 散点图 (线型) marker</span></span><br><span class="line">    plot_kws=<span class="built_in">dict</span>(s=<span class="number">50</span>, linewidth=<span class="number">1.5</span>),    <span class="comment"># 散点图 marker 大小, 线宽</span></span><br><span class="line">    diag_kws=<span class="built_in">dict</span>(shade=<span class="literal">True</span>)    <span class="comment"># kde 图是否填充</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_26_0.png" alt="png"></p>
<p><strong>Remark</strong>: 在设置 pairplot() 中的 markers 参数时, 不允许将 <strong>填充标记</strong> 和 <strong>线标记</strong> 混合使用!</p>
<pre><code>1. 填充标记有: &quot;o&quot;, &quot;v&quot;, &quot;^&quot;, &quot;&lt;&quot;, &quot;&gt;&quot;, &quot;8&quot;, &quot;s&quot;, &quot;p&quot;, &quot;P&quot;, &quot;*&quot;, &quot;h&quot;, &quot;H&quot;, &quot;X&quot;, &quot;D&quot;, &quot;d&quot;;
2. 线标记有: &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;+&quot;, &quot;x&quot;.
</code></pre><h1 id="seaborn-PairGrid-方法"><a href="#seaborn-PairGrid-方法" class="headerlink" title="seaborn.PairGrid 方法"></a>seaborn.PairGrid 方法</h1><h2 id="每个子图均绘制散点图"><a href="#每个子图均绘制散点图" class="headerlink" title="每个子图均绘制散点图"></a>每个子图均绘制散点图</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;husl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">g.<span class="built_in">map</span>(plt.scatter)    <span class="comment"># 设置每个子图均为散点图</span></span><br><span class="line">g.add_legend()        <span class="comment"># 添加图例</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_30_0.png" alt="png"></p>
<h2 id="分别设置对角线上子图类型-amp-非对角线上子图类型"><a href="#分别设置对角线上子图类型-amp-非对角线上子图类型" class="headerlink" title="分别设置对角线上子图类型 &amp; 非对角线上子图类型"></a>分别设置对角线上子图类型 &amp; 非对角线上子图类型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">g.map_diag(plt.hist)          <span class="comment"># 对角线绘制直方图</span></span><br><span class="line">g.map_offdiag(plt.scatter)    <span class="comment"># 非对角线绘制散点图</span></span><br><span class="line">g.add_legend()</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_32_0.png" alt="png"></p>
<h2 id="对角线上方、对角线下方、对角线之上分别指定子图类型"><a href="#对角线上方、对角线下方、对角线之上分别指定子图类型" class="headerlink" title="对角线上方、对角线下方、对角线之上分别指定子图类型"></a>对角线上方、对角线下方、对角线之上分别指定子图类型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>)</span><br><span class="line"></span><br><span class="line">g.map_upper(sns.scatterplot)    <span class="comment"># 对角线以上, 绘制散点图</span></span><br><span class="line">g.map_lower(sns.kdeplot)        <span class="comment"># 对角线以下, 绘制 kde 图</span></span><br><span class="line">g.map_diag(sns.kdeplot, lw=<span class="number">2</span>)   <span class="comment"># 对角线之上, 绘制 kde 图</span></span><br><span class="line">g.add_legend()</span><br><span class="line"></span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_34_0.png" alt="png"></p>
<h2 id="其它一些参数修改"><a href="#其它一些参数修改" class="headerlink" title="其它一些参数修改"></a>其它一些参数修改</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 hue_kws=&#123;<span class="string">&quot;marker&quot;</span>: [<span class="string">&quot;o&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;D&quot;</span>]&#125;,</span><br><span class="line">                 diag_sharey=<span class="literal">True</span>)    <span class="comment"># 是否共享对角线上 kde 图的 y 轴尺度</span></span><br><span class="line"></span><br><span class="line">g.map_upper(sns.scatterplot, edgecolor=<span class="string">&quot;k&quot;</span>, s=<span class="number">45</span>)      <span class="comment"># 设置点大小, 边框颜色</span></span><br><span class="line">g.map_lower(sns.kdeplot)</span><br><span class="line">g.map_diag(sns.kdeplot, lw=<span class="number">2.5</span>)                        <span class="comment"># 设置对角线上 kde 图的线宽</span></span><br><span class="line">g.add_legend()</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_36_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 hue_kws=&#123;<span class="string">&quot;marker&quot;</span>: [<span class="string">&quot;o&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;D&quot;</span>]&#125;,</span><br><span class="line">                 diag_sharey=<span class="literal">False</span>)    <span class="comment"># 不共享可以更&quot;饱满&quot;地展示每条 kde 曲线</span></span><br><span class="line"></span><br><span class="line">g.map_upper(sns.scatterplot, edgecolor=<span class="string">&quot;k&quot;</span>, s=<span class="number">45</span>)</span><br><span class="line">g.map_lower(sns.kdeplot)</span><br><span class="line">g.map_diag(sns.kdeplot, lw=<span class="number">2.5</span>)</span><br><span class="line">g.add_legend()</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_37_0.png" alt="png"></p>
<p><strong>Remark</strong>:</p>
<pre><code>1. diag_sharey 代表在對角線上的圖共用 y 軸唷。預設應該是 True, 所以當變成 False 後, 每個圖會自動調整成比較好的顯示狀態。
2. hue_kws=&#123;&quot;marker&quot;: [&quot;o&quot;, &quot;s&quot;, &quot;D&quot;]&#125; 不起作用是因为对角线上方、对角线下方和对角线之上的子图是分别绘制的, 如果改成单个 g.map(plt.scatter, linewidths=1, edgecolor=&quot;w&quot;, s=40) 则可以看到 marker 的效果.
</code></pre>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Ensemble Learning Methods</title>
    <url>/2023/03/08/Ensemble-Learning-Methods/</url>
    <content><![CDATA[<p><strong>Ensemble Learning Methods</strong></p>
<p><img src="/2023/03/08/Ensemble-Learning-Methods/Ensemble Learning and Random Forests.jpg" style="width: 1200px;" align="center"></p>
<span id="more"></span>
<h1 id="Voting-Classifiers"><a href="#Voting-Classifiers" class="headerlink" title="Voting Classifiers"></a>Voting Classifiers</h1><h2 id="Hard-Voting-Classifier"><a href="#Hard-Voting-Classifier" class="headerlink" title="Hard Voting Classifier"></a>Hard Voting Classifier</h2><ol>
<li>以<strong>得票数最高</strong>的类别最为最终预测类别；<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-2.png" style="width: 300px;" align="center"></li>
</ol>
<hr>
<ol>
<li>Remark：<ol>
<li>理论上，只要有足够多、足够不同的弱学习器，将它们集成在一起就可能获得一个<strong>强学习器</strong></li>
<li><strong>弱学习器</strong>是指比瞎猜强一点儿的学习器</li>
<li>关于 <strong>将弱学习器集成为强学习器</strong> 的理论中有一个<strong>假设</strong>：学习器必须是<strong>独立的</strong>，它们所犯的错误互不相关</li>
<li>当各学习器在同一数据集上训练时，该假设并不成立</li>
<li>当学习器尽可能相互独立时，集成方法的效果最好</li>
<li>使用完全不同的算法训练得到的学习器具有 <strong>多样性</strong>，这能够增加学习器犯不同类型错误的概率</li>
</ol>
</li>
</ol>
<h3 id="Imports"><a href="#Imports" class="headerlink" title="Imports"></a>Imports</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_openml</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> xgboost</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, ExtraTreesClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor, GradientBoostingClassifier</span><br></pre></td></tr></table></figure>
<h3 id="Code-Example-1"><a href="#Code-Example-1" class="headerlink" title="Code Example 1"></a>Code Example 1</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 准备 &amp; 展示数据集</span></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">500</span>, noise=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">42</span>)</span><br><span class="line">data = pd.DataFrame(np.c_[X, y])</span><br><span class="line">data.columns = [<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line">sns.scatterplot(x=<span class="string">&#x27;x&#x27;</span>, y=<span class="string">&#x27;y&#x27;</span>, data=data, hue=<span class="string">&#x27;label&#x27;</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建 &amp; 训练（硬）投票分类器</span></span><br><span class="line">log_clf = LogisticRegression(solver=<span class="string">&quot;lbfgs&quot;</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rnd_clf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line">svm_clf = SVC(gamma=<span class="string">&quot;scale&quot;</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">voting_clf = VotingClassifier(</span><br><span class="line">    estimators=[(<span class="string">&#x27;lr&#x27;</span>, log_clf), (<span class="string">&#x27;rf&#x27;</span>, rnd_clf), (<span class="string">&#x27;svc&#x27;</span>, svm_clf)],</span><br><span class="line">    voting=<span class="string">&#x27;hard&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练 &amp; 评估模型</span></span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;clf.__class__.__name__&#125;</span>: <span class="subst">&#123;<span class="built_in">round</span>(accuracy_score(y_test, y_pred), <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2023/03/08/Ensemble-Learning-Methods/output_8_0.png" alt="png"></p>
<pre><code>LogisticRegression: 0.864
RandomForestClassifier: 0.896
SVC: 0.896
VotingClassifier: 0.912
</code></pre><p>Remark：</p>
<pre><code>  1. 事实上，若将 make_moons() 中的 noise 参数值降低，也有可能出现 SVC 分类准确率最高的情形
  2. 当 N 个学习器中有 1 个的效果明显优于其它学习器时，通过 Voting 得到的学习器未必总优于所有基础学习器
  3. 也就是说，在投票过程中，效果最好的学习器可能会受到其它学习器的 **拖累**
  4. 当学习器效果都差不多时，通常 Voting 得到的学习器效果会更好
</code></pre><h2 id="Soft-Voting-Classifier"><a href="#Soft-Voting-Classifier" class="headerlink" title="Soft Voting Classifier"></a>Soft Voting Classifier</h2><ol>
<li><strong>软投票</strong> 是指在所有学习器预测的<strong>概率</strong>上求平均，然后按最高平均概率给出最终预测的集成方法</li>
</ol>
<hr>
<ol>
<li>Remark：<ol>
<li>使用 soft voting 需确保所有学习器都能预测类别概率</li>
<li>soft voting 通常比 hard voting 具有更好的泛化性能</li>
</ol>
</li>
</ol>
<h3 id="Code-Example-2"><a href="#Code-Example-2" class="headerlink" title="Code Example 2"></a>Code Example 2</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 使用相同的数据集，但是 Soft Voting</span></span><br><span class="line">log_clf = LogisticRegression(solver=<span class="string">&quot;lbfgs&quot;</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rnd_clf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line">svm_clf = SVC(gamma=<span class="string">&quot;scale&quot;</span>, probability=<span class="literal">True</span>, random_state=<span class="number">42</span>)    <span class="comment"># probability=True 可使 SVC 输出类概率</span></span><br><span class="line"></span><br><span class="line">voting_clf = VotingClassifier(</span><br><span class="line">    estimators=[(<span class="string">&#x27;lr&#x27;</span>, log_clf), (<span class="string">&#x27;rf&#x27;</span>, rnd_clf), (<span class="string">&#x27;svc&#x27;</span>, svm_clf)],</span><br><span class="line">    voting=<span class="string">&#x27;soft&#x27;</span>    <span class="comment"># 设置为 Soft Voting</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(clf.__class__.__name__, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure>
<pre><code>LogisticRegression 0.864
RandomForestClassifier 0.896
SVC 0.896
VotingClassifier 0.92
</code></pre><p>Remark：</p>
<pre><code>  1. 从上例发现，Soft Voting 的预测准确率的确高于 Hard Voting
  2. 即使是 Soft Voting 仍可能出现 **被拖累** 的现象
</code></pre><h1 id="Bagging-and-Pasting"><a href="#Bagging-and-Pasting" class="headerlink" title="Bagging and Pasting"></a>Bagging and Pasting</h1><ol>
<li>要使学习器具备<strong>多样性</strong>有两种方法：<ol>
<li>使用不同模型</li>
<li>在不同随机子集上训练（同一种模型）</li>
<li>基于 2 的思路就产生了两种随机子集的<strong>抽样</strong>方法：<ol>
<li><strong>有放回</strong> —— <strong>Bagging</strong> </li>
<li><strong>无放回</strong> —— <strong>Pasting</strong><br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-4.png" style="width: 360px;" align="center"></li>
</ol>
</li>
</ol>
</li>
</ol>
<ol>
<li>Bagging 和 Pasting 模型的<strong>推断</strong>：<ol>
<li>分类问题：投票法</li>
<li>回归问题：加权平均</li>
<li><span class="mark">Remark</span>：在 BaggingClassifier 的推断中，有两个方法：<code>predict</code> 和 <code>predict_proba</code>：<ol>
<li>前者默认使用 Soft Voting 预测类别，但当基础分类器不支持类概率型输出时则使用 Hard Voting</li>
<li>后者默认使用 Soft Voting 预测类概率，但当基础分类器不支持类概率型输出时则使用 Hard Voting 并最终输出 <strong>预测类别频率</strong> 作为概率的近似</li>
</ol>
</li>
</ol>
</li>
</ol>
<hr>
<ol>
<li>Remark：<ol>
<li>以 bagging or pasting 方法得到的学习器  <strong>VS</strong>  在原始训练集上的单个学习器：<ol>
<li>两者 Bias 相近 </li>
<li>前者 Variance 更小  </li>
<li>前者可<strong>并行地</strong>训练和推断，因此规模易于拓展</li>
</ol>
</li>
<li>Bagging  <strong>VS</strong>  Pasting:<ol>
<li>从 bias 角度：bagging 略大于 pasting</li>
<li>从 variance 角度：bagging 略小于 pasting</li>
<li>通常 bagging 模型泛化性能更好</li>
<li>算力充足时，可通过交叉验证来确定 bagging 和 pasting 哪个更适合当前问题</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="Code-Example-3"><a href="#Code-Example-3" class="headerlink" title="Code Example 3"></a>Code Example 3</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.Bagging 模型的构建、训练和预测</span></span><br><span class="line">bag_clf = BaggingClassifier(</span><br><span class="line">    base_estimator=DecisionTreeClassifier(), </span><br><span class="line">    n_estimators=<span class="number">500</span>,</span><br><span class="line">    max_samples=<span class="number">100</span>, </span><br><span class="line">    bootstrap=<span class="literal">True</span>,    <span class="comment"># bootstrap=True 为 Bagging，否则为 Pasting </span></span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">bag_clf.fit(X_train, y_train)</span><br><span class="line">y_pred = bag_clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Bagging Classifier: <span class="subst">&#123;accuracy_score(y_test, y_pred)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.单个决策树模型的构建、训练和预测</span></span><br><span class="line">tree_clf = DecisionTreeClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">tree_clf.fit(X_train, y_train)</span><br><span class="line">y_pred_tree = tree_clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tree Classifier: <span class="subst">&#123;accuracy_score(y_test, y_pred_tree)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Bagging Classifier: 0.904
Tree Classifier: 0.856
</code></pre><p>Remark：</p>
<pre><code>  1. 从上例得到的**启示**——集成模型与单个模型相比：
     1. 偏差相近（训练集中的 errors 数差不多，&lt;span class=&quot;burk&quot;&gt;但说实话我觉得挺多的--!&lt;/span&gt;）
     2. 方差更小（决策边界更平滑，泛化性能可能更好）
        &lt;img src=&quot;./Fig7-5.png&quot;  style=&quot;width: 600px;&quot;  align=&quot;center&quot;/&gt;
</code></pre><h2 id="Out-of-Bag-Evaluation"><a href="#Out-of-Bag-Evaluation" class="headerlink" title="Out-of-Bag Evaluation"></a>Out-of-Bag Evaluation</h2><ol>
<li>由于 bagging 的抽样机制，无需额外划分验证集，即可在 <strong>包外实例</strong> 上评估模型</li>
</ol>
<h3 id="Code-Example-4"><a href="#Code-Example-4" class="headerlink" title="Code Example 4"></a>Code Example 4</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.带包外评估的 Bagging 模型</span></span><br><span class="line">bag_clf = BaggingClassifier(</span><br><span class="line">    base_estimator=DecisionTreeClassifier(), </span><br><span class="line">    n_estimators=<span class="number">500</span>,</span><br><span class="line">    bootstrap=<span class="literal">True</span>, </span><br><span class="line">    oob_score=<span class="literal">True</span>,    <span class="comment"># 开启包外评估（注意：该参数在 bootstrap=False（即 pasting）时不可设为 False）</span></span><br><span class="line">    random_state=<span class="number">40</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">bag_clf.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;包外评估准确率: <span class="subst">&#123;<span class="built_in">round</span>(bag_clf.oob_score_, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;测试集准确率: <span class="subst">&#123;accuracy_score(y_test, bag_clf.predict(X_test))&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># bag_clf.oob_decision_function_</span></span><br></pre></td></tr></table></figure>
<pre><code>包外评估准确率: 0.8987
测试集准确率: 0.912
</code></pre><p>Remark：</p>
<pre><code>  1. `oob_decision_function_`并非一个函数，它只是存储了学习器在包外实例上的预测结果
  2. 当基学习器可预测类概率时，`oob_decision_function_`的返回结果也将是每个**训练**实例的类概率
</code></pre><h2 id="Random-Patches-and-Random-Subspaces"><a href="#Random-Patches-and-Random-Subspaces" class="headerlink" title="Random Patches and Random Subspaces"></a>Random Patches and Random Subspaces</h2><ol>
<li>BaggingClassifier 还支持<strong>对特征的抽样</strong>，此抽样由两个参数控制：max_features &amp; bootstrap_features </li>
<li>这种抽样对特征数庞大的数据集（例如图像）非常有用，它进一步增加了基模型的多样性，代价则是一点儿 bias</li>
<li>不同抽样策略形成不同的集成学习方法：<ol>
<li><strong>随机补丁</strong>：同时对样本和特征抽样</li>
<li><strong>随机子空间</strong>：仅对特征抽样</li>
</ol>
</li>
</ol>
<h2 id="Random-Forests"><a href="#Random-Forests" class="headerlink" title="Random Forests"></a>Random Forests</h2><ol>
<li>不必通过 BaggingClassifier 创建随机森林分类器，直接使用 RandomForestClassifier 更方便</li>
<li>RandomForestClassifier 对决策树专门优化过</li>
<li>RandomForest 通常采用 bagging 策略，且它默认使用完整训练集</li>
<li>随机森林引进了<strong>额外随机性</strong>：在分裂节点时，选取一个随机特征子集，并从中寻找最佳特征以划分节点</li>
<li>上述做法还是用稍高的 Bias 换取更低的 Variance，通常模型效果也更好</li>
</ol>
<h3 id="Code-Example-5-amp-6"><a href="#Code-Example-5-amp-6" class="headerlink" title="Code Example 5 &amp; 6"></a>Code Example 5 &amp; 6</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.构建并训练一个随机森林分类器</span></span><br><span class="line">rnd_clf = RandomForestClassifier(</span><br><span class="line">    n_estimators=<span class="number">500</span>, </span><br><span class="line">    max_leaf_nodes=<span class="number">16</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">rnd_clf.fit(X_train, y_train)</span><br><span class="line">y_pred_rf = rnd_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.构建并训练了一个等价的 Bagging 模型</span></span><br><span class="line">bag_clf = BaggingClassifier(</span><br><span class="line">    DecisionTreeClassifier(</span><br><span class="line">        max_features=<span class="string">&quot;sqrt&quot;</span>, </span><br><span class="line">        max_leaf_nodes=<span class="number">16</span></span><br><span class="line">    ),</span><br><span class="line">    n_estimators=<span class="number">500</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">bag_clf.fit(X_train, y_train)</span><br><span class="line">y_pred = bag_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.对比上面两个模型的效果，发现预测结果几乎完全一致</span></span><br><span class="line">np.<span class="built_in">sum</span>(y_pred == y_pred_rf) / <span class="built_in">len</span>(y_pred)</span><br></pre></td></tr></table></figure>
<pre><code>1.0
</code></pre><h2 id="Extra-Trees"><a href="#Extra-Trees" class="headerlink" title="Extra-Trees"></a>Extra-Trees</h2><ol>
<li><strong>Extra-Trees</strong>：不搜索最佳分裂阈值，而采取<strong>随机分裂</strong>的方式，为决策树添加更多随机性</li>
<li>上述策略仍是以稍高的 bias 换取更低的 variance</li>
<li>由于节点分裂方式是随机的，因此 Extra-Trees 的<strong>训练速度更快</strong>（与 Random Forest 相比）</li>
<li>通常无法预见随机森林和极端随机树谁的性能更好，只有实际尝试两种模型并通过交叉验证搜索最佳超参数组合，才能最终确定更适合当前任务的模型</li>
</ol>
<h2 id="Feature-Importance"><a href="#Feature-Importance" class="headerlink" title="Feature Importance"></a>Feature Importance</h2><ol>
<li>随机森林的一个优点：可方便地度量<strong>特征的相对重要性</strong></li>
<li>所有特征的重要性之和等于 1</li>
<li>注：<strong>排列重要性</strong> 和 <strong>SHAP</strong> 能更好地对特征重要性做出分析</li>
</ol>
<h3 id="Code-Example-7"><a href="#Code-Example-7" class="headerlink" title="Code Example 7"></a>Code Example 7</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.基于鸢尾花数据集构建随机森林分类器</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">rnd_clf = RandomForestClassifier(</span><br><span class="line">    n_estimators=<span class="number">500</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">rnd_clf.fit(iris[<span class="string">&quot;data&quot;</span>], iris[<span class="string">&quot;target&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.特征重要性</span></span><br><span class="line"><span class="keyword">for</span> name, score <span class="keyword">in</span> <span class="built_in">zip</span>(iris[<span class="string">&quot;feature_names&quot;</span>], rnd_clf.feature_importances_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> 的特征重要性分数：<span class="subst">&#123;<span class="built_in">round</span>(score, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>sepal length (cm) 的特征重要性分数：0.1125
sepal width (cm) 的特征重要性分数：0.0231
petal length (cm) 的特征重要性分数：0.441
petal width (cm) 的特征重要性分数：0.4234
</code></pre><h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><ol>
<li>Boosting 的核心想法：<strong>序列式</strong>地训练一系列预测器，每一个都尝试修正前一个预测器所犯错误</li>
<li>说白了就是不断逼近目标值 or 不断修正误差的方法</li>
<li>目前最主流的两个 Boosting 方法： <ol>
<li>AdaBoost</li>
<li>Gradient Boosting</li>
</ol>
</li>
</ol>
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><ol>
<li><strong>核心思想</strong>（以分类器角度举例）：<ol>
<li>对分类器  <strong>误分类的实例</strong>  增加权重</li>
<li>在更新权重后的训练集上训练下一个分类器</li>
<li>不断迭代（训练、预测、更新权重、训练、预测、更新权重、……），从而新分类器将更关注那些  <strong>较难分类的实例</strong><br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-7.png" style="width: 400px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li>不同<strong>学习率</strong>对 AdaBoost 决策边界的影响：<ol>
<li>较大的学习率：决策边界变动剧烈</li>
<li>较小的学习率：决策边界变动缓和<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-8.png" style="width: 500px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li><strong>推断方式</strong>：<ol>
<li>分类问题：<strong>类似投票法</strong></li>
<li>回归问题：加权平均</li>
<li>AdaBoost 使用所有学习器对新实例进行预测，然后按<strong>每个学习器的权重</strong>对预测结果进行加权，即得最终预测结果</li>
<li><span class="mark">Remark</span>：分类问题的推断方式不再区分 Hard / Soft Voting，而是在 <code>AdaBoostClassifier</code> 中给出了两种方法：<code>predict</code> 和 <code>predict_proba</code></li>
</ol>
</li>
</ol>
<ol>
<li>Remark：<ol>
<li>由于 AdaBoost 的<strong>序列式训练方式</strong>，它难以被<strong>并行化</strong>处理</li>
<li>AdaBoost <strong>训练停止条件</strong>：<ol>
<li>学习器个数达到上限</li>
<li>得到了完美的学习器</li>
</ol>
</li>
<li>防止过拟合の措施：<ol>
<li>减少基学习器数量</li>
<li>使用带强正则化的基学习器</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="Code-Example-8"><a href="#Code-Example-8" class="headerlink" title="Code Example 8"></a>Code Example 8</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.构建并训练 AdaBoost 分类器</span></span><br><span class="line">ada_clf = AdaBoostClassifier(</span><br><span class="line">    base_estimator=DecisionTreeClassifier(max_depth=<span class="number">1</span>), </span><br><span class="line">    n_estimators=<span class="number">200</span>,</span><br><span class="line">    algorithm=<span class="string">&quot;SAMME.R&quot;</span>, </span><br><span class="line">    learning_rate=<span class="number">0.5</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">ada_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p><style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable<strong>label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable</strong>label-arrow:before {content: “▸”;float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable<strong>label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable</strong>label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable<strong>content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable</strong>content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable<strong>control:checked~div.sk-toggleable</strong>content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label-arrow:before {content: “▾”;}#sk-container-id-1 div.sk-estimator input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden—visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: “”;width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: “”;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: “”;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/<em> jupyter’s <code>normalize.less</code> sets <code>[hidden] &#123; display: none; &#125;</code> but bootstrap.min.css set <code>[hidden] &#123; display: none !important; &#125;</code> so we also need the <code>!important</code> here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: <a href="https://github.com/scikit-learn/scikit-learn/issues/21755">https://github.com/scikit-learn/scikit-learn/issues/21755</a> </em>/display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),</pre></div></div></p>
<pre><code>               learning_rate=0.5, n_estimators=200, random_state=42)&lt;/pre&gt;&lt;b&gt;In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. &lt;br /&gt;On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;sk-container&quot; hidden&gt;&lt;div class=&quot;sk-item sk-dashed-wrapped&quot;&gt;&lt;div class=&quot;sk-label-container&quot;&gt;&lt;div class=&quot;sk-label sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-1&quot; type=&quot;checkbox&quot; &gt;&lt;label for=&quot;sk-estimator-id-1&quot; class=&quot;sk-toggleable__label sk-toggleable__label-arrow&quot;&gt;AdaBoostClassifier&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content&quot;&gt;&lt;pre&gt;AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),
               learning_rate=0.5, n_estimators=200, random_state=42)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;sk-parallel&quot;&gt;&lt;div class=&quot;sk-parallel-item&quot;&gt;&lt;div class=&quot;sk-item&quot;&gt;&lt;div class=&quot;sk-label-container&quot;&gt;&lt;div class=&quot;sk-label sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-2&quot; type=&quot;checkbox&quot; &gt;&lt;label for=&quot;sk-estimator-id-2&quot; class=&quot;sk-toggleable__label sk-toggleable__label-arrow&quot;&gt;base_estimator: DecisionTreeClassifier&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content&quot;&gt;&lt;pre&gt;DecisionTreeClassifier(max_depth=1)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;sk-serial&quot;&gt;&lt;div class=&quot;sk-item&quot;&gt;&lt;div class=&quot;sk-estimator sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-3&quot; type=&quot;checkbox&quot; &gt;&lt;label for=&quot;sk-estimator-id-3&quot; class=&quot;sk-toggleable__label sk-toggleable__label-arrow&quot;&gt;DecisionTreeClassifier&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content&quot;&gt;&lt;pre&gt;DecisionTreeClassifier(max_depth=1)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</code></pre><h2 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h2><ol>
<li><strong>核心思想</strong>：<ol>
<li>Gradient Boosting 也是<strong>序列式</strong>地训练一系列学习器</li>
<li>与 AdaBoost 的区别：Gradient Boosting 中的每个学习器都是对前一个学习器的 <strong>残差</strong> 进行拟合</li>
</ol>
</li>
</ol>
<ol>
<li><strong>推断方法</strong>：在新实例上将每个学习器的预测结果<strong>求和</strong><br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-9.png" style="width: 600px;" align="center"></li>
</ol>
<ol>
<li>Gradient Boosting 中的<strong>学习率</strong>：<ol>
<li>取值很小时，需要更多树来拟合训练集，但泛化性能通常更好</li>
<li>下图比较了树太多 / 太少的情况</li>
<li><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-10.png" style="width: 600px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li>针对设定好学习率的 Gradient Boosting 模型<strong>选取最佳树数量</strong>的方法：<ol>
<li>使用过量的树建模并训练</li>
<li>在验证过程中通过 <strong>提前停止</strong> 找到最佳树数量</li>
<li>Code Example 10</li>
</ol>
</li>
</ol>
<ol>
<li><p>实现提前停止的另一种思路：</p>
<ol>
<li><strong>增量式训练</strong></li>
<li>Code Example 11</li>
</ol>
</li>
<li><p><strong>随机梯度提升</strong>：</p>
<ol>
<li>在 Gradient Boosting 中，使每个学习器基于 <strong>完整训练集的一个随机子集</strong> 拟合残差</li>
<li>通过设置 subsample 超参数实现</li>
<li>好处：<ol>
<li>用高一点儿的偏差换取更低的方差，增加模型泛化性能</li>
<li>加快训练速度</li>
</ol>
</li>
</ol>
</li>
</ol>
<ol>
<li>Remark：<ol>
<li>Gradient Boosting 模型中可自定义损失函数</li>
<li><strong>XGBoost</strong>：Gradient Boosting 的一个优化实现 —— Code Example 12</li>
</ol>
</li>
</ol>
<h3 id="Code-Example-9"><a href="#Code-Example-9" class="headerlink" title="Code Example 9"></a>Code Example 9</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 0.准备数据集</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>) - <span class="number">0.5</span></span><br><span class="line">y = <span class="number">3</span>*X[:, <span class="number">0</span>]**<span class="number">2</span> + <span class="number">0.05</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建并训练一个 GradientBoosting 回归器</span></span><br><span class="line">gbrt = GradientBoostingRegressor(</span><br><span class="line">    max_depth=<span class="number">2</span>, </span><br><span class="line">    n_estimators=<span class="number">3</span>, </span><br><span class="line">    learning_rate=<span class="number">1.0</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">gbrt.fit(X, y)</span><br></pre></td></tr></table></figure>
<p><style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable<strong>label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable</strong>label-arrow:before {content: “▸”;float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable<strong>label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable</strong>label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable<strong>content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable</strong>content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable<strong>control:checked~div.sk-toggleable</strong>content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label-arrow:before {content: “▾”;}#sk-container-id-2 div.sk-estimator input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden—visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: “”;width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: “”;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: “”;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/<em> jupyter’s <code>normalize.less</code> sets <code>[hidden] &#123; display: none; &#125;</code> but bootstrap.min.css set <code>[hidden] &#123; display: none !important; &#125;</code> so we also need the <code>!important</code> here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: <a href="https://github.com/scikit-learn/scikit-learn/issues/21755">https://github.com/scikit-learn/scikit-learn/issues/21755</a> </em>/display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,</pre></div></div></p>
<pre><code>                      random_state=42)&lt;/pre&gt;&lt;b&gt;In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. &lt;br /&gt;On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;sk-container&quot; hidden&gt;&lt;div class=&quot;sk-item&quot;&gt;&lt;div class=&quot;sk-estimator sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-4&quot; type=&quot;checkbox&quot; checked&gt;&lt;label for=&quot;sk-estimator-id-4&quot; class=&quot;sk-toggleable__label sk-toggleable__label-arrow&quot;&gt;GradientBoostingRegressor&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content&quot;&gt;&lt;pre&gt;GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,
                      random_state=42)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</code></pre><h3 id="Code-Example-10"><a href="#Code-Example-10" class="headerlink" title="Code Example 10"></a>Code Example 10</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.设置过量的树来训练</span></span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=<span class="number">49</span>)</span><br><span class="line"></span><br><span class="line">gbrt = GradientBoostingRegressor(</span><br><span class="line">    max_depth=<span class="number">2</span>, </span><br><span class="line">    n_estimators=<span class="number">120</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">gbrt.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.基于验证集寻找最佳树数量</span></span><br><span class="line">errors = [mean_squared_error(y_val, y_pred) <span class="keyword">for</span> y_pred <span class="keyword">in</span> gbrt.staged_predict(X_val)]    <span class="comment"># staged_predict</span></span><br><span class="line">bst_n_estimators = np.argmin(errors) + <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最佳树数量：<span class="subst">&#123;bst_n_estimators&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.以最佳树数量重新建模、训练</span></span><br><span class="line">gbrt_best = GradientBoostingRegressor(</span><br><span class="line">    max_depth=<span class="number">2</span>, </span><br><span class="line">    n_estimators=bst_n_estimators,    <span class="comment"># 最佳树数量</span></span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">gbrt_best.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<pre><code>最佳树数量：56
</code></pre><p><style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable<strong>label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable</strong>label-arrow:before {content: “▸”;float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable<strong>label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable</strong>label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable<strong>content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable</strong>content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable<strong>control:checked~div.sk-toggleable</strong>content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label-arrow:before {content: “▾”;}#sk-container-id-3 div.sk-estimator input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden—visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: “”;width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: “”;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: “”;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/<em> jupyter’s <code>normalize.less</code> sets <code>[hidden] &#123; display: none; &#125;</code> but bootstrap.min.css set <code>[hidden] &#123; display: none !important; &#125;</code> so we also need the <code>!important</code> here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: <a href="https://github.com/scikit-learn/scikit-learn/issues/21755">https://github.com/scikit-learn/scikit-learn/issues/21755</a> </em>/display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" checked><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">GradientBoostingRegressor</label><div class="sk-toggleable__content"><pre>GradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=42)</pre></div>&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</div></div></div></div></p>
<p>Remark：</p>
<pre><code>  1. 验证误差 vs 树数量
  2. 最佳树数量下的 GradientBoosting 模型
     &lt;img src=&quot;./Fig7-11.png&quot;  style=&quot;width: 600px;&quot;  align=&quot;center&quot;/&gt;
</code></pre><h3 id="Code-Example-11"><a href="#Code-Example-11" class="headerlink" title="Code Example 11"></a>Code Example 11</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.构建 GradientBoosting 模型</span></span><br><span class="line">gbrt = GradientBoostingRegressor(</span><br><span class="line">    max_depth=<span class="number">2</span>, </span><br><span class="line">    warm_start=<span class="literal">True</span>,    <span class="comment"># warm_start=True  </span></span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.初始化验证误差 &amp; 每次迭代都训练模型并计算验证误差</span></span><br><span class="line">min_val_error = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">error_going_up = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> n_estimators <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">120</span>):</span><br><span class="line">    gbrt.n_estimators = n_estimators    <span class="comment"># 每次迭代更新树数量</span></span><br><span class="line">    gbrt.fit(X_train, y_train)</span><br><span class="line">    y_pred = gbrt.predict(X_val)</span><br><span class="line">    val_error = mean_squared_error(y_val, y_pred)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3.检验验证误差是否下降 </span></span><br><span class="line">    <span class="keyword">if</span> val_error &lt; min_val_error:</span><br><span class="line">        min_val_error = val_error</span><br><span class="line">        error_going_up = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        error_going_up += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> error_going_up == <span class="number">5</span>:    <span class="comment"># early stopping with patience = 5</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最佳树数量：<span class="subst">&#123;gbrt.n_estimators&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最低验证误差：<span class="subst">&#123;<span class="built_in">round</span>(min_val_error, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>最佳树数量：61
最低验证误差：0.0027
</code></pre><h3 id="Code-Example-12"><a href="#Code-Example-12" class="headerlink" title="Code Example 12"></a>Code Example 12</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.在整个训练集上拟合 XGBoost</span></span><br><span class="line">xgb_reg = xgboost.XGBRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">xgb_reg.fit(X_train, y_train)</span><br><span class="line">y_pred = xgb_reg.predict(X_val)</span><br><span class="line">val_error = mean_squared_error(y_val, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;在整个训练集上拟合 XGBoost 时的验证误差：<span class="subst">&#123;<span class="built_in">round</span>(val_error, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.基于提前停止训练 XGBoost</span></span><br><span class="line">xgb_reg = xgboost.XGBRegressor(</span><br><span class="line">    early_stopping_rounds=<span class="number">2</span>,    <span class="comment"># patience = 2 </span></span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">xgb_reg.fit(</span><br><span class="line">    X_train, y_train,</span><br><span class="line">    eval_set=[(X_val, y_val)],    <span class="comment"># 验证集</span></span><br><span class="line">)</span><br><span class="line">y_pred = xgb_reg.predict(X_val)</span><br><span class="line">val_error = mean_squared_error(y_val, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;基于提前停止拟合 XGBoost 时的验证误差：<span class="subst">&#123;<span class="built_in">round</span>(val_error, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>在整个训练集上拟合 XGBoost 时的验证误差：0.004
[0]    validation_0-rmse:0.22834
[1]    validation_0-rmse:0.16224
[2]    validation_0-rmse:0.11843
[3]    validation_0-rmse:0.08760
[4]    validation_0-rmse:0.06848
[5]    validation_0-rmse:0.05709
[6]    validation_0-rmse:0.05297
[7]    validation_0-rmse:0.05129
[8]    validation_0-rmse:0.05155
基于提前停止拟合 XGBoost 时的验证误差：0.0026
</code></pre><h1 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h1><ol>
<li><strong>核心思想</strong>：<ol>
<li>不再使用简单函数进行聚合（例如 hard voting）</li>
<li>而是训练一个模型来聚合基学习器的预测结果</li>
<li>这是一种<strong>复合函数</strong>，只不过外层函数是由模型拟合得到的</li>
</ol>
</li>
</ol>
<ol>
<li><strong>推断方法</strong>：<ol>
<li>先由基学习器对新实例进行预测</li>
<li><strong>元学习器</strong>再基于这些预测值预测出最终结果</li>
<li>元学习器：即 Stacking 的外层聚合函数<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-12.png" style="width: 500px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li><strong>训练方法</strong>：<ol>
<li>将初始训练集划分为两部分 A 和 B</li>
<li>在子集 A 上训练基学习器</li>
<li>以训练好的基学习器在子集 B 上预测，得到子集 C</li>
<li>在子集 C 上训练元学习器<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-13.png" style="width: 500px;" align="center"><br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-14.png" style="width: 500px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li>更进一步，元学习器可以形成一个中间层：<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-15.png" style="width: 500px;" align="center"></li>
</ol>
<h3 id="Code-Example-13"><a href="#Code-Example-13" class="headerlink" title="Code Example 13"></a>Code Example 13</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 0.读取数据集</span></span><br><span class="line">mnist = fetch_openml(<span class="string">&#x27;mnist_784&#x27;</span>, version=<span class="number">1</span>, as_frame=<span class="literal">False</span>)</span><br><span class="line">mnist.target = mnist.target.astype(np.uint8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.划分训练、验证、测试集</span></span><br><span class="line">X_train_val, X_test, y_train_val, y_test = train_test_split(mnist.data, mnist.target, test_size=<span class="number">10000</span>, random_state=<span class="number">42</span>)</span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=<span class="number">10000</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.5简化数据集</span></span><br><span class="line">pca = PCA(n_components=<span class="number">0.90</span>)</span><br><span class="line">pca.fit_transform(X_train)</span><br><span class="line">pca.transform(X_val)</span><br><span class="line">pca.transform(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;降维后，数据集维数：<span class="subst">&#123;pca.n_components_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.构建不同分类器</span></span><br><span class="line">clf_1 = Pipeline([(<span class="string">&#x27;rdf&#x27;</span>, RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>))])</span><br><span class="line">clf_2 = Pipeline([(<span class="string">&#x27;extra_trees&#x27;</span>, ExtraTreesClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>))])</span><br><span class="line">clf_3 = Pipeline([(<span class="string">&#x27;scaler&#x27;</span>, StandardScaler()), (<span class="string">&#x27;mlp&#x27;</span>, MLPClassifier(random_state=<span class="number">42</span>))])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.训练不同分类器</span></span><br><span class="line">estimators = [clf_1, clf_2, clf_3]</span><br><span class="line"><span class="keyword">for</span> estimator <span class="keyword">in</span> estimators:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training the&quot;</span>, estimator[-<span class="number">1</span>])</span><br><span class="line">    estimator.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 4.验证集上评估各分类器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;每个分类器在验证集上的准确率：<span class="subst">&#123;[estimator.score(X_val, y_val) <span class="keyword">for</span> estimator <span class="keyword">in</span> estimators]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.构建 Voting 分类器</span></span><br><span class="line">voting_clf = VotingClassifier([</span><br><span class="line">    (<span class="string">&quot;random_forest&quot;</span>, clf_1),</span><br><span class="line">    (<span class="string">&quot;extra_trees&quot;</span>, clf_2),</span><br><span class="line">    (<span class="string">&quot;mlp&quot;</span>, clf_3)], </span><br><span class="line">    voting=<span class="string">&#x27;hard&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">voting_clf.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Voting 分类器在验证集上的准确率：<span class="subst">&#123;voting_clf.score(X_val, y_val)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Voting 分类器中每个子分类器在验证集上的准确率：<span class="subst">&#123;[estimator.score(X_val, y_val) <span class="keyword">for</span> estimator <span class="keyword">in</span> voting_clf.estimators_]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>降维后，数据集维数：87
---------------------------------------------------------------
Training the RandomForestClassifier(random_state=42)
Training the ExtraTreesClassifier(random_state=42)
Training the MLPClassifier(random_state=42)
---------------------------------------------------------------
每个分类器在验证集上的准确率：[0.9692, 0.9715, 0.9747]
---------------------------------------------------------------
Voting 分类器在验证集上的准确率：0.9751
Voting 分类器中每个子分类器在验证集上的准确率：[0.9692, 0.9715, 0.9747]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 6.尝试软投票的准确率</span></span><br><span class="line">voting_clf.voting = <span class="string">&quot;soft&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;软投票分类器的验证准确率：<span class="subst">&#123;voting_clf.score(X_val, y_val)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.在测试集上各分类器的准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;软投票分类器在测试集上的准确率：<span class="subst">&#123;voting_clf.score(X_test, y_test)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;各子分类器在测试集上的准确率：<span class="subst">&#123;[estimator.score(X_test, y_test) <span class="keyword">for</span> estimator <span class="keyword">in</span> voting_clf.estimators_]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br></pre></td></tr></table></figure>
<pre><code>软投票分类器的验证准确率：0.9783
---------------------------------------------------------------
软投票分类器在测试集上的准确率：0.9757
各子分类器在测试集上的准确率：[0.9645, 0.9691, 0.9719]
---------------------------------------------------------------
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 8.使用上述子分类器在验证集上进行预测，由此得到一个新训练集</span></span><br><span class="line">X_val_predictions = np.empty(shape=(<span class="built_in">len</span>(X_val), <span class="built_in">len</span>(estimators)), dtype=np.float32)</span><br><span class="line"><span class="keyword">for</span> index, estimator <span class="keyword">in</span> <span class="built_in">enumerate</span>(estimators):</span><br><span class="line">    X_val_predictions[:, index] = estimator.predict(X_val)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.在上述新训练集上，训练元学习器</span></span><br><span class="line">blender_1 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;sclaer&#x27;</span>, StandardScaler()), </span><br><span class="line">    (<span class="string">&#x27;LR&#x27;</span>, LogisticRegression(max_iter=<span class="number">10000</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">blender_2 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;sclaer&#x27;</span>, StandardScaler()), </span><br><span class="line">    (<span class="string">&#x27;SVC&#x27;</span>, SVC())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">blender_3 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;RF&#x27;</span>, RandomForestClassifier(max_depth=<span class="number">5</span>, n_estimators=<span class="number">500</span>, random_state=<span class="number">42</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">blender_4 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;GradientBoosting&#x27;</span>, GradientBoostingClassifier(max_depth=<span class="number">5</span>, n_estimators=<span class="number">500</span>, random_state=<span class="number">42</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">blender_5 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;sclaer&#x27;</span>, StandardScaler()), </span><br><span class="line">    (<span class="string">&#x27;MLP&#x27;</span>, MLPClassifier(hidden_layer_sizes=(<span class="number">30</span>, ), max_iter=<span class="number">10000</span>, random_state=<span class="number">42</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 10.通过交叉验证，选择最好的元学习器</span></span><br><span class="line"><span class="keyword">for</span> blender <span class="keyword">in</span> [blender_1, blender_2, blender_3, blender_4, blender_5]:</span><br><span class="line">    cv_scores = cross_val_score(blender, X_val_predictions, y_val, cv=<span class="number">4</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;🐬<span class="subst">&#123;blender[-<span class="number">1</span>]&#125;</span> 的交叉验证准确率为：<span class="subst">&#123;<span class="built_in">round</span>(cv_scores.mean(), <span class="number">3</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>🐬LogisticRegression(max_iter=10000) 的交叉验证准确率为：0.959
🐬SVC() 的交叉验证准确率为：0.966
🐬RandomForestClassifier(max_depth=5, n_estimators=500, random_state=42) 的交叉验证准确率为：0.976
🐬GradientBoostingClassifier(max_depth=5, n_estimators=500, random_state=42) 的交叉验证准确率为：0.975
🐬MLPClassifier(hidden_layer_sizes=(30,), max_iter=10000, random_state=42) 的交叉验证准确率为：0.972
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 11.拟合最佳 Blender</span></span><br><span class="line">blender_3.fit(X_val_predictions, y_val)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 12.基于最佳 Blender 给出 Stacking 模型的预测</span></span><br><span class="line">X_test_predictions = np.empty((<span class="built_in">len</span>(X_test), <span class="built_in">len</span>(estimators)), dtype=np.float32)</span><br><span class="line"><span class="keyword">for</span> index, estimator <span class="keyword">in</span> <span class="built_in">enumerate</span>(estimators):</span><br><span class="line">    X_test_predictions[:, index] = estimator.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 12.评估 Stacking 模型的预测准确率</span></span><br><span class="line">y_pred = blender_3.predict(X_test_predictions)    <span class="comment"># 表现最好的 blender</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Stacking 模型在测试集上的准确率：<span class="subst">&#123;accuracy_score(y_test, y_pred)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Stacking 中的各基分类器在测试集上的准确率：<span class="subst">&#123;[estimator.score(X_test, y_test) <span class="keyword">for</span> estimator <span class="keyword">in</span> estimators]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Stacking 模型在测试集上的准确率：0.9717
Stacking 中的各基分类器在测试集上的准确率：[0.9645, 0.9691, 0.9719]
</code></pre><p>Remark：</p>
<pre><code>  1. 本例中，Voting 模型在泛化性能上略优于 Stacking 模型
  2. Stacking 模型的泛化性能甚至低于最佳基分类器 (MLPClassifier) 的泛化性能
  3. 数据集划分按照 label 进行分层抽样更合理
  4. 基分类器和元学习器都没有经过超参优化，这导致最终的 Voting &amp; Stacking 模型都未必是最优的
</code></pre>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/01/15/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Neural Style Transfer</title>
    <url>/2022/01/25/Neural-Style-Transfer/</url>
    <content><![CDATA[<p>使用预训练的卷积神经网络将图像风格迁移到你喜欢的图片上~</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">mpl.rcParams[<span class="string">&#x27;axes.grid&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> PIL.Image</span><br><span class="line"><span class="keyword">import</span> IPython.display <span class="keyword">as</span> display</span><br></pre></td></tr></table></figure>
<h2 id="加载并展示-style-image-和-content-image"><a href="#加载并展示-style-image-和-content-image" class="headerlink" title="加载并展示 style image 和 content image"></a>加载并展示 style image 和 content image</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># content image 和 style image 的路径</span></span><br><span class="line">content_path = <span class="string">r&#x27;.\images\content\content_4.jpg&#x27;</span> </span><br><span class="line">style_path = <span class="string">r&#x27;.\images\style\style_1.jpg&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_img</span>(<span class="params">path_to_img</span>):</span></span><br><span class="line">    max_dim = <span class="number">512</span></span><br><span class="line">    img = tf.io.read_file(path_to_img)</span><br><span class="line">    img = tf.image.decode_image(img, channels=<span class="number">3</span>)</span><br><span class="line">    img = tf.image.convert_image_dtype(img, tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取 img 的宽高 &amp; 计算缩放后的 shape</span></span><br><span class="line">    shape = tf.cast(tf.shape(img)[:-<span class="number">1</span>], tf.float32)    </span><br><span class="line">    long_dim = <span class="built_in">max</span>(shape)</span><br><span class="line">    scale = max_dim / long_dim</span><br><span class="line">    new_shape = tf.cast(shape * scale, tf.int32)</span><br><span class="line"></span><br><span class="line">    img = tf.image.resize(img, new_shape)</span><br><span class="line">    img = img[tf.newaxis, :]    <span class="comment"># 还原 batch 维度</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span>(<span class="params">image, title=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(image.shape) &gt; <span class="number">3</span>:</span><br><span class="line">        image = tf.squeeze(image, axis=<span class="number">0</span>)</span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    <span class="keyword">if</span> title:</span><br><span class="line">        plt.title(title)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载图像并展示</span></span><br><span class="line">content_image = load_img(content_path)</span><br><span class="line">style_image = load_img(style_path)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">imshow(content_image, <span class="string">&#x27;Content Image&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">imshow(style_image, <span class="string">&#x27;Style Image&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/25/Neural-Style-Transfer/output_5_0.png" alt="png"></p>
<h2 id="加载预训练的-VGG19-并选择表征-content-和-style-的层"><a href="#加载预训练的-VGG19-并选择表征-content-和-style-的层" class="headerlink" title="加载预训练的 VGG19 并选择表征 content 和 style 的层"></a>加载预训练的 VGG19 并选择表征 content 和 style 的层</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载 VGG19 (不含全连接层)</span></span><br><span class="line">vgg = tf.keras.applications.VGG19(include_top=<span class="literal">False</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示 VGG19 中层的名称</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> vgg.layers:</span><br><span class="line">    <span class="built_in">print</span>(layer.name)</span><br></pre></td></tr></table></figure>
<pre><code>input_1
block1_conv1
block1_conv2
block1_pool
block2_conv1
block2_conv2
block2_pool
block3_conv1
block3_conv2
block3_conv3
block3_conv4
block3_pool
block4_conv1
block4_conv2
block4_conv3
block4_conv4
block4_pool
block5_conv1
block5_conv2
block5_conv3
block5_conv4
block5_pool
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 选取内容层</span></span><br><span class="line">content_layers = [<span class="string">&#x27;block5_conv3&#x27;</span>, <span class="string">&#x27;block5_conv4&#x27;</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取风格层</span></span><br><span class="line">style_layers = [</span><br><span class="line">    <span class="string">&#x27;block1_conv2&#x27;</span>,    </span><br><span class="line">    <span class="string">&#x27;block2_conv2&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;block3_conv2&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;block4_conv1&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;block4_conv2&#x27;</span>, </span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">num_content_layers = <span class="built_in">len</span>(content_layers)</span><br><span class="line">num_style_layers = <span class="built_in">len</span>(style_layers)</span><br></pre></td></tr></table></figure>
<h2 id="获取-style-layers-的输出-进而计算-style"><a href="#获取-style-layers-的输出-进而计算-style" class="headerlink" title="获取 style_layers 的输出, 进而计算 style"></a>获取 style_layers 的输出, 进而计算 style</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_layers</span>(<span class="params">layer_names</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;  </span></span><br><span class="line"><span class="string">        该函数生成一个模型, 以返回 VGG19 的中间层的输出</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载不含全连接层的 VGG19 并设置为不可训练</span></span><br><span class="line">    vgg = tf.keras.applications.VGG19(include_top=<span class="literal">False</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line">    vgg.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据 &lt;层名称&gt; 获取 &lt;层输出&gt;, 并构建模型</span></span><br><span class="line">    outputs = [vgg.get_layer(name).output <span class="keyword">for</span> name <span class="keyword">in</span> layer_names]</span><br><span class="line">    model = tf.keras.Model([vgg.<span class="built_in">input</span>], outputs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取 style_layers 在 style_image 上的输出</span></span><br><span class="line">style_extractor = vgg_layers(style_layers)</span><br><span class="line">style_outputs = style_extractor(style_image * <span class="number">255</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span>(<span class="params">input_tensor</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        该函数用于从 style_layers 上的输出计算出 style</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    result = tf.linalg.einsum(<span class="string">&#x27;bijc,bijd-&gt;bcd&#x27;</span>, input_tensor, input_tensor)</span><br><span class="line">    input_shape = tf.shape(input_tensor)</span><br><span class="line">    num_locations = tf.cast(input_shape[<span class="number">1</span>] * input_shape[<span class="number">2</span>], tf.float32)</span><br><span class="line">    <span class="keyword">return</span> result/(num_locations)</span><br></pre></td></tr></table></figure>
<p>Remark: 图像的 style (如纹理等) 可使用 CNN 中不同层的相互关系表示, 因此可以使用不同层输出的 Gram Matrix 定义 style.</p>
<h2 id="提取-style-和-content"><a href="#提取-style-和-content" class="headerlink" title="提取 style 和 content"></a>提取 style 和 content</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StyleContentModel</span>(<span class="params">tf.keras.models.Model</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        该类用于提取 content(content_layers 的输出) 和 style(用 style_layers 的输出计算得到)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, style_layers, content_layers</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(StyleContentModel, self).__init__()</span><br><span class="line">        <span class="comment"># 创建一个提取 style_layers 和 content_layers 的输出的(不可训练的)模型</span></span><br><span class="line">        self.vgg = vgg_layers(style_layers + content_layers)</span><br><span class="line">        self.style_layers = style_layers</span><br><span class="line">        self.content_layers = content_layers</span><br><span class="line">        self.num_style_layers = <span class="built_in">len</span>(style_layers)</span><br><span class="line">        self.vgg.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="string">&quot;Expects float input in [0,1]&quot;</span></span><br><span class="line">        <span class="comment"># 分别获取 style_layers 和 content_layers 的输出</span></span><br><span class="line">        inputs = inputs * <span class="number">255.0</span></span><br><span class="line">        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)</span><br><span class="line">        outputs = self.vgg(preprocessed_input)</span><br><span class="line">        style_outputs, content_outputs = (outputs[:self.num_style_layers],</span><br><span class="line">                                          outputs[self.num_style_layers:])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据 style_outputs 计算 style</span></span><br><span class="line">        style_outputs = [gram_matrix(style_output)</span><br><span class="line">                         <span class="keyword">for</span> style_output <span class="keyword">in</span> style_outputs]</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># content_layers 到 content 的映射</span></span><br><span class="line">        content_dict = &#123;content_name: value <span class="keyword">for</span> content_name, value</span><br><span class="line">                        <span class="keyword">in</span> <span class="built_in">zip</span>(self.content_layers, content_outputs)&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># style_layers 到 style 的映射</span></span><br><span class="line">        style_dict = &#123;style_name: value <span class="keyword">for</span> style_name, value</span><br><span class="line">                      <span class="keyword">in</span> <span class="built_in">zip</span>(self.style_layers, style_outputs)&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;content&#x27;</span>: content_dict, <span class="string">&#x27;style&#x27;</span>: style_dict&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取 style_image 的 style 和 content_image 的 content</span></span><br><span class="line"><span class="comment"># 这就是 &lt;目标内容&gt; 和 &lt;目标风格&gt;, 用来构造 loss func</span></span><br><span class="line">extractor = StyleContentModel(style_layers, content_layers)</span><br><span class="line"></span><br><span class="line">style_targets = extractor(style_image)[<span class="string">&#x27;style&#x27;</span>]</span><br><span class="line">content_targets = extractor(content_image)[<span class="string">&#x27;content&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="开始风格迁移"><a href="#开始风格迁移" class="headerlink" title="开始风格迁移"></a>开始风格迁移</h2><p>使用 Gradient Descent 将 style_image 的 style 转移到 content_image 上, 同时保留其原本的 content</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 该变量存储要生成的图像</span></span><br><span class="line">image = tf.Variable(content_image)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip_0_1</span>(<span class="params">image</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        该函数保证生成图像的像素强度在 [0, 1] 中</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> tf.clip_by_value(image, clip_value_min=<span class="number">0.0</span>, clip_value_max=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adam 优化器</span></span><br><span class="line">opt = tf.optimizers.Adam(learning_rate=<span class="number">0.02</span>, beta_1=<span class="number">0.99</span>, epsilon=<span class="number">1e-1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置 style loss &amp; content loss 的权重</span></span><br><span class="line">style_weight = <span class="number">1e-2</span></span><br><span class="line">content_weight = <span class="number">1e4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 total_variation_loss 的权重</span></span><br><span class="line">total_variation_weight = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_content_loss</span>(<span class="params">outputs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        定义完整的 loss func</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 生成图像的 style 和 content</span></span><br><span class="line">    style_outputs = outputs[<span class="string">&#x27;style&#x27;</span>]</span><br><span class="line">    content_outputs = outputs[<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># style loss = mean((current_style - target_style)^2) * corresponding_weight</span></span><br><span class="line">    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name])**<span class="number">2</span>) </span><br><span class="line">                           <span class="keyword">for</span> name <span class="keyword">in</span> style_outputs.keys()])</span><br><span class="line">    style_loss *= style_weight / num_style_layers</span><br><span class="line"></span><br><span class="line">    <span class="comment"># content loss = mean((current_content - target_content)^2) * corresponding_weight</span></span><br><span class="line">    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_targets[name])**<span class="number">2</span>) </span><br><span class="line">                             <span class="keyword">for</span> name <span class="keyword">in</span> content_outputs.keys()])</span><br><span class="line">    content_loss *= content_weight / num_content_layers</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># actual loss </span></span><br><span class="line">    loss = style_loss + content_loss</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">image</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        该函数执行 Gradient Descent  </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        outputs = extractor(image)</span><br><span class="line">        loss = style_content_loss(outputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加一个正则化损失项, 以降低&quot;高频伪影&quot;</span></span><br><span class="line">        loss += total_variation_weight * tf.image.total_variation(image)</span><br><span class="line"></span><br><span class="line">    grad = tape.gradient(loss, image)</span><br><span class="line">    opt.apply_gradients([(grad, image)])</span><br><span class="line">    image.assign(clip_0_1(image))</span><br></pre></td></tr></table></figure>
<p>Remark: The total variation is the sum of the absolute differences for neighboring pixel-values in the input images. This measures how much noise is in the images.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensor_to_image</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        从 tensor 生成 PIL.Image</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tensor = tensor * <span class="number">255</span></span><br><span class="line">    tensor = np.array(tensor, dtype=np.uint8)</span><br><span class="line">    <span class="keyword">if</span> np.ndim(tensor) &gt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">assert</span> tensor.shape[<span class="number">0</span>] == <span class="number">1</span></span><br><span class="line">        tensor = tensor[<span class="number">0</span>]    <span class="comment"># 消除 batch 维度</span></span><br><span class="line">    <span class="keyword">return</span> PIL.Image.fromarray(tensor)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 结合上述内容即可开始风格迁移</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">steps_per_epoch = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(steps_per_epoch):</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line">        train_step(image)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;.&quot;</span>, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line">    display.display(tensor_to_image(image))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Train step: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(step))</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total time: &#123;:.1f&#125;&quot;</span>.<span class="built_in">format</span>(end-start))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/25/Neural-Style-Transfer/output_23_0.png" alt="png"></p>
<pre><code>Train step: 1000
Total time: 192.5
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 效果展示</span></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">18</span>, <span class="number">24</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">imshow(content_image, <span class="string">&#x27;Content Image&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">imshow(style_image, <span class="string">&#x27;Style Image&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">imshow(image, <span class="string">&#x27;Generated Image&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/25/Neural-Style-Transfer/output_24_0.png" alt="png"></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>【笔记】数学分析新讲-第1篇</title>
    <url>/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/</url>
    <content><![CDATA[<h1 align="center" style="font-size:50px">第一篇 &nbsp &nbsp 分析基础</h1>

<span id="more"></span>
<p>[TOC]</p>
<h1 id="❄第零章-预备知识"><a href="#❄第零章-预备知识" class="headerlink" title="❄第零章        预备知识"></a>❄第零章        预备知识</h1><h2 id="●-核心概念-amp-结论"><a href="#●-核心概念-amp-结论" class="headerlink" title="● 核心概念 &amp; 结论"></a>● 核心概念 &amp; 结论</h2><p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Chapter 0 预备知识.jpg" style="zoom:100%;">  </p>
<ol>
<li><p>$ \sum_{k=1}^{n}k = \frac{1}{2} n(n+1) $ </p>
</li>
<li><p>$ \sum_{k=1}^{n}k^2 = \frac{1}{6} n(n+1)(2n+1) $ </p>
</li>
<li><p>$ \sum_{k=1}^{n}k^3 = [\frac{1}{2} n(n+1)]^2 $ </p>
</li>
</ol>
<hr>
<h1 id="❄第一章-实数"><a href="#❄第一章-实数" class="headerlink" title="❄第一章        实数"></a>❄第一章        实数</h1><h2 id="●-核心概念-amp-结论-1"><a href="#●-核心概念-amp-结论-1" class="headerlink" title="● 核心概念 &amp; 结论"></a>● 核心概念 &amp; 结论</h2><p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\Chapter 1 实数.jpg" width="90%"> </p>
<ol>
<li><p><strong>确界原理</strong>： $\R$ 的任意非空有上界的子集在 $\R$ 中有上确界；</p>
</li>
<li><p>常用不等式：</p>
<p>$<br>①\quad \big||a|-|b|\big|\leq|a \pm b|\leq|a|+|b| \\<br>②\quad |a_{1} + a_{2} + \dots + a_{n}| \leq |a_{1}| + |a_{2}| + \dots + |a_{n}| \\<br>③\quad \text{Bernoulli 不等式：} (1 + x)^{n} \geq 1 + nx, \; \forall \; x \geq -1\\<br>④\quad \text{算术平均-几何平均不等式：} \frac{x_{1} + x_{2} + \dots + x_{n}}{n} \geq \sqrt[n]            </p>
<pre><code>&#123;x_&#123;1&#125;x_&#123;2&#125; \dots x_&#123;n&#125;&#125;, \; \forall \; x_&#123;1&#125;, x_&#123;2&#125;, \dots,x_&#123;n&#125; \geq 0 \\
</code></pre><p>⑤\quad \sin x &lt; x &lt; \tan x, \; \forall \; x \in (0, \frac{\pi}{2}) \\<br>⑥\quad |\sin x| \leq |x|, \; \forall \; x \in \R<br>$ </p>
</li>
</ol>
<hr>
<h1 id="❄第二章-极限"><a href="#❄第二章-极限" class="headerlink" title="❄第二章        极限"></a>❄第二章        极限</h1><h2 id="●-核心概念-amp-结论-2"><a href="#●-核心概念-amp-结论-2" class="headerlink" title="● 核心概念 &amp; 结论"></a>● 核心概念 &amp; 结论</h2><p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\Chapter 2 极限.jpg" width="100%">  </p>
<ol>
<li>Relation ①：</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2thm3.png" width="50%" align="left"></p>
<ol>
<li>Relation ②：</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s4thm4.png" width="50%" align="left"></p>
<ol>
<li>Relation ③：</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s6thm1.png" width="50%" align="left"></p>
<ol>
<li>有界序列与无穷小序列：</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1thm1.png" width="50%" align="left"></p>
<ol>
<li>关于 “收敛序列保序性の推论” 的注记：</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2remark.png" width="50%" align="left"></p>
<ol>
<li>关于 “闭区间套原理” 的注记：</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3remark.png" width="50%" align="left"></p>
<h2 id="●-本章の注记"><a href="#●-本章の注记" class="headerlink" title="● 本章の注记"></a>● 本章の注记</h2><ol>
<li>本章介绍了分析基础中最基本的两个概念：<strong>收敛序列</strong> 和 <strong>函数极限</strong> </li>
<li>序列收敛性常由 $\epsilon-N$ 定义描述</li>
<li>函数于一点的收敛性常由 $\epsilon-\delta$ 定义 or 序列式定义描述 </li>
<li>柯西收敛原理仅适用于有穷极限的情况</li>
<li>单调收敛原理可适用于无穷极限的情况</li>
</ol>
<h2 id="●-典型问题解析"><a href="#●-典型问题解析" class="headerlink" title="● 典型问题解析"></a>● 典型问题解析</h2><h3 id="第一节"><a href="#第一节" class="headerlink" title="@ 第一节"></a><strong>@ 第一节</strong></h3><p>1、按定义证明序列有界 / 无界（基于二项展开的不等式缩放、基于分项合并的不等式缩放）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e3.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e5.png" width="400px" align="left"> </p>
<p>2、按 $\epsilon-N$ 定义证明序列为无穷小（基于 Bernoulli 不等式的缩放、基于二项展开的不等式缩放）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e7.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e8.png" width="400px" align="left"> </p>
<p>3、按 Squeeze Theorem 证明序列为无穷小（逐项缩放）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e9.png" width="400px" align="left"> </p>
<p>4、有限个无穷小序列的和与积也是无穷小序列、有界量与无穷小序列的积也是无穷小序列</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e10.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e11.png" width="400px" align="left">  </p>
<p>5、按 $\epsilon-N$ 定义证明序列为无穷小（n 项和的形式，拆为两部分，分别缩放）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e12.png" width="400px" align="left"></p>
<p>6、 按 Squeeze Theorem 证明序列为无穷小（算数平均-几何平均不等式）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e13.png" width="400px" align="left"></p>
<p><strong>Remark</strong>：由上面两个例子，有：以 “由无穷小序列的前 n 项构造的算数平均数、几何平均数” 为通项的序列也是无穷小序列 </p>
<p>7、将 $z_{n}$ 看作无穷小序列 $\{\frac{1}{n}\}$ 的前 n 项所构造的几何平均数 </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s1e14.png" width="400px" align="left"> </p>
<h3 id="第二节"><a href="#第二节" class="headerlink" title="@ 第二节"></a><strong>@ 第二节</strong></h3><p>1、证明 $x_{n}\to x$ <strong>等价于证明</strong> $(x_{n}-x) \to 0$ <strong>等价于证明</strong> $x_{n} = x + \alpha_{n}$，其中 $\alpha_{n}$ 是无穷小序列</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e3.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e4.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e6.png" width="400px" align="left"></p>
<p>2、利用序列极限的四则运算性质计算新序列极限</p>
<p> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e7.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e9.png" width="400px" align="left"></p>
<p><strong>Remark</strong>：</p>
<ol>
<li><p>由例 3 和例 7 知：$\forall \; a &gt; 0, \; \sqrt[n]{a} \to 1, n \to \infty$  </p>
</li>
<li><p>由例 6 知：当 $x_{n} \to x$ 时，$\dfrac{x_{1} + x_{2} + \dots + x_{n}}{n} \to x, \; n \to \infty$ </p>
</li>
<li><p>由下面例 10 知：当正实数序列 $x_{n} \to x(&gt;0)$ 时，$\sqrt[n]{x_{1}x_{2} \dots x_{n}} \to x, \; n \to \infty$ </p>
</li>
</ol>
<p>3、需要对 $c$ 的取值分类讨论，其中当 $c&gt;0$ 时，可通过缩放、上述 Remark 1、Squeeze Theorem 得到结论 </p>
<p> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e8.png" width="400px" align="left"> </p>
<p>4、利用 Squeeze Theorem 证明的序列收敛性、算术平均-几何平均不等式（扩展）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e10_1.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e10_2.png" width="400px" align="left"></p>
<p>5、将 $x_{n} \to x$ 等价转化为 $x_{n} = x + \alpha_{n}$，其中 $\alpha_{n}$ 是无穷小序列后，证明序列收敛的问题得到简化</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e11_1.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e11_2.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e12_1.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e12_2.png" width="400px" align="left"></p>
<p>6、通过比值极限的保序性，推出分子与分母的大小关系</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s2e13.png" width="400px" align="left"></p>
<h3 id="第三节"><a href="#第三节" class="headerlink" title="@ 第三节"></a><strong>@ 第三节</strong></h3><p>1、利用<strong>单调收敛原理</strong>求序列极限的三个例子（递推公式两边取极限，得到关于极限值的方程）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3e1.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3e2_1.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3e2_2.png" width="400px" align="left">  </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3e3_1.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3e3_2.png" width="400px" align="left"></p>
<p>2、重要极限 $(1 + \dfrac{1}{n})^{n} \to e, \; n \to \infty$ 的出处</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3e4.png" width="400PX" align="left"></p>
<p>3、利用<strong>柯西收敛原理</strong>证明序列发散 or 收敛</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3e6.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3e7.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s3e8.png" width="400px" align="left"></p>
<h3 id="第四节"><a href="#第四节" class="headerlink" title="@ 第四节"></a><strong>@ 第四节</strong></h3><p>1、利用 Stolz 定理求 $\dfrac{*}{\infty}$ 型极限</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s4e5.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s4e6.png" width="400px" align="left"></p>
<h3 id="第五节"><a href="#第五节" class="headerlink" title="@ 第五节"></a><strong>@ 第五节</strong></h3><p>1、 按 Heine 序列式定义证明函数（在连续点的）极限</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e3.png" style="width:400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e5.png" style="width:400px" align="left"></p>
<p>2、按  Heine 序列式定义证明函数（在非连续点的）极限</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e6.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e7_1.png" style="width:400px" align="left"></p>
<p> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e7_2.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e8.png" style="width:400px" align="left"></p>
<p>3、<strong>多项式函数</strong> 与 <strong>有理函数</strong> 的极限、经过<strong>换元</strong>得到的有理函数极限</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e9_1.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e9_2.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e10.png" style="width:400px" align="left"></p>
<p>Remark：</p>
<ol>
<li>例 7 为重要极限：$\lim_\limits{x\to0}\dfrac{\sin x}{x}=1$ </li>
<li>例 9 介绍了多项式及有理函数的极限</li>
</ol>
<p>4、利用重要极限计算极限的例子（函数极限的四则运算、$1 - \cos x = 2(\sin(\frac{x}{2}))^2$）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e12.png" style="width:400px" align="left"></p>
<p>5、按 $\epsilon-\delta$ 定义证明函数极限</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e13.png" style="width:400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c2s5e14.png" style="width:400px" align="left"></p>
<h3 id="Final-Remark"><a href="#Final-Remark" class="headerlink" title="Final Remark"></a><strong>Final Remark</strong></h3><p><strong>① 证明序列收敛、求收敛序列极限</strong>  </p>
<ol>
<li><p>$\epsilon-N$ 定义 &amp; 不等式缩放</p>
</li>
<li><p>Squeeze Theorem &amp; 不等式缩放</p>
</li>
<li><p>序列极限の四则运算</p>
</li>
<li><p>$(x_{n} \to x) \Leftrightarrow \; (x_{n} - x \to 0) \Leftrightarrow x_{n} = x + \alpha_{n}$，其中 $\alpha_{n}$ 为无穷小序列</p>
</li>
<li><p>单调收敛原理 &amp; 递推式两边取极限</p>
</li>
<li><p>Cauchy 收敛原理 &amp; 不等式缩放 （只能用于证明序列收敛与否）</p>
</li>
<li><p>Stolz Theorem （$\dfrac{\ast}{+\infty}$ 型极限）</p>
</li>
<li><p>不等式缩放技巧：二项式定理、Bernoulli 不等式、逐项缩放、拆分 N 项和，分别缩放、算术平均-几何平均不等式</p>
</li>
<li><p>一些已证明的结论：</p>
<p>| $\dfrac{1}{a^{n}}, \dfrac{n}{a^{n}} \to 0, \; |a| &gt; 1$       | $\dfrac{n^{k}}{a^{n}} \to 0, \; a &gt; 1, k \in \N$             | $\dfrac{a^{n}}{n!} \to 0, \; a &gt; 0$                          |<br>| :—————————————————————————————- | :—————————————————————————————- | :—————————————————————————————- |<br>| $\dfrac{\sum_{k=1}^{n} \alpha_{k}}{n} \to \alpha \in \R, \; \alpha_{n} \to \alpha$ | $\sqrt[n]{\prod_{k=1}^{n} \alpha_{k}} \to \alpha &gt; 0, \; \alpha_{n} \to \alpha, \; \alpha_{n} &gt; 0$ | $\sqrt[n]{\prod_{k=1}^{n} \alpha_{k}} \to 0, \; \alpha_{n} \to 0, \alpha_{n} \geq 0$ |<br>| $\sqrt[n]{a} \to 1, \; a &gt; 0$                                | $\sqrt[n]{n} \to 1$                                          |                                                              |<br>| $(1 + \dfrac{1}{n})^{n} \to e$                               | $x_{n} = \dfrac{1}{2}(x_{n-1} + \dfrac{a}{x_{n-1}}) \to \sqrt{a}, \; x_{0} &gt; 0, \; a &gt; 0$ |                                                              |</p>
</li>
</ol>
<p><strong>② 对充分大的 n，判别序列通项的大小关系</strong></p>
<ol>
<li>收敛序列保序性（$n^{k} &lt; k^{n} &lt; n!$，自然数 $k \geq 2$）</li>
</ol>
<p><strong>③ 证明函数于某点收敛</strong></p>
<ol>
<li>Heine 序列式定义 &amp; 不等式缩放（&amp; Squeeze Theorem）</li>
<li>$\epsilon-\delta$ 定义 &amp; 不等式缩放</li>
<li>Cauchy 收敛原理</li>
</ol>
<p><strong>④ 计算函数极限</strong></p>
<ol>
<li>四则运算</li>
<li>重要极限（$\lim\limits_{x \to 0}\dfrac{\sin x}{x} = 1$）</li>
<li>多项式、有理函数の极限</li>
</ol>
<hr>
<h1 id="❄第三章-连续函数"><a href="#❄第三章-连续函数" class="headerlink" title="❄第三章        连续函数"></a>❄第三章        连续函数</h1><h2 id="●-核心概念-amp-结论-3"><a href="#●-核心概念-amp-结论-3" class="headerlink" title="● 核心概念 &amp; 结论"></a>● 核心概念 &amp; 结论</h2><p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\Chapter 3 连续函数.jpg" align="center">   </p>
<ol>
<li><p>Relation ①：</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s1thm6.png" width="50%" align="left"></p>
</li>
</ol>
<ol>
<li>Remark：最大值、最小值定理中的最值即为 $f$ 在 $[a, b]$ 中的上、下确界</li>
</ol>
<ol>
<li><p><strong>连续与一致连续的区别</strong>（按 ε-δ 定义）：</p>
<ol>
<li><p>连续：对给定 $\epsilon &gt; 0$，在不同 $x_{0}$ 处相应的 $\delta$ 未必相同</p>
</li>
<li><p>一致连续：对任意 $\epsilon &gt; 0$，存在适用于一切 $x_{0}$ 的 $\delta &gt; 0$，只要 $|x - x_{0}| &lt; \delta$，就有 $|f(x) - f(x_{0})| &lt; \epsilon$ </p>
</li>
</ol>
</li>
</ol>
<ol>
<li><p>① 介值定理</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s2thm2.png" width="50%" align="left"></p>
<p>② 几何式陈述</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s3thm1.png" width="50%" align="left"></p>
<p>③ 上述定理的逆命题一般而言是不成立的……</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s3thm2.png" width="70%" align="left"></p>
</li>
</ol>
<ol>
<li><p>指数函数の定义过程：</p>
<p>​    ① 本章第三节例 5 中定义了<strong>算术根</strong>。</p>
<p>​    ② 基于算术根定义<strong>分数次方幂</strong>如下：</p>
<p>​            设 $m, n \in \N$：</p>
<p>​            Ⅰ、 定义  $a^{\frac{m}{n}}=(\sqrt[n]{a})^m, \; a \geq 0$ </p>
<p>​            Ⅱ、 定义  $a^{-\frac{m}{n}} = \dfrac{1}{a^{\frac{m}{n}}} = \dfrac{1}{(\sqrt[n]{a})^m}, \; a &gt; 0$ </p>
<p>​            Ⅲ、 定义  $a^{0} = 1, \; a &gt; 0$ </p>
<p>​            至此，当 $a &gt; 0$ 时，$\forall r \in \Q, \; a^{r}$ 有定义。</p>
<p>​    ③ 基于分数次方幂（及本章第四节の引理 1 &amp; 引理 2）定义<strong>实数次方幂</strong>如下：</p>
<p>​            设 $a&gt;0, \; \forall x \in \R$，定义 $a^{x} = \lim\limits_{n \to \infty}a^{q_{n}}$，其中 $\{q_{n}\}$ 为收敛于 $x$ 的任意<strong>有理</strong>序列。</p>
<p>​            另有实数次方幂の性质如下：</p>
<p>​            <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s4thm2.png" width="50%" align="center"> </p>
</li>
</ol>
<ol>
<li><p>基本初等函数：</p>
<p>​    ① 幂函数（以及多项式函数） </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_powerx_3.png" width="60%" align="center"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_powerx_4.png" width="60%" align="center"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_powerx_5.png" width="60%" align="center"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_powerx_2.png" width="90%" align="center"></p>
</li>
</ol>
<p>   ​    ② 三角函数、反三角函数</p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_sinx.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_cosx.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_tanx_cotx.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_tanx_cotx_2.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_arcsinx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_arccosx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_arctanx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_arccotx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_secx_cscx.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_secx_cscx_2.png" width="90%" align="center"></p>
<p>   ​    ③ 对数函数、指数函数</p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_expx.png" width="70%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_expx_2.png" width="70%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_logx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\func_logx_2.png" width="30%" align="center"></p>
<ol>
<li><p>符号 $O()、o()、～$ 的定义：</p>
<script type="math/tex; mode=display">
\text{当} \; x \to a \; \text{时}
    \begin{cases}
        f(x) = O\Big(g(x)\Big): & \Big\lvert \frac{f(x)}{g(x)} \Big\lvert \leq M, \; x                     \in \check{U}(a) \\
        f(x) = o\Big(g(x)\Big): & \lim\limits_{x \to a} \frac{f(x)}{g(x)} = 0 \\
        f(x) \; ～ \; g(x): & \lim\limits_{x \to a} \frac{f(x)}{g(x)} = 1 \\ 
    \end{cases}</script></li>
<li><p><strong>重要极限</strong>总结：</p>
<p>​    ①  $\lim\limits_{x \to 0}\dfrac{\sin x}{x} = 1$ </p>
<p>​    ②  $\lim\limits_{x \to \infty} \Big( 1 + \dfrac{1}{x} \Big) ^ x = e \; \Longleftrightarrow \; \lim\limits_{a \to 0} \big( 1 + a \big) ^ \frac{1}{a} = e$ </p>
<p>​    ③  对 ② 右边取对数，得：$\lim\limits_{a \to 0}\dfrac{\ln(1+a)}{a} = 1$ 【类似地有： $\lim\limits_{a \to 0}\dfrac{\log_{b}(1+a)}{a} = \dfrac{1}{\ln b}$】</p>
<p>​    ④  $\lim\limits_{a \to 0}\dfrac{e^{a} - 1}{a} = 1$ 【令 $\beta = e^{a} - 1$ 即可转化为 ③ の倒数形式，类似地有：$\lim\limits_{a \to 0}\dfrac{b^{a} - 1}{a} = \ln b$】</p>
<p>​    ⑤  $\lim \limits_{a \to 0} \dfrac{(1 + a)^{\mu}-1}{a} = \mu$ 【由 $(1+a)^{\mu} = e^{\mu \times \ln (1+a)}$，原极限可化为 ③ 和 ④ 之积の形式】</p>
</li>
</ol>
<h2 id="●-本章の注记-1"><a href="#●-本章の注记-1" class="headerlink" title="● 本章の注记"></a>● 本章の注记</h2><ol>
<li>本章介绍了 <strong>函数于一点の连续性</strong> 以及 <strong>闭区间上连续函数の性质</strong></li>
<li>函数于一点的连续性本质上是通过极限定义的，因而有两种等价定义：<strong>序列式定义</strong> 和 <strong>$\epsilon-\delta$ 定义</strong>；由于这种定义方式，函数在连续点附近是<strong>局部有界</strong>、<strong>局部保序</strong>的，同时<strong>四则运算、函数复合也保持该点的连续性</strong></li>
<li>间断点是对应于连续点的概念，它可由该点处单侧极限的情况区分为<strong>两类间断点</strong></li>
<li>由函数在一点连续（及单侧连续）即可定义函数在一个闭区间上连续，此时函数具有如下<strong>整体性质</strong>：<ul>
<li>零值定理、介值定理</li>
<li>有界性、最值定理</li>
<li>一致连续性</li>
</ul>
</li>
<li>从单调函数出发定义了<strong>反函数</strong>、从算术根出发定义了<strong>指数函数</strong>、进而定义了其反函数——<strong>对数函数</strong>，并介绍了它们的基本性质</li>
<li>指出了<strong>初等函数の连续性</strong></li>
<li>介绍了五类<strong>重要极限</strong></li>
<li>介绍了<strong>等价量</strong> &amp; 它们在函数极限计算中的作用</li>
</ol>
<h2 id="●-典型问题解析-1"><a href="#●-典型问题解析-1" class="headerlink" title="● 典型问题解析"></a>● 典型问题解析</h2><h3 id="第一节-1"><a href="#第一节-1" class="headerlink" title="@ 第一节"></a><strong>@ 第一节</strong></h3><p>1、利用左右极限判断函数の间断点类型（包括特殊の<strong>狄利克雷函数和黎曼函数</strong>）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s1e4e5.png" style="width:400px" align="left"><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s1e6e7.png" style="width:400px" align="left"></p>
<h3 id="第二节-1"><a href="#第二节-1" class="headerlink" title="@ 第二节"></a><strong>@ 第二节</strong></h3><p>1、零值定理の应用：<strong>Brouwer 不动点定理 &amp; 对分区间法求方程根的近似值</strong> </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s2e1.png" style="width:400px" align="left"><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s2e2.png" style="width:400px" align="left"></p>
<p>2、从定义出发判断一致连续性</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s2e5e6.png" style="width:400px" align="left"> </p>
<p>3、利用一致连续性の等价条件（序列式描述）证明函数不一致连续</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s2e7.png" style="width:400px" align="left"> </p>
<h3 id="第三节-1"><a href="#第三节-1" class="headerlink" title="@ 第三节"></a><strong>@ 第三节</strong></h3><p>1、 虽然连续函数可将区间映射为区间，但将区间映射为区间的未必是连续函数</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s3e1.png" style="width:400px" align="left"></p>
<p>2、算术根の定义与存在唯一性</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s3e5.png" style="width:400px" align="left"></p>
<h3 id="第四节-1"><a href="#第四节-1" class="headerlink" title="@ 第四节"></a><strong>@ 第四节</strong></h3><p>1、<strong>幂函数</strong>与<strong>幂指函数</strong>的连续性（将他们视为连续函数的复合）</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s4e1e2.png" style="width:400px" align="left"></p>
<h3 id="第五节-1"><a href="#第五节-1" class="headerlink" title="@ 第五节"></a><strong>@ 第五节</strong></h3><p>1、 $(x-a)^{k} \;\; (x \to a)$ 型无穷小、$\dfrac{1}{(x-a)^{k}} \;\; (x \to a)$ 型无穷大、$x^{k} \;\; (x \to \infty)$ 型无穷大</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s5e6e7e8.png" style="width:400px" align="left"></p>
<p>2、$x \to +\infty$ 时，无穷大の阶<strong>降序排列</strong>为：$a^{x}$、$x^{\mu}$、$\log_{a}x$，其中 $a &gt; 1, \mu &gt; 0$  </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s5e9.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s5e10.png" style="width:400px" align="left"></p>
<p>3、利用等价因式替换求函数极限</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\学习笔记-Typora\c3s5e11e12e13e14.png" style="width:400px" align="left"> </p>
<h3 id="Final-Remark-1"><a href="#Final-Remark-1" class="headerlink" title="Final Remark"></a><strong>Final Remark</strong></h3><p><strong>① 函数间断点（及其类型）の判定</strong> </p>
<ol>
<li>$f(a-), \; f(a+)$ の存在性、取值 &amp; $f(a)$ の取值</li>
</ol>
<p><strong>② 求方程根の近似值</strong></p>
<ol>
<li>区间对分法（理论依据为零值定理）</li>
</ol>
<p><strong>③ 判断函数の一致连续性</strong></p>
<ol>
<li>$\epsilon-\delta$ 定义</li>
<li>一致连续性の等价条件（序列式描述）常用于证明函数不一致连续</li>
</ol>
<p><strong>④ 判定高阶无穷小（大）</strong></p>
<ol>
<li>极限定义</li>
</ol>
<p><strong>⑤ 求函数极限</strong></p>
<ol>
<li>等价因式替换</li>
</ol>
<hr>
<h1 id="☀第一篇の总结"><a href="#☀第一篇の总结" class="headerlink" title="☀第一篇の总结"></a>☀第一篇の总结</h1><p>本篇介绍了数学分析中一些<strong>最基本的概念</strong>：</p>
<ol>
<li>集合 &amp; 映射、实数系 $\R$；</li>
<li>序列极限 &amp; 函数极限 の 概念与基本性质；</li>
<li>函数在一点的连续性 &amp; 局部性质、单侧连续 &amp; 间断点；</li>
<li>函数在闭区间上的连续性 &amp; <strong>整体性质</strong>、初等函数的连续性；</li>
</ol>
<p>同时也介绍了许多常用不等式和重要极限。</p>
<p>涉及的<strong>典型问题</strong>包括：</p>
<ol>
<li>序列收敛性 &amp; 极限计算；</li>
<li>函数收敛性 &amp; 极限计算；</li>
<li>函数间断点的判定、一致连续性的判定</li>
</ol>
<p>相应例题和总结可在第二、三章的典型问题解析中找到。</p>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>Mathematics</tag>
        <tag>Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>变分自编码器 (Variational Autoencoder)</title>
    <url>/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/</url>
    <content><![CDATA[<p><strong>Variational Autoencoder</strong>:</p>
<ol>
<li><p>是一种概率自编码器, 其输出有一定的随机性;</p>
</li>
<li><p>是一种生成式自编码器, 能够生成类似训练集中数据的新实例.</p>
<span id="more"></span>
</li>
</ol>
<hr>
<p>与常规自编码器不同, 变分自编码器不直接对给定 input 进行编码, 而是使 encoder 生成均值 $μ$ 和标准差 $σ$, 然后从对应高斯分布 $N(μ, σ^2)$ 中随机采样得到 input 对应的编码, 然后 decoder 再对此编码进行解码, 就得到与训练实例类似的数据.</p>
<hr>
<p>变分自编码器的 cost func 由两部分组成: </p>
<ol>
<li>常规的重构损失(可用 cross entropy 表示) </li>
<li>latent loss(由高斯分布与实际分布之间的 $KL$散度 表示)</li>
</ol>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/variational_autoencoder.PNG" width="70%"></p>
<p>下面基于 Fashion MNIST 构建一个变分自编码器.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<h2 id="加载-amp-划分数据集"><a href="#加载-amp-划分数据集" class="headerlink" title="加载 &amp; 划分数据集"></a>加载 &amp; 划分数据集</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line">X_train_full = X_train_full.astype(np.float32) / <span class="number">255</span></span><br><span class="line">X_test = X_test.astype(np.float32) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">X_train, X_valid = X_train_full[:-<span class="number">5000</span>], X_train_full[-<span class="number">5000</span>:]</span><br><span class="line">y_train, y_valid = y_train_full[:-<span class="number">5000</span>], y_train_full[-<span class="number">5000</span>:]</span><br></pre></td></tr></table></figure>
<h2 id="自定义采样层"><a href="#自定义采样层" class="headerlink" title="自定义采样层"></a>自定义采样层</h2><p>给定参数 $μ, σ$, 下面的层对 codings 采样.<br>[下面假定 latent loss 是基于 $γ = log(σ^2)$ 的 $KL$ 散度]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">K = keras.backend</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sampling</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        mean, log_var = inputs</span><br><span class="line">        <span class="keyword">return</span> K.random_normal(tf.shape(log_var)) * K.exp(log_var/<span class="number">2</span>) + mean</span><br></pre></td></tr></table></figure>
<p>有了这个层, 就可以从高斯分布 $N(μ, σ^2)$ 中对 codings 向量采样了.</p>
<h2 id="构建-encoder"><a href="#构建-encoder" class="headerlink" title="构建 encoder"></a>构建 encoder</h2><p>codings_size, dense layer 中的 neurons 和 activation 都可以修改.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">codings_size = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">inputs = keras.layers.Input(shape=[<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">z = keras.layers.Flatten()(inputs)</span><br><span class="line">z = keras.layers.Dense(<span class="number">14</span> * <span class="number">14</span>, activation=<span class="string">&quot;elu&quot;</span>)(z)</span><br><span class="line">z = keras.layers.Dense(<span class="number">7</span> * <span class="number">7</span>, activation=<span class="string">&quot;elu&quot;</span>)(z)</span><br><span class="line"><span class="comment"># 计算 μ &amp; γ</span></span><br><span class="line">codings_mean = keras.layers.Dense(codings_size)(z)</span><br><span class="line">codings_log_var = keras.layers.Dense(codings_size)(z)</span><br><span class="line"><span class="comment"># 对编码采样</span></span><br><span class="line">codings = Sampling()([codings_mean, codings_log_var])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合为 encoder</span></span><br><span class="line">variational_encoder = keras.models.Model(</span><br><span class="line">    inputs=[inputs], </span><br><span class="line">    outputs=[codings_mean, codings_log_var, codings]    <span class="comment"># 前两个输出仅作检查用</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="构建-decoder"><a href="#构建-decoder" class="headerlink" title="构建 decoder"></a>构建 decoder</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">decoder_inputs = keras.layers.Input(shape=[codings_size])    <span class="comment"># 考虑一下为什么是这个 shape ~</span></span><br><span class="line"><span class="comment"># 关于编码层对称的两个 Dense 层</span></span><br><span class="line">x = keras.layers.Dense(<span class="number">7</span> * <span class="number">7</span>, activation=<span class="string">&quot;elu&quot;</span>)(decoder_inputs)</span><br><span class="line">x = keras.layers.Dense(<span class="number">14</span> * <span class="number">14</span>, activation=<span class="string">&quot;elu&quot;</span>)(x)</span><br><span class="line"><span class="comment"># 还原输入的神经元数</span></span><br><span class="line"><span class="comment"># sigmoid 激活将每个像素值看作分类任务 (对应编译时的重构损失)</span></span><br><span class="line">x = keras.layers.Dense(<span class="number">28</span> * <span class="number">28</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)(x)</span><br><span class="line">outputs = keras.layers.Reshape([<span class="number">28</span>, <span class="number">28</span>])(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合为 decoder</span></span><br><span class="line">variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])</span><br></pre></td></tr></table></figure>
<h2 id="组合-encoder-amp-decoder"><a href="#组合-encoder-amp-decoder" class="headerlink" title="组合 encoder &amp; decoder"></a>组合 encoder &amp; decoder</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_, _, codings = variational_encoder(inputs)</span><br><span class="line">reconstructions = variational_decoder(codings)</span><br><span class="line"></span><br><span class="line">variational_autoencoder = keras.models.Model(inputs=[inputs], outputs=[reconstructions])</span><br></pre></td></tr></table></figure>
<h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p>这里的 loss 分为 latent loss 和 reconstruction loss 两部分.<br>[另外, 不妨试试其他优化器~]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义的 metrics</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rounded_accuracy</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> keras.metrics.binary_accuracy(tf.<span class="built_in">round</span>(y_true), tf.<span class="built_in">round</span>(y_pred))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算 loss</span></span><br><span class="line">latent_loss = -<span class="number">0.5</span> * K.<span class="built_in">sum</span>(</span><br><span class="line">    <span class="number">1</span> + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean), </span><br><span class="line">    axis=-<span class="number">1</span>)</span><br><span class="line">variational_autoencoder.add_loss(K.mean(latent_loss) / <span class="number">784.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译</span></span><br><span class="line">optimizer = keras.optimizers.RMSprop(learning_rate=<span class="number">0.003</span>)</span><br><span class="line"></span><br><span class="line">variational_autoencoder.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, </span><br><span class="line">                                optimizer=optimizer, </span><br><span class="line">                                metrics=[rounded_accuracy])</span><br></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">performance_lr_scheduler = keras.callbacks.ReduceLROnPlateau(patience=<span class="number">7</span>, factor=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">history = variational_autoencoder.fit(X_train, X_train, </span><br><span class="line">                                      epochs=<span class="number">100</span>, batch_size=<span class="number">64</span>, </span><br><span class="line">                                      validation_data=(X_valid, X_valid), </span><br><span class="line">                                      callbacks=[early_stopping_cb, performance_lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3663 - rounded_accuracy: 0.8768 - val_loss: 0.3566 - val_rounded_accuracy: 0.8752 - lr: 0.0030
Epoch 2/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3336 - rounded_accuracy: 0.9013 - val_loss: 0.3351 - val_rounded_accuracy: 0.9037 - lr: 0.0030
Epoch 3/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3275 - rounded_accuracy: 0.9065 - val_loss: 0.3298 - val_rounded_accuracy: 0.9024 - lr: 0.0030
Epoch 4/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3245 - rounded_accuracy: 0.9089 - val_loss: 0.3288 - val_rounded_accuracy: 0.9057 - lr: 0.0030
Epoch 5/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3223 - rounded_accuracy: 0.9108 - val_loss: 0.3267 - val_rounded_accuracy: 0.9102 - lr: 0.0030
Epoch 6/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3208 - rounded_accuracy: 0.9121 - val_loss: 0.3250 - val_rounded_accuracy: 0.9070 - lr: 0.0030
Epoch 7/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3196 - rounded_accuracy: 0.9130 - val_loss: 0.3237 - val_rounded_accuracy: 0.9138 - lr: 0.0030
Epoch 8/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3188 - rounded_accuracy: 0.9138 - val_loss: 0.3233 - val_rounded_accuracy: 0.9115 - lr: 0.0030
Epoch 9/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3181 - rounded_accuracy: 0.9143 - val_loss: 0.3259 - val_rounded_accuracy: 0.9118 - lr: 0.0030
Epoch 10/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3175 - rounded_accuracy: 0.9148 - val_loss: 0.3270 - val_rounded_accuracy: 0.9098 - lr: 0.0030
Epoch 11/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3171 - rounded_accuracy: 0.9152 - val_loss: 0.3200 - val_rounded_accuracy: 0.9167 - lr: 0.0030
Epoch 12/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3167 - rounded_accuracy: 0.9155 - val_loss: 0.3205 - val_rounded_accuracy: 0.9138 - lr: 0.0030
Epoch 13/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3164 - rounded_accuracy: 0.9157 - val_loss: 0.3219 - val_rounded_accuracy: 0.9112 - lr: 0.0030
Epoch 14/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3160 - rounded_accuracy: 0.9160 - val_loss: 0.3242 - val_rounded_accuracy: 0.9137 - lr: 0.0030
Epoch 15/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3159 - rounded_accuracy: 0.9161 - val_loss: 0.3160 - val_rounded_accuracy: 0.9178 - lr: 0.0030
Epoch 16/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3156 - rounded_accuracy: 0.9162 - val_loss: 0.3217 - val_rounded_accuracy: 0.9097 - lr: 0.0030
Epoch 17/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3154 - rounded_accuracy: 0.9164 - val_loss: 0.3207 - val_rounded_accuracy: 0.9105 - lr: 0.0030
Epoch 18/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3154 - rounded_accuracy: 0.9164 - val_loss: 0.3236 - val_rounded_accuracy: 0.9114 - lr: 0.0030
Epoch 19/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3152 - rounded_accuracy: 0.9168 - val_loss: 0.3173 - val_rounded_accuracy: 0.9166 - lr: 0.0030
Epoch 20/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3150 - rounded_accuracy: 0.9168 - val_loss: 0.3196 - val_rounded_accuracy: 0.9127 - lr: 0.0030
Epoch 21/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3149 - rounded_accuracy: 0.9169 - val_loss: 0.3235 - val_rounded_accuracy: 0.9120 - lr: 0.0030
Epoch 22/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3147 - rounded_accuracy: 0.9170 - val_loss: 0.3213 - val_rounded_accuracy: 0.9144 - lr: 0.0030
Epoch 23/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3071 - rounded_accuracy: 0.9248 - val_loss: 0.3098 - val_rounded_accuracy: 0.9238 - lr: 6.0000e-04
Epoch 24/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3066 - rounded_accuracy: 0.9253 - val_loss: 0.3093 - val_rounded_accuracy: 0.9240 - lr: 6.0000e-04
Epoch 25/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3064 - rounded_accuracy: 0.9255 - val_loss: 0.3095 - val_rounded_accuracy: 0.9232 - lr: 6.0000e-04
Epoch 26/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3062 - rounded_accuracy: 0.9257 - val_loss: 0.3093 - val_rounded_accuracy: 0.9239 - lr: 6.0000e-04
Epoch 27/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3061 - rounded_accuracy: 0.9258 - val_loss: 0.3088 - val_rounded_accuracy: 0.9242 - lr: 6.0000e-04
Epoch 28/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3060 - rounded_accuracy: 0.9259 - val_loss: 0.3090 - val_rounded_accuracy: 0.9245 - lr: 6.0000e-04
Epoch 29/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3060 - rounded_accuracy: 0.9260 - val_loss: 0.3090 - val_rounded_accuracy: 0.9247 - lr: 6.0000e-04
Epoch 30/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3059 - rounded_accuracy: 0.9260 - val_loss: 0.3088 - val_rounded_accuracy: 0.9243 - lr: 6.0000e-04
Epoch 31/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3058 - rounded_accuracy: 0.9261 - val_loss: 0.3085 - val_rounded_accuracy: 0.9249 - lr: 6.0000e-04
Epoch 32/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3057 - rounded_accuracy: 0.9262 - val_loss: 0.3085 - val_rounded_accuracy: 0.9247 - lr: 6.0000e-04
Epoch 33/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3057 - rounded_accuracy: 0.9262 - val_loss: 0.3088 - val_rounded_accuracy: 0.9246 - lr: 6.0000e-04
Epoch 34/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3056 - rounded_accuracy: 0.9263 - val_loss: 0.3083 - val_rounded_accuracy: 0.9247 - lr: 6.0000e-04
Epoch 35/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3056 - rounded_accuracy: 0.9264 - val_loss: 0.3086 - val_rounded_accuracy: 0.9249 - lr: 6.0000e-04
Epoch 36/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3055 - rounded_accuracy: 0.9265 - val_loss: 0.3086 - val_rounded_accuracy: 0.9252 - lr: 6.0000e-04
Epoch 37/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3054 - rounded_accuracy: 0.9265 - val_loss: 0.3086 - val_rounded_accuracy: 0.9242 - lr: 6.0000e-04
Epoch 38/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3054 - rounded_accuracy: 0.9266 - val_loss: 0.3085 - val_rounded_accuracy: 0.9250 - lr: 6.0000e-04
Epoch 39/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3054 - rounded_accuracy: 0.9266 - val_loss: 0.3085 - val_rounded_accuracy: 0.9250 - lr: 6.0000e-04
Epoch 40/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3053 - rounded_accuracy: 0.9266 - val_loss: 0.3080 - val_rounded_accuracy: 0.9255 - lr: 6.0000e-04
Epoch 41/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3053 - rounded_accuracy: 0.9267 - val_loss: 0.3080 - val_rounded_accuracy: 0.9253 - lr: 6.0000e-04
Epoch 42/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3052 - rounded_accuracy: 0.9267 - val_loss: 0.3084 - val_rounded_accuracy: 0.9251 - lr: 6.0000e-04
Epoch 43/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3052 - rounded_accuracy: 0.9268 - val_loss: 0.3082 - val_rounded_accuracy: 0.9252 - lr: 6.0000e-04
Epoch 44/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3052 - rounded_accuracy: 0.9269 - val_loss: 0.3081 - val_rounded_accuracy: 0.9257 - lr: 6.0000e-04
Epoch 45/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3051 - rounded_accuracy: 0.9268 - val_loss: 0.3084 - val_rounded_accuracy: 0.9248 - lr: 6.0000e-04
Epoch 46/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3051 - rounded_accuracy: 0.9270 - val_loss: 0.3080 - val_rounded_accuracy: 0.9252 - lr: 6.0000e-04
Epoch 47/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3050 - rounded_accuracy: 0.9270 - val_loss: 0.3080 - val_rounded_accuracy: 0.9257 - lr: 6.0000e-04
Epoch 48/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9280 - val_loss: 0.3068 - val_rounded_accuracy: 0.9270 - lr: 1.2000e-04
Epoch 49/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9281 - val_loss: 0.3069 - val_rounded_accuracy: 0.9266 - lr: 1.2000e-04
Epoch 50/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9280 - val_loss: 0.3067 - val_rounded_accuracy: 0.9268 - lr: 1.2000e-04
Epoch 51/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9280 - val_loss: 0.3069 - val_rounded_accuracy: 0.9267 - lr: 1.2000e-04
Epoch 52/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9280 - val_loss: 0.3068 - val_rounded_accuracy: 0.9267 - lr: 1.2000e-04
Epoch 53/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3040 - rounded_accuracy: 0.9282 - val_loss: 0.3068 - val_rounded_accuracy: 0.9270 - lr: 1.2000e-04
Epoch 54/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3067 - val_rounded_accuracy: 0.9269 - lr: 1.2000e-04
Epoch 55/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3068 - val_rounded_accuracy: 0.9266 - lr: 1.2000e-04
Epoch 56/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3068 - val_rounded_accuracy: 0.9268 - lr: 1.2000e-04
Epoch 57/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3069 - val_rounded_accuracy: 0.9266 - lr: 1.2000e-04
Epoch 58/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9269 - lr: 2.4000e-05
Epoch 59/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3065 - val_rounded_accuracy: 0.9271 - lr: 2.4000e-05
Epoch 60/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9270 - lr: 2.4000e-05
Epoch 61/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9269 - lr: 2.4000e-05
Epoch 62/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9268 - lr: 2.4000e-05
Epoch 63/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9268 - lr: 2.4000e-05
Epoch 64/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9268 - lr: 2.4000e-05
Epoch 65/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 2.4000e-05
Epoch 66/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9270 - lr: 4.8000e-06
Epoch 67/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3064 - val_rounded_accuracy: 0.9270 - lr: 4.8000e-06
Epoch 68/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3064 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 69/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3064 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 70/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 4.8000e-06
Epoch 71/100
860/860 [==============================] - 11s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 4.8000e-06
Epoch 72/100
860/860 [==============================] - 12s 13ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 73/100
860/860 [==============================] - 12s 14ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3065 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 74/100
860/860 [==============================] - 11s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9273 - lr: 4.8000e-06
Epoch 75/100
860/860 [==============================] - 11s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9269 - lr: 9.6000e-07
Epoch 76/100
860/860 [==============================] - 12s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 9.6000e-07
Epoch 77/100
860/860 [==============================] - 12s 14ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9272 - lr: 9.6000e-07
Epoch 78/100
860/860 [==============================] - 12s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9273 - lr: 9.6000e-07
Epoch 79/100
860/860 [==============================] - 12s 14ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3065 - val_rounded_accuracy: 0.9272 - lr: 9.6000e-07
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">variational_autoencoder.evaluate(X_valid, X_valid)</span><br></pre></td></tr></table></figure>
<pre><code>157/157 [==============================] - 1s 5ms/step - loss: 0.3066 - rounded_accuracy: 0.9273

[0.306575208902359, 0.9272765517234802]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.DataFrame(history.history).plot(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_24_0.png" alt="png"></p>
<h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span>(<span class="params">image</span>):</span></span><br><span class="line">    plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_reconstructions</span>(<span class="params">model, images=X_valid, n_images=<span class="number">12</span></span>):</span></span><br><span class="line">    <span class="comment"># 使用自编码器获得重构图像</span></span><br><span class="line">    reconstructions = model.predict(images[:n_images])</span><br><span class="line">    <span class="comment"># 绘制 &lt;原图像&gt; 与 &lt;重构图像&gt; の 对比</span></span><br><span class="line">    fig = plt.figure(figsize=(n_images * <span class="number">1.5</span>, <span class="number">3</span>))</span><br><span class="line">    <span class="keyword">for</span> image_index <span class="keyword">in</span> <span class="built_in">range</span>(n_images):</span><br><span class="line">        plt.subplot(<span class="number">2</span>, n_images, <span class="number">1</span> + image_index)</span><br><span class="line">        plot_image(images[image_index])</span><br><span class="line">        plt.subplot(<span class="number">2</span>, n_images, <span class="number">1</span> + n_images + image_index)</span><br><span class="line">        plot_image(reconstructions[image_index])</span><br><span class="line"></span><br><span class="line">show_reconstructions(variational_autoencoder)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_26_0.png" alt="png"></p>
<h2 id="生成新实例"><a href="#生成新实例" class="headerlink" title="生成新实例"></a>生成新实例</h2><p>下面使用刚才训练好的变分自编码器, 尝试生成一些新实例.</p>
<p>只需要从(对应)高斯分布中随机采样 codings 向量, 再将它们解码即可.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机采样 12 个 codings 向量并解码</span></span><br><span class="line">codings = tf.random.normal(shape=[<span class="number">12</span>, codings_size])</span><br><span class="line">images = variational_decoder(codings).numpy()</span><br></pre></td></tr></table></figure>
<p>展示生成的新实例.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiple_images</span>(<span class="params">images, n_cols=<span class="literal">None</span></span>):</span></span><br><span class="line">    n_cols = n_cols <span class="keyword">or</span> <span class="built_in">len</span>(images)</span><br><span class="line">    n_rows = (<span class="built_in">len</span>(images) - <span class="number">1</span>) // n_cols + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> images.shape[-<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 对灰度图像, 压缩 channels 维度</span></span><br><span class="line">        images = np.squeeze(images, axis=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    plt.figure(figsize=(<span class="number">2</span> * n_cols, <span class="number">2</span> * n_rows))</span><br><span class="line">    <span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        plt.subplot(n_rows, n_cols, index + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">plot_multiple_images(images, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_31_0.png" alt="png"></p>
<p>可以看出生成的新实例中多数确实形如 Fashion MNIST 中的图像, 但是它们还是比较模糊的.</p>
<h2 id="语义插值"><a href="#语义插值" class="headerlink" title="语义插值"></a>语义插值</h2><p>依靠变分自编码器, 可<strong>在 codings 级别进行插值</strong>, 而不是在像素级别进行插值(这导致结果看起来像两张图象重叠在一起).</p>
<p>只需让两张图像通过 encoder, 对所得 codings 进行插值, 再将结果通过 decoder 即可得到最终的插值图像.</p>
<p>下面通过实践感受一下语义插值的效果.(其中的 codings 是[9]中生成的新实例的 codings)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.对 codings 进行插值后解码</span></span><br><span class="line">codings_grid = tf.reshape(codings, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, codings_size])</span><br><span class="line"><span class="comment"># 这里 tf.image.resize() 默认执行双线性插值</span></span><br><span class="line">larger_grid = tf.image.resize(codings_grid, size=[<span class="number">5</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">interpolated_codings = tf.reshape(larger_grid, [-<span class="number">1</span>, codings_size])</span><br><span class="line">images = variational_decoder(interpolated_codings).numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.展示解码后的图像</span></span><br><span class="line">plt.figure(figsize=(<span class="number">7</span> * <span class="number">1.5</span>, <span class="number">5</span> * <span class="number">1.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">7</span>, index + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> index%<span class="number">7</span>%<span class="number">2</span>==<span class="number">0</span> <span class="keyword">and</span> index//<span class="number">7</span>%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        plt.gca().get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        plt.gca().get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_35_0.png" alt="png"></p>
<p>观察第1行, 第2列的鞋子图像, 它很自然地过渡于其左右的两张鞋子的图像之间~</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>各种学习率调度之间的对比</title>
    <url>/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>❓ 如何选择使用什么学习率调度? 选项有: </p>
<ol>
<li>幂调度  </li>
<li>指数调度</li>
<li>分段常数调度</li>
<li>性能调度</li>
<li>1Cycle 调度</li>
</ol>
<p>另外, 学习率调度有<strong>两种不同的实现方式</strong>, 分别对应 &lt;在每个 epoch 开始前更新学习率&gt; 和 &lt;在每个 step 开始前更新学习率&gt;. 它们在性能上表现如何?</p>
<span id="more"></span>
<p>※ 下面针对相同的 &lt;网络架构 &amp; 优化器&gt;, 对比不同学习率调度的<strong>收敛速度</strong>和<strong>模型性能</strong>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>🔺 针对 Fashion MNIST 数据集, 开展下面的测试.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集 (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist 中数字标签对应的类别名称</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 展示部分训练集实例</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m 行 n 列</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_6_0.png" alt="png"></p>
<h1 id="定义构建模型的函数"><a href="#定义构建模型的函数" class="headerlink" title="定义构建模型的函数"></a>定义构建模型的函数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">optimizer, activation=<span class="string">&quot;elu&quot;</span>, n_layers=<span class="number">4</span>, n_neurons=<span class="number">80</span></span>):</span></span><br><span class="line">    model = keras.models.Sequential([</span><br><span class="line">        keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">        </span><br><span class="line">        keras.layers.Dense(<span class="number">400</span>, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">        </span><br><span class="line">        keras.layers.Dense(<span class="number">200</span>, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers):</span><br><span class="line">        model.add(keras.layers.Dense(n_neurons, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">        model.add(keras.layers.BatchNormalization())</span><br><span class="line">        </span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                  optimizer=optimizer, </span><br><span class="line">                  metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>Remark: 基于训练速度考虑, 下面选用了 batch_size=128 以加速训练, 但使用更小的 batch_size (如 32) 可能获得更好的模型.</p>
<h1 id="幂调度"><a href="#幂调度" class="headerlink" title="幂调度"></a>幂调度</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义(带幂调度的)优化器, 构建模型</span></span><br><span class="line"><span class="comment"># decay value 表示每 5000 个 step 更新一次 learning rate</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>, decay=<span class="number">1</span>/<span class="number">5000</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 13s 24ms/step - loss: 0.4903 - accuracy: 0.8215 - val_loss: 0.3833 - val_accuracy: 0.8706
Epoch 2/100
430/430 [==============================] - 10s 23ms/step - loss: 0.3652 - accuracy: 0.8666 - val_loss: 0.3747 - val_accuracy: 0.8706
Epoch 3/100
430/430 [==============================] - 10s 23ms/step - loss: 0.3305 - accuracy: 0.8774 - val_loss: 0.4111 - val_accuracy: 0.8530
Epoch 4/100
430/430 [==============================] - 10s 23ms/step - loss: 0.3070 - accuracy: 0.8880 - val_loss: 0.3452 - val_accuracy: 0.8750
Epoch 5/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2825 - accuracy: 0.8951 - val_loss: 0.3383 - val_accuracy: 0.8792
Epoch 6/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2699 - accuracy: 0.9001 - val_loss: 0.3114 - val_accuracy: 0.8920
Epoch 7/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2561 - accuracy: 0.9040 - val_loss: 0.3404 - val_accuracy: 0.8792
Epoch 8/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2477 - accuracy: 0.9082 - val_loss: 0.3086 - val_accuracy: 0.8876
Epoch 9/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2317 - accuracy: 0.9154 - val_loss: 0.3457 - val_accuracy: 0.8740
Epoch 10/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2214 - accuracy: 0.9187 - val_loss: 0.3439 - val_accuracy: 0.8930
Epoch 11/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2117 - accuracy: 0.9207 - val_loss: 0.3006 - val_accuracy: 0.8974
Epoch 12/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2027 - accuracy: 0.9248 - val_loss: 0.3517 - val_accuracy: 0.8904
Epoch 13/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1972 - accuracy: 0.9273 - val_loss: 0.3063 - val_accuracy: 0.8968
Epoch 14/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1875 - accuracy: 0.9302 - val_loss: 0.3425 - val_accuracy: 0.8902
Epoch 15/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1791 - accuracy: 0.9344 - val_loss: 0.3241 - val_accuracy: 0.8928
Epoch 16/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1743 - accuracy: 0.9356 - val_loss: 0.3675 - val_accuracy: 0.8822
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_12_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3343 - accuracy: 0.8893

[0.33429378271102905, 0.8892999887466431]
</code></pre><h1 id="指数调度"><a href="#指数调度" class="headerlink" title="指数调度"></a>指数调度</h1><h2 id="第一种实现"><a href="#第一种实现" class="headerlink" title="第一种实现"></a>第一种实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.定义指数调度函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exp_decay</span>(<span class="params">lr_0, s</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">exp_decay_func</span>(<span class="params">epoch</span>):</span></span><br><span class="line">        <span class="keyword">return</span> lr_0 * <span class="number">0.1</span>**(epoch / s)    <span class="comment"># 每 s 个 epoch 更新一次 learning rate</span></span><br><span class="line">    <span class="keyword">return</span> exp_decay_func</span><br><span class="line"></span><br><span class="line">exp_decay_func = exp_decay(lr_0=<span class="number">0.009</span>, s=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remark: 从下面训练输出来看, 并非每 s 个 epoch 更新一次 learning rate......</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.使用上述调度函数创建 LearningRateScheduler 回调</span></span><br><span class="line">exp_lr_scheduler = keras.callbacks.LearningRateScheduler(exp_decay_func)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.将此回调传递给 fit() 中的 callbacks 参数</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 构建模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb, exp_lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 12s 23ms/step - loss: 0.4903 - accuracy: 0.8211 - val_loss: 0.3804 - val_accuracy: 0.8740 - lr: 0.0090
Epoch 2/100
430/430 [==============================] - 10s 23ms/step - loss: 0.3309 - accuracy: 0.8782 - val_loss: 0.3392 - val_accuracy: 0.8768 - lr: 0.0042
Epoch 3/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2749 - accuracy: 0.8970 - val_loss: 0.3342 - val_accuracy: 0.8770 - lr: 0.0019
Epoch 4/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2371 - accuracy: 0.9122 - val_loss: 0.2884 - val_accuracy: 0.8946 - lr: 9.0000e-04
Epoch 5/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2071 - accuracy: 0.9219 - val_loss: 0.2804 - val_accuracy: 0.9000 - lr: 4.1774e-04
Epoch 6/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1918 - accuracy: 0.9280 - val_loss: 0.2745 - val_accuracy: 0.8990 - lr: 1.9390e-04
Epoch 7/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1816 - accuracy: 0.9314 - val_loss: 0.2703 - val_accuracy: 0.9018 - lr: 9.0000e-05
Epoch 8/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1772 - accuracy: 0.9336 - val_loss: 0.2718 - val_accuracy: 0.9018 - lr: 4.1774e-05
Epoch 9/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1742 - accuracy: 0.9356 - val_loss: 0.2728 - val_accuracy: 0.9018 - lr: 1.9390e-05
Epoch 10/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1726 - accuracy: 0.9359 - val_loss: 0.2729 - val_accuracy: 0.9030 - lr: 9.0000e-06
Epoch 11/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1727 - accuracy: 0.9354 - val_loss: 0.2731 - val_accuracy: 0.9018 - lr: 4.1774e-06
Epoch 12/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1726 - accuracy: 0.9362 - val_loss: 0.2731 - val_accuracy: 0.9024 - lr: 1.9390e-06
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_18_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.2981 - accuracy: 0.8966

[0.29814115166664124, 0.8966000080108643]
</code></pre><h2 id="第二种实现"><a href="#第二种实现" class="headerlink" title="第二种实现"></a>第二种实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.使用 keras.optimizers.schedule 中的调度来定义学习率</span></span><br><span class="line">s = <span class="number">15</span> * <span class="built_in">len</span>(x_train) // <span class="number">128</span>    </span><br><span class="line"><span class="comment"># s 为假设训练 15 个轮次且 batch size=128 时总的训练 step 数</span></span><br><span class="line">learning_rate = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=<span class="number">0.009</span>,</span><br><span class="line">                                                            decay_steps=s, decay_rate=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>要使用此方法实现<strong>其他学习率调度</strong>, 可参考下面的链接:<br><a href="https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/schedules?hl=en">https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/schedules?hl=en</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.以上述学习率作为优化器学习率 (这里不能用 Nadam...) </span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=learning_rate)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.使用上述优化器训练模型</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">model = build_model(optimizer)</span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 14ms/step - loss: 0.4937 - accuracy: 0.8215 - val_loss: 0.3949 - val_accuracy: 0.8542
Epoch 2/100
430/430 [==============================] - 5s 12ms/step - loss: 0.3724 - accuracy: 0.8629 - val_loss: 0.3517 - val_accuracy: 0.8770
Epoch 3/100
430/430 [==============================] - 5s 12ms/step - loss: 0.3303 - accuracy: 0.8783 - val_loss: 0.4212 - val_accuracy: 0.8522
Epoch 4/100
430/430 [==============================] - 5s 12ms/step - loss: 0.3029 - accuracy: 0.8887 - val_loss: 0.3395 - val_accuracy: 0.8770
Epoch 5/100
430/430 [==============================] - 5s 12ms/step - loss: 0.2692 - accuracy: 0.8999 - val_loss: 0.2835 - val_accuracy: 0.8914
Epoch 6/100
430/430 [==============================] - 5s 12ms/step - loss: 0.2477 - accuracy: 0.9080 - val_loss: 0.2935 - val_accuracy: 0.8916
Epoch 7/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2252 - accuracy: 0.9165 - val_loss: 0.3083 - val_accuracy: 0.8902
Epoch 8/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2067 - accuracy: 0.9232 - val_loss: 0.2852 - val_accuracy: 0.8958
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1877 - accuracy: 0.9306 - val_loss: 0.2797 - val_accuracy: 0.9030
Epoch 10/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1701 - accuracy: 0.9356 - val_loss: 0.3188 - val_accuracy: 0.8972
Epoch 11/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1559 - accuracy: 0.9411 - val_loss: 0.3083 - val_accuracy: 0.8968
Epoch 12/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1412 - accuracy: 0.9469 - val_loss: 0.3034 - val_accuracy: 0.8988
Epoch 13/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1277 - accuracy: 0.9523 - val_loss: 0.2980 - val_accuracy: 0.9078
Epoch 14/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1123 - accuracy: 0.9582 - val_loss: 0.3333 - val_accuracy: 0.8972
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_24_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3387 - accuracy: 0.8896

[0.3386974036693573, 0.8895999789237976]
</code></pre><h1 id="分段常数调度"><a href="#分段常数调度" class="headerlink" title="分段常数调度"></a>分段常数调度</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.定义分段常数调度函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">piecewise_constant_func</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    <span class="keyword">if</span> epoch &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.009</span></span><br><span class="line">    <span class="keyword">elif</span> epoch &lt; <span class="number">6</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.003</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.001</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.使用上述调度函数创建 LearningRateScheduler 回调</span></span><br><span class="line">piecewise_constant_lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_func)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.将此回调传递给 fit() 中的 callbacks 参数</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 构建模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb, piecewise_constant_lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 13s 24ms/step - loss: 0.4903 - accuracy: 0.8213 - val_loss: 0.3870 - val_accuracy: 0.8682 - lr: 0.0090
Epoch 2/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3643 - accuracy: 0.8672 - val_loss: 0.3671 - val_accuracy: 0.8720 - lr: 0.0090
Epoch 3/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3302 - accuracy: 0.8775 - val_loss: 0.4405 - val_accuracy: 0.8404 - lr: 0.0090
Epoch 4/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2599 - accuracy: 0.9039 - val_loss: 0.2952 - val_accuracy: 0.8934 - lr: 0.0030
Epoch 5/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2377 - accuracy: 0.9097 - val_loss: 0.2852 - val_accuracy: 0.8972 - lr: 0.0030
Epoch 6/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2253 - accuracy: 0.9159 - val_loss: 0.2886 - val_accuracy: 0.8970 - lr: 0.0030- ETA: 0s - loss: 0.2253 - accuracy
Epoch 7/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1828 - accuracy: 0.9320 - val_loss: 0.2748 - val_accuracy: 0.9040 - lr: 0.0010
Epoch 8/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1668 - accuracy: 0.9373 - val_loss: 0.2692 - val_accuracy: 0.9028 - lr: 0.0010
Epoch 9/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1561 - accuracy: 0.9424 - val_loss: 0.2827 - val_accuracy: 0.9022 - lr: 0.0010
Epoch 10/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1468 - accuracy: 0.9441 - val_loss: 0.3056 - val_accuracy: 0.9016 - lr: 0.0010
Epoch 11/100
430/430 [==============================] - 11s 24ms/step - loss: 0.1372 - accuracy: 0.9484 - val_loss: 0.3081 - val_accuracy: 0.8996 - lr: 0.0010
Epoch 12/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1287 - accuracy: 0.9518 - val_loss: 0.3078 - val_accuracy: 0.9030 - lr: 0.0010
Epoch 13/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1210 - accuracy: 0.9545 - val_loss: 0.3082 - val_accuracy: 0.9010 - lr: 0.0010
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_29_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3019 - accuracy: 0.8990: 0s - los

[0.30188095569610596, 0.8989999890327454]
</code></pre><h1 id="性能调度"><a href="#性能调度" class="headerlink" title="性能调度"></a>性能调度</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.创建 ReduceLROnPlateau 回调</span></span><br><span class="line">performance_lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">0.25</span>, patience=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.将此回调传递给 fit() 中的 callbacks 参数</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 构建模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb, performance_lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 13s 24ms/step - loss: 0.4903 - accuracy: 0.8213 - val_loss: 0.3849 - val_accuracy: 0.8700 - lr: 0.0090
Epoch 2/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3642 - accuracy: 0.8670 - val_loss: 0.3712 - val_accuracy: 0.8716 - lr: 0.0090
Epoch 3/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3305 - accuracy: 0.8787 - val_loss: 0.4373 - val_accuracy: 0.8462 - lr: 0.0090
Epoch 4/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3077 - accuracy: 0.8865 - val_loss: 0.3519 - val_accuracy: 0.8670 - lr: 0.0090
Epoch 5/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2831 - accuracy: 0.8946 - val_loss: 0.3161 - val_accuracy: 0.8792 - lr: 0.0090
Epoch 6/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2691 - accuracy: 0.9006 - val_loss: 0.3206 - val_accuracy: 0.8834 - lr: 0.0090
Epoch 7/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2561 - accuracy: 0.9046 - val_loss: 0.3191 - val_accuracy: 0.8844 - lr: 0.0090
Epoch 8/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1977 - accuracy: 0.9258 - val_loss: 0.2603 - val_accuracy: 0.9028 - lr: 0.0022
Epoch 9/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1764 - accuracy: 0.9343 - val_loss: 0.2731 - val_accuracy: 0.8996 - lr: 0.0022
Epoch 10/100
430/430 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.93 - 10s 24ms/step - loss: 0.1656 - accuracy: 0.9373 - val_loss: 0.2882 - val_accuracy: 0.9024 - lr: 0.0022
Epoch 11/100
430/430 [==============================] - 11s 24ms/step - loss: 0.1377 - accuracy: 0.9487 - val_loss: 0.2825 - val_accuracy: 0.9028 - lr: 5.6250e-04
Epoch 12/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1282 - accuracy: 0.9525 - val_loss: 0.2891 - val_accuracy: 0.9040 - lr: 5.6250e-04
Epoch 13/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1192 - accuracy: 0.9559 - val_loss: 0.2879 - val_accuracy: 0.9088 - lr: 1.4062e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_33_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.2989 - accuracy: 0.8971

[0.2988533079624176, 0.8970999717712402]
</code></pre><h1 id="1Cycle-调度"><a href="#1Cycle-调度" class="headerlink" title="1Cycle 调度"></a>1Cycle 调度</h1><h2 id="寻找最佳学习率"><a href="#寻找最佳学习率" class="headerlink" title="寻找最佳学习率"></a>寻找最佳学习率</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">K = keras.backend</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExponentialLearningRate</span>(<span class="params">keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; 继承 Callback 以自定义回调 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, factor</span>):</span></span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.rates = []    </span><br><span class="line">        self.losses = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_batch_end</span>(<span class="params">self, batch, logs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; 在每个 step 结束时：记录当前学习率和 loss 值并更新当前学习率 &quot;&quot;&quot;</span></span><br><span class="line">        self.rates.append(K.get_value(self.model.optimizer.lr))</span><br><span class="line">        self.losses.append(logs[<span class="string">&quot;loss&quot;</span>])</span><br><span class="line">        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_learning_rate</span>(<span class="params">model, X, y, epochs=<span class="number">1</span>, batch_size=<span class="number">32</span>, min_rate=<span class="number">10</span>**-<span class="number">5</span>, max_rate=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; 该函数通过上述自定义回调，获取训练中的学习率以及对应的 loss &quot;&quot;&quot;</span></span><br><span class="line">    init_weights = model.get_weights()</span><br><span class="line">    iterations = <span class="built_in">len</span>(X) // batch_size * epochs</span><br><span class="line">    factor = np.exp(np.log(max_rate / min_rate) / iterations)</span><br><span class="line">    init_lr = K.get_value(model.optimizer.lr)</span><br><span class="line">    K.set_value(model.optimizer.lr, min_rate)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建回调实例并训练模型</span></span><br><span class="line">    exp_lr = ExponentialLearningRate(factor)</span><br><span class="line">    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,</span><br><span class="line">                        callbacks=[exp_lr])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练完毕后，重置学习率和模型的连接权重</span></span><br><span class="line">    K.set_value(model.optimizer.lr, init_lr)</span><br><span class="line">    model.set_weights(init_weights)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> exp_lr.rates, exp_lr.losses</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_lr_vs_loss</span>(<span class="params">rates, losses</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; 绘制 lr vs loss 曲线 &quot;&quot;&quot;</span></span><br><span class="line">    plt.plot(rates, losses)</span><br><span class="line">    plt.gca().set_xscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    plt.hlines(<span class="built_in">min</span>(losses), <span class="built_in">min</span>(rates), <span class="built_in">max</span>(rates))</span><br><span class="line">    plt.axis([<span class="built_in">min</span>(rates), <span class="built_in">max</span>(rates), <span class="built_in">min</span>(losses), (losses[<span class="number">0</span>] + <span class="built_in">min</span>(losses)) / <span class="number">2</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Learning rate&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 构建模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 1 个轮次并获取 learning &amp; loss </span></span><br><span class="line">rates, losses = find_learning_rate(model, x_train, y_train, epochs=<span class="number">1</span>, batch_size=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据 learning &amp; loss 作图</span></span><br><span class="line">plot_lr_vs_loss(rates, losses)</span><br></pre></td></tr></table></figure>
<pre><code>430/430 [==============================] - 13s 25ms/step - loss: 2129.6172 - accuracy: 0.5144
</code></pre><p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_37_1.png" alt="png"></p>
<p>Remark: 根据实际测试, 从上图中选取最佳学习率的效果并不好…</p>
<h2 id="根据所选学习率-使用-1cycle-调度"><a href="#根据所选学习率-使用-1cycle-调度" class="headerlink" title="根据所选学习率, 使用 1cycle 调度"></a>根据所选学习率, 使用 1cycle 调度</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneCycleScheduler</span>(<span class="params">keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; 继承 Callback 类, 实现 1Cycle 调度 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, iterations, max_rate, start_rate=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 last_iterations=<span class="literal">None</span>, last_rate=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.iterations = iterations    </span><br><span class="line">        self.max_rate = max_rate</span><br><span class="line">        self.start_rate = start_rate <span class="keyword">or</span> max_rate / <span class="number">10</span></span><br><span class="line">        self.last_iterations = last_iterations <span class="keyword">or</span> iterations // <span class="number">10</span> + <span class="number">1</span></span><br><span class="line">        self.half_iteration = (iterations - self.last_iterations) // <span class="number">2</span></span><br><span class="line">        self.last_rate = last_rate <span class="keyword">or</span> self.start_rate / <span class="number">1000</span></span><br><span class="line">        self.iteration = <span class="number">0</span> </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_interpolate</span>(<span class="params">self, iter1, iter2, rate1, rate2</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_batch_begin</span>(<span class="params">self, batch, logs</span>):</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.iteration &lt; self.half_iteration:          <span class="comment"># 第一段的学习率</span></span><br><span class="line">            rate = self._interpolate(<span class="number">0</span>, self.half_iteration, self.start_rate, self.max_rate)</span><br><span class="line">        <span class="keyword">elif</span> self.iteration &lt; <span class="number">2</span> * self.half_iteration:    <span class="comment"># 第二段的学习率</span></span><br><span class="line">            rate = self._interpolate(self.half_iteration, <span class="number">2</span> * self.half_iteration, self.max_rate, self.start_rate)</span><br><span class="line">        <span class="keyword">else</span>:                                             <span class="comment"># 最后一段的学习率</span></span><br><span class="line">            rate = self._interpolate(<span class="number">2</span> * self.half_iteration, self.iterations, self.start_rate, self.last_rate)</span><br><span class="line">            rate = <span class="built_in">max</span>(rate, self.last_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 当前迭代数自增 1，同时更新学习率</span></span><br><span class="line">        self.iteration += <span class="number">1</span></span><br><span class="line">        K.set_value(self.model.optimizer.lr, rate)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_epochs = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 1Cycle 调度回调实例, 并使用该回调训练模型</span></span><br><span class="line">onecycle = OneCycleScheduler(<span class="built_in">len</span>(x_train) // batch_size * n_epochs, max_rate=<span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, epochs=n_epochs,</span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[onecycle])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
430/430 [==============================] - 11s 25ms/step - loss: 0.4601 - accuracy: 0.8402 - val_loss: 0.3513 - val_accuracy: 0.8760
Epoch 2/20
430/430 [==============================] - 11s 25ms/step - loss: 0.3500 - accuracy: 0.8732 - val_loss: 0.3418 - val_accuracy: 0.8762
Epoch 3/20
430/430 [==============================] - 11s 25ms/step - loss: 0.3218 - accuracy: 0.8815 - val_loss: 0.3950 - val_accuracy: 0.8578
Epoch 4/20
430/430 [==============================] - 11s 25ms/step - loss: 0.3073 - accuracy: 0.8866 - val_loss: 0.3414 - val_accuracy: 0.8740
Epoch 5/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2881 - accuracy: 0.8909 - val_loss: 0.2952 - val_accuracy: 0.8892
Epoch 6/20
430/430 [==============================] - 11s 26ms/step - loss: 0.2787 - accuracy: 0.8969 - val_loss: 0.3304 - val_accuracy: 0.8826
Epoch 7/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2713 - accuracy: 0.8986 - val_loss: 0.3386 - val_accuracy: 0.8776
Epoch 8/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2624 - accuracy: 0.9034 - val_loss: 0.3541 - val_accuracy: 0.8758
Epoch 9/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2537 - accuracy: 0.9058 - val_loss: 0.3528 - val_accuracy: 0.8760
Epoch 10/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2419 - accuracy: 0.9105 - val_loss: 0.3156 - val_accuracy: 0.8914
Epoch 11/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2232 - accuracy: 0.9164 - val_loss: 0.3197 - val_accuracy: 0.8896
Epoch 12/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2038 - accuracy: 0.9248 - val_loss: 0.3303 - val_accuracy: 0.8864
Epoch 13/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1909 - accuracy: 0.9282 - val_loss: 0.2927 - val_accuracy: 0.8974
Epoch 14/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1696 - accuracy: 0.9369 - val_loss: 0.3014 - val_accuracy: 0.8966
Epoch 15/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1521 - accuracy: 0.9432 - val_loss: 0.2978 - val_accuracy: 0.8960
Epoch 16/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1325 - accuracy: 0.9504 - val_loss: 0.3239 - val_accuracy: 0.8954
Epoch 17/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1079 - accuracy: 0.9596 - val_loss: 0.3080 - val_accuracy: 0.9064
Epoch 18/20
430/430 [==============================] - 11s 25ms/step - loss: 0.0824 - accuracy: 0.9700 - val_loss: 0.3283 - val_accuracy: 0.9054
Epoch 19/20
430/430 [==============================] - 11s 25ms/step - loss: 0.0620 - accuracy: 0.9776 - val_loss: 0.3511 - val_accuracy: 0.9090
Epoch 20/20
430/430 [==============================] - 11s 25ms/step - loss: 0.0505 - accuracy: 0.9823 - val_loss: 0.3590 - val_accuracy: 0.9068
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_42_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.4023 - accuracy: 0.9012

[0.4022747278213501, 0.901199996471405]
</code></pre><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Scheduler</th>
<th>Initial Learning Rate</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Evaluation on x_test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Power scheduling</td>
<td>0.009</td>
<td>10s/epoch</td>
<td>11 epochs</td>
<td>accuracy=0.8893</td>
</tr>
<tr>
<td>Exponential scheduling 1</td>
<td>0.009</td>
<td>10s/epoch</td>
<td>7 epochs</td>
<td>accuracy=0.8966</td>
</tr>
<tr>
<td>Exponential scheduling 2</td>
<td>0.009</td>
<td>6s/epoch</td>
<td>9 epochs</td>
<td>accuracy=0.8896</td>
</tr>
<tr>
<td>Piecewise constant scheduling</td>
<td>0.009</td>
<td>10s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8990</td>
</tr>
<tr>
<td>Performance scheduling</td>
<td>0.009</td>
<td>10s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8971</td>
</tr>
<tr>
<td>1cycle scheduling</td>
<td>0.001</td>
<td>11s/epoch</td>
<td>/</td>
<td>accuracy=0.9012</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>幂调度在各方面都不优于指数调度.</li>
<li>指数调度的第1种实现: 训练速度一般, 收敛速度较快, 模型性能更好.</li>
<li>指数调度的第2种实现: <strong>训练速度快</strong>, 收敛速度较慢, 模型性能稍差.</li>
<li>分段常数调度的训练 &amp; 收敛速度都不错, 模型性能也较好, 但<strong>难以选择超参数</strong>的最优组合.</li>
<li>性能调度的训练 &amp; 收敛速度都不错, 模型性能也较好, 并且<strong>实现非常简单</strong>, 超参数选择也不困难.</li>
<li>1Cycle 调度的学习率呈分段的线性函数变化, <strong>需要提前预估 epochs</strong>, 并且虽然达到了最佳性能, 但从其训练输出来看, 这样的结果似乎有点儿难以理解…</li>
</ol>
<p>🐒 可以改进的点: 困难还是在于难以快速地获取最佳学习率…[6.1]中的方法似乎也不很可靠.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>各种优化器之间的对比</title>
    <url>/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>❓ 如何选择优化器? Momentum, Nesterov Momentum, RMSprop, Adam, Nadam?</p>
<span id="more"></span>
<p>※ 下面在相同网络架构上, 使用不同优化器, 对比它们的<strong>收敛速度</strong>和<strong>模型性能</strong>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>🔺 针对 Fashion MNIST 数据集, 开展下面的测试.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集 (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist 中数字标签对应的类别名称</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 展示部分训练集实例</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m 行 n 列</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_6_0.png" alt="png"></p>
<h1 id="定义构建-amp-训练模型的函数"><a href="#定义构建-amp-训练模型的函数" class="headerlink" title="定义构建 &amp; 训练模型的函数"></a>定义构建 &amp; 训练模型的函数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">optimizer, activation=<span class="string">&quot;elu&quot;</span>, n_layers=<span class="number">4</span>, n_neurons=<span class="number">100</span></span>):</span></span><br><span class="line">    <span class="comment"># 1.模型构建</span></span><br><span class="line">    model = keras.models.Sequential([</span><br><span class="line">        keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">        </span><br><span class="line">        keras.layers.Dense(<span class="number">400</span>, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">        </span><br><span class="line">        keras.layers.Dense(<span class="number">200</span>, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers):</span><br><span class="line">        model.add(keras.layers.Dense(n_neurons, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">        model.add(keras.layers.BatchNormalization())</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    <span class="comment"># 2.模型编译</span></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                  optimizer=optimizer, </span><br><span class="line">                  metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">model, batch_size=<span class="number">128</span>, patience=<span class="number">5</span></span>):</span></span><br><span class="line">    <span class="comment"># 默认 batch_size=128 以加速训练, 若模型性能变差, 可设置 batch_size=32</span></span><br><span class="line">    early_stopping_cb = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">    history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                        validation_data=(x_valid, y_valid),</span><br><span class="line">                        batch_size=batch_size,</span><br><span class="line">                        callbacks=[early_stopping_cb])</span><br><span class="line">    <span class="keyword">return</span> history</span><br></pre></td></tr></table></figure>
<h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">model_with_momentum = build_model(momentum)</span><br><span class="line">history = train_model(model_with_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 13ms/step - loss: 0.6790 - accuracy: 0.7709 - val_loss: 0.4564 - val_accuracy: 0.8396
Epoch 2/100
430/430 [==============================] - 6s 15ms/step - loss: 0.4439 - accuracy: 0.8425 - val_loss: 0.4004 - val_accuracy: 0.8566
Epoch 3/100
430/430 [==============================] - 7s 15ms/step - loss: 0.3972 - accuracy: 0.8589 - val_loss: 0.3731 - val_accuracy: 0.8660
Epoch 4/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3691 - accuracy: 0.8685 - val_loss: 0.3591 - val_accuracy: 0.8722
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3443 - accuracy: 0.8772 - val_loss: 0.3512 - val_accuracy: 0.8768
Epoch 6/100
430/430 [==============================] - 6s 13ms/step - loss: 0.3311 - accuracy: 0.8793 - val_loss: 0.3423 - val_accuracy: 0.8766
Epoch 7/100
430/430 [==============================] - 6s 13ms/step - loss: 0.3154 - accuracy: 0.8867 - val_loss: 0.3334 - val_accuracy: 0.8812
Epoch 8/100
430/430 [==============================] - 7s 15ms/step - loss: 0.3037 - accuracy: 0.8893 - val_loss: 0.3271 - val_accuracy: 0.8820
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2911 - accuracy: 0.8926 - val_loss: 0.3231 - val_accuracy: 0.8812
Epoch 10/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2805 - accuracy: 0.8977 - val_loss: 0.3231 - val_accuracy: 0.8800
Epoch 11/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2707 - accuracy: 0.9016 - val_loss: 0.3206 - val_accuracy: 0.8824
Epoch 12/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2627 - accuracy: 0.9039 - val_loss: 0.3182 - val_accuracy: 0.8822
Epoch 13/100
430/430 [==============================] - 6s 13ms/step - loss: 0.2552 - accuracy: 0.9067 - val_loss: 0.3145 - val_accuracy: 0.8840
Epoch 14/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2464 - accuracy: 0.9097 - val_loss: 0.3167 - val_accuracy: 0.8868
Epoch 15/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2374 - accuracy: 0.9141 - val_loss: 0.3174 - val_accuracy: 0.8834
Epoch 16/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2329 - accuracy: 0.9149 - val_loss: 0.3188 - val_accuracy: 0.8856
Epoch 17/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2242 - accuracy: 0.9187 - val_loss: 0.3166 - val_accuracy: 0.8842
Epoch 18/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2188 - accuracy: 0.9204 - val_loss: 0.3149 - val_accuracy: 0.8872
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_11_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3419 - accuracy: 0.8763

[0.34192100167274475, 0.8762999773025513]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.003</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">model_with_momentum = build_model(momentum)</span><br><span class="line">history = train_model(model_with_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 9s 17ms/step - loss: 0.5472 - accuracy: 0.8101 - val_loss: 0.3869 - val_accuracy: 0.8626
Epoch 2/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3755 - accuracy: 0.8643 - val_loss: 0.3510 - val_accuracy: 0.8716
Epoch 3/100
430/430 [==============================] - 7s 17ms/step - loss: 0.3345 - accuracy: 0.8773 - val_loss: 0.3355 - val_accuracy: 0.8802
Epoch 4/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3073 - accuracy: 0.8879 - val_loss: 0.3271 - val_accuracy: 0.8786
Epoch 5/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2814 - accuracy: 0.8966 - val_loss: 0.3202 - val_accuracy: 0.8868
Epoch 6/100
430/430 [==============================] - 8s 20ms/step - loss: 0.2678 - accuracy: 0.9011 - val_loss: 0.3181 - val_accuracy: 0.8812
Epoch 7/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2494 - accuracy: 0.9088 - val_loss: 0.3124 - val_accuracy: 0.8866
Epoch 8/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2354 - accuracy: 0.9132 - val_loss: 0.3114 - val_accuracy: 0.8876
Epoch 9/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2205 - accuracy: 0.9174 - val_loss: 0.3042 - val_accuracy: 0.8892
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2089 - accuracy: 0.9223 - val_loss: 0.3146 - val_accuracy: 0.8898
Epoch 11/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1996 - accuracy: 0.9258 - val_loss: 0.3133 - val_accuracy: 0.8910
Epoch 12/100
430/430 [==============================] - 7s 16ms/step - loss: 0.1881 - accuracy: 0.9310 - val_loss: 0.3129 - val_accuracy: 0.8906
Epoch 13/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1803 - accuracy: 0.9340 - val_loss: 0.3143 - val_accuracy: 0.8912
Epoch 14/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1695 - accuracy: 0.9373 - val_loss: 0.3307 - val_accuracy: 0.8896
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_13_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3337 - accuracy: 0.8836

[0.3336751461029053, 0.8835999965667725]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.009</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">model_with_momentum = build_model(momentum)</span><br><span class="line">history = train_model(model_with_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 8s 16ms/step - loss: 0.4763 - accuracy: 0.8306 - val_loss: 0.3680 - val_accuracy: 0.8668
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3401 - accuracy: 0.8759 - val_loss: 0.3340 - val_accuracy: 0.8764
Epoch 3/100
430/430 [==============================] - 6s 13ms/step - loss: 0.3024 - accuracy: 0.8875 - val_loss: 0.3413 - val_accuracy: 0.8802
Epoch 4/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2754 - accuracy: 0.8982 - val_loss: 0.3193 - val_accuracy: 0.8828
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2497 - accuracy: 0.9059 - val_loss: 0.3031 - val_accuracy: 0.8896
Epoch 6/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2333 - accuracy: 0.9141 - val_loss: 0.3073 - val_accuracy: 0.8860
Epoch 7/100
430/430 [==============================] - 5s 12ms/step - loss: 0.2164 - accuracy: 0.9181 - val_loss: 0.3164 - val_accuracy: 0.8874
Epoch 8/100
430/430 [==============================] - 6s 13ms/step - loss: 0.2020 - accuracy: 0.9241 - val_loss: 0.3006 - val_accuracy: 0.8922
Epoch 9/100
430/430 [==============================] - 7s 15ms/step - loss: 0.1873 - accuracy: 0.9299 - val_loss: 0.3126 - val_accuracy: 0.8906
Epoch 10/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1764 - accuracy: 0.9322 - val_loss: 0.3361 - val_accuracy: 0.8880
Epoch 11/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1689 - accuracy: 0.9363 - val_loss: 0.3185 - val_accuracy: 0.8922
Epoch 12/100
430/430 [==============================] - 7s 16ms/step - loss: 0.1582 - accuracy: 0.9418 - val_loss: 0.3250 - val_accuracy: 0.8908
Epoch 13/100
430/430 [==============================] - 7s 16ms/step - loss: 0.1489 - accuracy: 0.9450 - val_loss: 0.3271 - val_accuracy: 0.8936
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_15_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3313 - accuracy: 0.8851

[0.3312726616859436, 0.8851000070571899]
</code></pre><h1 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">nesterov_momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line">model_with_nesterov_momentum = build_model(nesterov_momentum)</span><br><span class="line">history = train_model(model_with_nesterov_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 8s 16ms/step - loss: 0.6741 - accuracy: 0.7723 - val_loss: 0.4556 - val_accuracy: 0.8396
Epoch 2/100
430/430 [==============================] - 7s 16ms/step - loss: 0.4430 - accuracy: 0.8431 - val_loss: 0.3998 - val_accuracy: 0.8572
Epoch 3/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3965 - accuracy: 0.8588 - val_loss: 0.3732 - val_accuracy: 0.8666
Epoch 4/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3683 - accuracy: 0.8685 - val_loss: 0.3587 - val_accuracy: 0.8720
Epoch 5/100
430/430 [==============================] - 8s 18ms/step - loss: 0.3436 - accuracy: 0.8774 - val_loss: 0.3504 - val_accuracy: 0.8762
Epoch 6/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3303 - accuracy: 0.8792 - val_loss: 0.3417 - val_accuracy: 0.8774
Epoch 7/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3147 - accuracy: 0.8866 - val_loss: 0.3326 - val_accuracy: 0.8810
Epoch 8/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3030 - accuracy: 0.8896 - val_loss: 0.3270 - val_accuracy: 0.8808
Epoch 9/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2903 - accuracy: 0.8928 - val_loss: 0.3225 - val_accuracy: 0.8818
Epoch 10/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2798 - accuracy: 0.8981 - val_loss: 0.3220 - val_accuracy: 0.8806
Epoch 11/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2701 - accuracy: 0.9021 - val_loss: 0.3202 - val_accuracy: 0.8820
Epoch 12/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2618 - accuracy: 0.9040 - val_loss: 0.3179 - val_accuracy: 0.8822
Epoch 13/100
430/430 [==============================] - 6s 13ms/step - loss: 0.2544 - accuracy: 0.9069 - val_loss: 0.3141 - val_accuracy: 0.8836
Epoch 14/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2456 - accuracy: 0.9099 - val_loss: 0.3166 - val_accuracy: 0.8866
Epoch 15/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2366 - accuracy: 0.9145 - val_loss: 0.3172 - val_accuracy: 0.8844
Epoch 16/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2320 - accuracy: 0.9152 - val_loss: 0.3188 - val_accuracy: 0.8852
Epoch 17/100
430/430 [==============================] - 6s 13ms/step - loss: 0.2233 - accuracy: 0.9190 - val_loss: 0.3167 - val_accuracy: 0.8852
Epoch 18/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2177 - accuracy: 0.9209 - val_loss: 0.3146 - val_accuracy: 0.8870
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_nesterov_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_18_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 6ms/step - loss: 0.3416 - accuracy: 0.8767

[0.34159716963768005, 0.8766999840736389]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">nesterov_momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.003</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line">model_with_nesterov_momentum = build_model(nesterov_momentum)</span><br><span class="line">history = train_model(model_with_nesterov_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 8s 15ms/step - loss: 0.5403 - accuracy: 0.8117 - val_loss: 0.3853 - val_accuracy: 0.8630
Epoch 2/100
430/430 [==============================] - 6s 15ms/step - loss: 0.3730 - accuracy: 0.8651 - val_loss: 0.3493 - val_accuracy: 0.8736
Epoch 3/100
430/430 [==============================] - 7s 15ms/step - loss: 0.3319 - accuracy: 0.8785 - val_loss: 0.3349 - val_accuracy: 0.8796
Epoch 4/100
430/430 [==============================] - 8s 19ms/step - loss: 0.3043 - accuracy: 0.8888 - val_loss: 0.3262 - val_accuracy: 0.8786
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2790 - accuracy: 0.8977 - val_loss: 0.3171 - val_accuracy: 0.8876
Epoch 6/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2642 - accuracy: 0.9030 - val_loss: 0.3163 - val_accuracy: 0.8830
Epoch 7/100
430/430 [==============================] - 7s 17ms/step - loss: 0.2465 - accuracy: 0.9098 - val_loss: 0.3131 - val_accuracy: 0.8856
Epoch 8/100
430/430 [==============================] - 8s 20ms/step - loss: 0.2321 - accuracy: 0.9139 - val_loss: 0.3107 - val_accuracy: 0.8886
Epoch 9/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2171 - accuracy: 0.9194 - val_loss: 0.3058 - val_accuracy: 0.8898
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2052 - accuracy: 0.9235 - val_loss: 0.3140 - val_accuracy: 0.8894
Epoch 11/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1953 - accuracy: 0.9275 - val_loss: 0.3126 - val_accuracy: 0.8906
Epoch 12/100
430/430 [==============================] - 8s 19ms/step - loss: 0.1836 - accuracy: 0.9327 - val_loss: 0.3174 - val_accuracy: 0.8902
Epoch 13/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1764 - accuracy: 0.9350 - val_loss: 0.3170 - val_accuracy: 0.8896
Epoch 14/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1651 - accuracy: 0.9396 - val_loss: 0.3289 - val_accuracy: 0.8906
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_nesterov_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_20_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3359 - accuracy: 0.8837

[0.3358677625656128, 0.8837000131607056]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">nesterov_momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.009</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line">model_with_nesterov_momentum = build_model(nesterov_momentum)</span><br><span class="line">history = train_model(model_with_nesterov_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 10s 19ms/step - loss: 0.4669 - accuracy: 0.8332 - val_loss: 0.3616 - val_accuracy: 0.8674
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3346 - accuracy: 0.8781 - val_loss: 0.3282 - val_accuracy: 0.8820
Epoch 3/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2964 - accuracy: 0.8901 - val_loss: 0.3420 - val_accuracy: 0.8800
Epoch 4/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2682 - accuracy: 0.9017 - val_loss: 0.3156 - val_accuracy: 0.8828
Epoch 5/100
430/430 [==============================] - 9s 21ms/step - loss: 0.2442 - accuracy: 0.9083 - val_loss: 0.2981 - val_accuracy: 0.8892
Epoch 6/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2266 - accuracy: 0.9159 - val_loss: 0.3098 - val_accuracy: 0.8854
Epoch 7/100
430/430 [==============================] - 7s 17ms/step - loss: 0.2102 - accuracy: 0.9207 - val_loss: 0.3190 - val_accuracy: 0.8914
Epoch 8/100
430/430 [==============================] - 8s 18ms/step - loss: 0.1951 - accuracy: 0.9265 - val_loss: 0.3074 - val_accuracy: 0.8900
Epoch 9/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1807 - accuracy: 0.9332 - val_loss: 0.3180 - val_accuracy: 0.8928
Epoch 10/100
430/430 [==============================] - 9s 20ms/step - loss: 0.1696 - accuracy: 0.9360 - val_loss: 0.3292 - val_accuracy: 0.8926
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_nesterov_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_22_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3270 - accuracy: 0.8803

[0.3269873261451721, 0.880299985408783]
</code></pre><h1 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_rmsprop = build_model(rmsprop)</span><br><span class="line">history = train_model(model_with_rmsprop)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 11s 20ms/step - loss: 0.4623 - accuracy: 0.8330 - val_loss: 0.3530 - val_accuracy: 0.8734
Epoch 2/100
430/430 [==============================] - 8s 19ms/step - loss: 0.3422 - accuracy: 0.8730 - val_loss: 0.3366 - val_accuracy: 0.8786
Epoch 3/100
430/430 [==============================] - 9s 20ms/step - loss: 0.3019 - accuracy: 0.8869 - val_loss: 0.3575 - val_accuracy: 0.8690
Epoch 4/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2734 - accuracy: 0.8982 - val_loss: 0.3104 - val_accuracy: 0.8848
Epoch 5/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2479 - accuracy: 0.9058 - val_loss: 0.2948 - val_accuracy: 0.8924
Epoch 6/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2303 - accuracy: 0.9139 - val_loss: 0.3111 - val_accuracy: 0.8890
Epoch 7/100
430/430 [==============================] - 9s 21ms/step - loss: 0.2133 - accuracy: 0.9207 - val_loss: 0.3307 - val_accuracy: 0.8870
Epoch 8/100
430/430 [==============================] - 9s 20ms/step - loss: 0.1970 - accuracy: 0.9259 - val_loss: 0.3165 - val_accuracy: 0.8876
Epoch 9/100
430/430 [==============================] - 9s 20ms/step - loss: 0.1806 - accuracy: 0.9318 - val_loss: 0.3165 - val_accuracy: 0.8942
Epoch 10/100
430/430 [==============================] - 8s 19ms/step - loss: 0.1691 - accuracy: 0.9363 - val_loss: 0.3131 - val_accuracy: 0.9012
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_rmsprop.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_25_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3321 - accuracy: 0.8803

[0.332133412361145, 0.880299985408783]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_rmsprop = build_model(rmsprop)</span><br><span class="line">history = train_model(model_with_rmsprop)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 10s 20ms/step - loss: 0.5125 - accuracy: 0.8139 - val_loss: 0.4109 - val_accuracy: 0.8638 - loss:
Epoch 2/100
430/430 [==============================] - 8s 19ms/step - loss: 0.3701 - accuracy: 0.8647 - val_loss: 0.3595 - val_accuracy: 0.8742
Epoch 3/100
430/430 [==============================] - 8s 20ms/step - loss: 0.3252 - accuracy: 0.8812 - val_loss: 0.3858 - val_accuracy: 0.8660
Epoch 4/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2951 - accuracy: 0.8920 - val_loss: 0.3519 - val_accuracy: 0.8740
Epoch 5/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2694 - accuracy: 0.9001 - val_loss: 0.3025 - val_accuracy: 0.8908
Epoch 6/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2508 - accuracy: 0.9078 - val_loss: 0.3159 - val_accuracy: 0.8904
Epoch 7/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2349 - accuracy: 0.9139 - val_loss: 0.3265 - val_accuracy: 0.8846
Epoch 8/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2176 - accuracy: 0.9212 - val_loss: 0.3428 - val_accuracy: 0.8858
Epoch 9/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2043 - accuracy: 0.9249 - val_loss: 0.3161 - val_accuracy: 0.8964
Epoch 10/100
430/430 [==============================] - 8s 19ms/step - loss: 0.1905 - accuracy: 0.9296 - val_loss: 0.3248 - val_accuracy: 0.8964
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_rmsprop.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_27_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3396 - accuracy: 0.8794

[0.33956459164619446, 0.8794000148773193]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">0.009</span>)</span><br><span class="line">model_with_rmsprop = build_model(rmsprop)</span><br><span class="line">history = train_model(model_with_rmsprop)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 10s 20ms/step - loss: 0.5813 - accuracy: 0.7944 - val_loss: 0.4772 - val_accuracy: 0.8612
Epoch 2/100
430/430 [==============================] - 9s 20ms/step - loss: 0.3879 - accuracy: 0.8614 - val_loss: 0.4010 - val_accuracy: 0.8702
Epoch 3/100
430/430 [==============================] - 9s 20ms/step - loss: 0.3422 - accuracy: 0.8759 - val_loss: 0.4354 - val_accuracy: 0.8574
Epoch 4/100
430/430 [==============================] - 8s 19ms/step - loss: 0.3121 - accuracy: 0.8870 - val_loss: 0.3847 - val_accuracy: 0.8690
Epoch 5/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2860 - accuracy: 0.8958 - val_loss: 0.3343 - val_accuracy: 0.8824
Epoch 6/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2694 - accuracy: 0.9021 - val_loss: 0.3159 - val_accuracy: 0.8862
Epoch 7/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2546 - accuracy: 0.9061 - val_loss: 0.3193 - val_accuracy: 0.8884
Epoch 8/100
430/430 [==============================] - 8s 20ms/step - loss: 0.2413 - accuracy: 0.9114 - val_loss: 0.3222 - val_accuracy: 0.8860
Epoch 9/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2282 - accuracy: 0.9165 - val_loss: 0.3308 - val_accuracy: 0.8856
Epoch 10/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2149 - accuracy: 0.9213 - val_loss: 0.3389 - val_accuracy: 0.8920
Epoch 11/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2079 - accuracy: 0.9230 - val_loss: 0.3290 - val_accuracy: 0.8928
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_rmsprop.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_29_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3519 - accuracy: 0.8817

[0.3518962860107422, 0.8816999793052673]
</code></pre><h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">adam = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_adam = build_model(adam)</span><br><span class="line">history = train_model(model_with_adam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 14ms/step - loss: 0.4501 - accuracy: 0.8382 - val_loss: 0.3560 - val_accuracy: 0.8696
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3363 - accuracy: 0.8768 - val_loss: 0.3209 - val_accuracy: 0.8856
Epoch 3/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2979 - accuracy: 0.8889 - val_loss: 0.3466 - val_accuracy: 0.8708
Epoch 4/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2736 - accuracy: 0.8976 - val_loss: 0.3208 - val_accuracy: 0.8808
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2500 - accuracy: 0.9059 - val_loss: 0.2769 - val_accuracy: 0.8962
Epoch 6/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2352 - accuracy: 0.9120 - val_loss: 0.2921 - val_accuracy: 0.8916
Epoch 7/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2159 - accuracy: 0.9192 - val_loss: 0.3060 - val_accuracy: 0.8930
Epoch 8/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2000 - accuracy: 0.9256 - val_loss: 0.3018 - val_accuracy: 0.8926
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1862 - accuracy: 0.9296 - val_loss: 0.2977 - val_accuracy: 0.8950
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1744 - accuracy: 0.9339 - val_loss: 0.3132 - val_accuracy: 0.8966
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_adam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_32_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3160 - accuracy: 0.8846

[0.31602275371551514, 0.8845999836921692]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">adam = keras.optimizers.Adam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_adam = build_model(adam)</span><br><span class="line">history = train_model(model_with_adam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 14ms/step - loss: 0.4631 - accuracy: 0.8328 - val_loss: 0.3626 - val_accuracy: 0.8640
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3586 - accuracy: 0.8675 - val_loss: 0.3419 - val_accuracy: 0.8762
Epoch 3/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3208 - accuracy: 0.8804 - val_loss: 0.3707 - val_accuracy: 0.8658
Epoch 4/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2975 - accuracy: 0.8901 - val_loss: 0.3285 - val_accuracy: 0.8802
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2699 - accuracy: 0.8980 - val_loss: 0.2971 - val_accuracy: 0.8898
Epoch 6/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2563 - accuracy: 0.9045 - val_loss: 0.3174 - val_accuracy: 0.8864
Epoch 7/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2437 - accuracy: 0.9086 - val_loss: 0.3319 - val_accuracy: 0.8888
Epoch 8/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2282 - accuracy: 0.9155 - val_loss: 0.2966 - val_accuracy: 0.8944
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2144 - accuracy: 0.9193 - val_loss: 0.3209 - val_accuracy: 0.8946
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2017 - accuracy: 0.9242 - val_loss: 0.3258 - val_accuracy: 0.8946
Epoch 11/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1937 - accuracy: 0.9283 - val_loss: 0.3211 - val_accuracy: 0.8926
Epoch 12/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1821 - accuracy: 0.9323 - val_loss: 0.3170 - val_accuracy: 0.8936
Epoch 13/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1746 - accuracy: 0.9346 - val_loss: 0.3107 - val_accuracy: 0.9008
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_adam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_34_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8778

[0.33767154812812805, 0.8777999877929688]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">adam = keras.optimizers.Adam(learning_rate=<span class="number">0.009</span>)</span><br><span class="line">model_with_adam = build_model(adam)</span><br><span class="line">history = train_model(model_with_adam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 14ms/step - loss: 0.4959 - accuracy: 0.8225 - val_loss: 0.4709 - val_accuracy: 0.8394
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3831 - accuracy: 0.8603 - val_loss: 0.3787 - val_accuracy: 0.8720
Epoch 3/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3473 - accuracy: 0.8739 - val_loss: 0.4398 - val_accuracy: 0.8482
Epoch 4/100
430/430 [==============================] - 6s 13ms/step - loss: 0.3249 - accuracy: 0.8811 - val_loss: 0.3315 - val_accuracy: 0.8800
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2948 - accuracy: 0.8922 - val_loss: 0.3096 - val_accuracy: 0.8852
Epoch 6/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2874 - accuracy: 0.8955 - val_loss: 0.3054 - val_accuracy: 0.8894
Epoch 7/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2701 - accuracy: 0.9006 - val_loss: 0.3290 - val_accuracy: 0.8862
Epoch 8/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2578 - accuracy: 0.9050 - val_loss: 0.2897 - val_accuracy: 0.8884
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2432 - accuracy: 0.9101 - val_loss: 0.3229 - val_accuracy: 0.8896
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2356 - accuracy: 0.9138 - val_loss: 0.3196 - val_accuracy: 0.8868
Epoch 11/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2280 - accuracy: 0.9151 - val_loss: 0.3303 - val_accuracy: 0.8858
Epoch 12/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2181 - accuracy: 0.9202 - val_loss: 0.3581 - val_accuracy: 0.8856
Epoch 13/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2088 - accuracy: 0.9235 - val_loss: 0.3011 - val_accuracy: 0.9010
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_adam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_36_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.8798

[0.3372490406036377, 0.879800021648407]
</code></pre><h1 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">nadam = keras.optimizers.Nadam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_nadam = build_model(nadam)</span><br><span class="line">history = train_model(model_with_nadam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 14s 27ms/step - loss: 0.4456 - accuracy: 0.8412 - val_loss: 0.3369 - val_accuracy: 0.8772
Epoch 2/100
430/430 [==============================] - 11s 27ms/step - loss: 0.3263 - accuracy: 0.8799 - val_loss: 0.3352 - val_accuracy: 0.8774oss: 0.3271 - accuracy: 0.87 - ETA: 0s - loss: 0
Epoch 3/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2903 - accuracy: 0.8909 - val_loss: 0.3364 - val_accuracy: 0.8744
Epoch 4/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2631 - accuracy: 0.9024 - val_loss: 0.3149 - val_accuracy: 0.8882
Epoch 5/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2388 - accuracy: 0.9103 - val_loss: 0.2889 - val_accuracy: 0.8912
Epoch 6/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2219 - accuracy: 0.9165 - val_loss: 0.3054 - val_accuracy: 0.8888
Epoch 7/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2070 - accuracy: 0.9219 - val_loss: 0.3177 - val_accuracy: 0.8852
Epoch 8/100
430/430 [==============================] - 11s 26ms/step - loss: 0.1913 - accuracy: 0.9279 - val_loss: 0.3169 - val_accuracy: 0.8878
Epoch 9/100
430/430 [==============================] - 11s 26ms/step - loss: 0.1759 - accuracy: 0.9336 - val_loss: 0.3141 - val_accuracy: 0.8952
Epoch 10/100
430/430 [==============================] - 11s 26ms/step - loss: 0.1635 - accuracy: 0.9376 - val_loss: 0.3068 - val_accuracy: 0.8994
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_nadam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_39_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8826

[0.3237777650356293, 0.8826000094413757]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">nadam = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_nadam = build_model(nadam)</span><br><span class="line">history = train_model(model_with_nadam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 14s 27ms/step - loss: 0.4557 - accuracy: 0.8346 - val_loss: 0.3408 - val_accuracy: 0.8752
Epoch 2/100
430/430 [==============================] - 11s 26ms/step - loss: 0.3419 - accuracy: 0.8734 - val_loss: 0.3310 - val_accuracy: 0.8770
Epoch 3/100
430/430 [==============================] - 11s 26ms/step - loss: 0.3072 - accuracy: 0.8852 - val_loss: 0.3605 - val_accuracy: 0.8708
Epoch 4/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2821 - accuracy: 0.8950 - val_loss: 0.3365 - val_accuracy: 0.8752
Epoch 5/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2581 - accuracy: 0.9026 - val_loss: 0.2931 - val_accuracy: 0.8932
Epoch 6/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2437 - accuracy: 0.9096 - val_loss: 0.3218 - val_accuracy: 0.8860
Epoch 7/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2304 - accuracy: 0.9137 - val_loss: 0.3155 - val_accuracy: 0.8890
Epoch 8/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2170 - accuracy: 0.9187 - val_loss: 0.3146 - val_accuracy: 0.8848
Epoch 9/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2012 - accuracy: 0.9241 - val_loss: 0.3021 - val_accuracy: 0.8976
Epoch 10/100
430/430 [==============================] - 11s 26ms/step - loss: 0.1881 - accuracy: 0.9287 - val_loss: 0.3103 - val_accuracy: 0.8944
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_nadam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_41_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8797

[0.3318653404712677, 0.8797000050544739]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器, 生成模型, 训练模型</span></span><br><span class="line">nadam = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)</span><br><span class="line">model_with_nadam = build_model(nadam)</span><br><span class="line">history = train_model(model_with_nadam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 13s 25ms/step - loss: 0.4911 - accuracy: 0.8221 - val_loss: 0.3915 - val_accuracy: 0.8698
Epoch 2/100
430/430 [==============================] - 11s 26ms/step - loss: 0.3658 - accuracy: 0.8668 - val_loss: 0.3957 - val_accuracy: 0.8624
Epoch 3/100
430/430 [==============================] - 11s 25ms/step - loss: 0.3326 - accuracy: 0.8770 - val_loss: 0.4334 - val_accuracy: 0.8420
Epoch 4/100
430/430 [==============================] - 11s 26ms/step - loss: 0.3062 - accuracy: 0.8876 - val_loss: 0.3499 - val_accuracy: 0.8732
Epoch 5/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2841 - accuracy: 0.8940 - val_loss: 0.3356 - val_accuracy: 0.87380 - accuracy: 0.89
Epoch 6/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2701 - accuracy: 0.8994 - val_loss: 0.3503 - val_accuracy: 0.8818
Epoch 7/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2602 - accuracy: 0.9035 - val_loss: 0.3306 - val_accuracy: 0.8786
Epoch 8/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2445 - accuracy: 0.9097 - val_loss: 0.3197 - val_accuracy: 0.8818
Epoch 9/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2328 - accuracy: 0.9140 - val_loss: 0.3531 - val_accuracy: 0.8774
Epoch 10/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2213 - accuracy: 0.9181 - val_loss: 0.3308 - val_accuracy: 0.8920
Epoch 11/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2129 - accuracy: 0.9201 - val_loss: 0.3762 - val_accuracy: 0.8728
Epoch 12/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2044 - accuracy: 0.9247 - val_loss: 0.3339 - val_accuracy: 0.8828
Epoch 13/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1946 - accuracy: 0.9283 - val_loss: 0.3053 - val_accuracy: 0.8970
Epoch 14/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1893 - accuracy: 0.9296 - val_loss: 0.3219 - val_accuracy: 0.8908
Epoch 15/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1775 - accuracy: 0.9345 - val_loss: 0.3630 - val_accuracy: 0.8820
Epoch 16/100
430/430 [==============================] - 11s 24ms/step - loss: 0.1723 - accuracy: 0.9365 - val_loss: 0.3538 - val_accuracy: 0.8884
Epoch 17/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1613 - accuracy: 0.9408 - val_loss: 0.3284 - val_accuracy: 0.8988TA: 3s - -
Epoch 18/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1573 - accuracy: 0.9410 - val_loss: 0.3598 - val_accuracy: 0.8862
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_with_nadam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_43_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3452 - accuracy: 0.8880

[0.34519216418266296, 0.8880000114440918]
</code></pre><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Optimizer</th>
<th>Learning Rate</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Evaluation on x_test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Momentum</td>
<td>0.001</td>
<td>6s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8763</td>
</tr>
<tr>
<td>Momentum</td>
<td>0.003</td>
<td>6s/epoch</td>
<td>9 epochs</td>
<td>accuracy=0.8836</td>
</tr>
<tr>
<td>Momentum</td>
<td>0.009</td>
<td>6s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8851</td>
</tr>
<tr>
<td>Nesterov Momentum</td>
<td>0.001</td>
<td>7s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8767</td>
</tr>
<tr>
<td>Nesterov Momentum</td>
<td>0.003</td>
<td>7s/epoch</td>
<td>9 epochs</td>
<td>accuracy=0.8837</td>
</tr>
<tr>
<td>Nesterov Momentum</td>
<td>0.009</td>
<td>7s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8803</td>
</tr>
<tr>
<td>RMSprop</td>
<td>0.001</td>
<td>9s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8803</td>
</tr>
<tr>
<td>RMSprop</td>
<td>0.003</td>
<td>9s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8794</td>
</tr>
<tr>
<td>RMSprop</td>
<td>0.009</td>
<td>9s/epoch</td>
<td>6 epochs</td>
<td>accuracy=0.8817</td>
</tr>
<tr>
<td>Adam</td>
<td>0.001</td>
<td>6s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8846</td>
</tr>
<tr>
<td>Adam</td>
<td>0.003</td>
<td>6s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8778</td>
</tr>
<tr>
<td>Adam</td>
<td>0.009</td>
<td>6s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8798</td>
</tr>
<tr>
<td>Nadam</td>
<td>0.001</td>
<td>11s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8826</td>
</tr>
<tr>
<td>Nadam</td>
<td>0.003</td>
<td>11s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8797</td>
</tr>
<tr>
<td>Nadam</td>
<td>0.009</td>
<td>11s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8880</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>Momentum 和 Nesterov Momentum 在训练速度和模型性能上相差不多, 在选取了<strong>恰当的学习率</strong>时, Nesterov Momentum <strong>收敛很快</strong>.</li>
<li>RMSprop 在训练速度上稍微慢于 Nesterov Momentum, 但<strong>收敛速度很快</strong>, 模型性能也不错.</li>
<li>Adam 的训练速度和收敛速度都不错, 模型性能也不赖.</li>
<li>Nadam 的<strong>训练速度明显慢于其他优化器</strong>, 但在<strong>恰当的学习率下, 收敛速度很快</strong>, 模型性能也不错.   </li>
</ol>
<p>🐒 可以改进的点: 或许针对不同优化器, 应该通过搜索超参数空间选取最佳学习率, 再比较各优化器的性能差异.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>总结 TensorFlow 中常用的 resize 图像的方法</title>
    <url>/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>下面以 2 张样本图像为一个 batch, 展示以下四种 resize 图像方法的异同:</p>
<ol>
<li><p>tf.image.resize()</p>
</li>
<li><p>tf.image.resize_with_pad()</p>
</li>
<li><p>tf.image.resize_with_crop_or_pad()</p>
</li>
<li><p>tf.image.crop_and_resize()</p>
<span id="more"></span>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_sample_image</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载 &amp; 展示样本图像</span></span><br><span class="line">china = load_sample_image(<span class="string">&quot;china.jpg&quot;</span>) / <span class="number">255</span></span><br><span class="line">flower = load_sample_image(<span class="string">&quot;flower.jpg&quot;</span>) / <span class="number">255</span></span><br><span class="line">images = np.array([china, flower])</span><br><span class="line"></span><br><span class="line">batch_size, height, width, channels = images.shape</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;样本图像的尺寸均为 (<span class="subst">&#123;height&#125;</span>, <span class="subst">&#123;width&#125;</span>)&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_images</span>(<span class="params">images</span>):</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow(images[i])</span><br><span class="line">        plt.axis(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plot_images(images)</span><br></pre></td></tr></table></figure>
<pre><code>样本图像的尺寸均为 (427, 640)
</code></pre><p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_3_1.png" alt="png"></p>
<h1 id="resize"><a href="#resize" class="headerlink" title="resize"></a>resize</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.基本用法</span></span><br><span class="line">imgs_resized = tf.image.resize(images, [<span class="number">256</span>, <span class="number">256</span>])</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_5_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.强制保持宽高比的写法</span></span><br><span class="line">imgs_resized = tf.image.resize(images, [<span class="number">256</span>, <span class="number">256</span>], preserve_aspect_ratio=<span class="literal">True</span>)    </span><br><span class="line">display(imgs_resized.shape)</span><br><span class="line"></span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<pre><code>TensorShape([2, 171, 256, 3])
</code></pre><p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_6_1.png" alt="png"></p>
<h2 id="Remark"><a href="#Remark" class="headerlink" title="Remark"></a>Remark</h2><ol>
<li>resize() 中接收的 image 的<strong>像素强度</strong>应该在 [0, 1] 中;</li>
<li>resize() 无法保持 image 的<strong>宽高比</strong>;</li>
<li>设置参数 preserve_aspect_ratio=True 以<strong>强制保持宽高比</strong>时, 得到图像的 shape 不一定能与 size 参数一致.</li>
</ol>
<h1 id="resize-with-pad"><a href="#resize-with-pad" class="headerlink" title="resize_with_pad"></a>resize_with_pad</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imgs_resized = tf.image.resize_with_pad(images, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_10_0.png" alt="png"></p>
<h2 id="Remark-1"><a href="#Remark-1" class="headerlink" title="Remark"></a>Remark</h2><ol>
<li>resize_with_pad() 中设置图像<strong>目标宽高的写法</strong>与 resize() 不同;</li>
<li>resize_with_pad() <strong>总是保持宽高比</strong>, 为此可能会在图像周围<strong>添加黑色带子</strong>.</li>
</ol>
<h1 id="resize-with-crop-or-pad"><a href="#resize-with-crop-or-pad" class="headerlink" title="resize_with_crop_or_pad"></a>resize_with_crop_or_pad</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.目标尺寸 &lt; 原始尺寸 = [中心裁剪]</span></span><br><span class="line">imgs_resized = tf.image.resize_with_crop_or_pad(images, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_14_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.目标尺寸 &gt; 原始尺寸 = [添加黑色带子]</span></span><br><span class="line">imgs_resized = tf.image.resize_with_crop_or_pad(images, <span class="number">256</span>*<span class="number">3</span>, <span class="number">256</span>*<span class="number">3</span>)</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_15_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.目标尺寸 &amp; 原始尺寸 无法简单比较大小时 = [裁剪 + 黑带]</span></span><br><span class="line">imgs_resized = tf.image.resize_with_crop_or_pad(images, <span class="number">256</span>*<span class="number">2</span>, <span class="number">256</span>*<span class="number">2</span>)</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_16_0.png" alt="png"></p>
<h2 id="Remark-2"><a href="#Remark-2" class="headerlink" title="Remark"></a>Remark</h2><ol>
<li>resize_with_crop_or_pad() 设置<strong>目标宽高的写法</strong>与 resize_with_pad() 相同;</li>
<li>resize_with_crop_or_pad() <strong>总是保持宽高比</strong>: 采取 <strong>中心裁剪</strong> or <strong>添加黑色带子</strong> or <strong>二者结合</strong> 的做法. </li>
</ol>
<h1 id="crop-and-resize"><a href="#crop-and-resize" class="headerlink" title="crop_and_resize"></a>crop_and_resize</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.恰当的裁剪框</span></span><br><span class="line">china_box = [<span class="number">0</span>, <span class="number">0.03</span>, <span class="number">1</span>, <span class="number">0.68</span>]    <span class="comment"># 规范化坐标 [y1, x1, y2, x2]</span></span><br><span class="line">flower_box = [<span class="number">0.19</span>, <span class="number">0.26</span>, <span class="number">0.86</span>, <span class="number">0.7</span>]</span><br><span class="line"></span><br><span class="line">imgs_resized = tf.image.crop_and_resize(images, </span><br><span class="line">                                        boxes=[china_box, flower_box], </span><br><span class="line">                                        box_indices=[<span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">                                        crop_size=[<span class="number">256</span>, <span class="number">256</span>])</span><br><span class="line"></span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_20_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.不恰当的裁剪框</span></span><br><span class="line">china_box = [<span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">1.0</span>, <span class="number">0.9</span>]    <span class="comment"># 看得出楼&quot;变瘦了&quot;</span></span><br><span class="line">flower_box = [<span class="number">0.0</span>, <span class="number">0.26</span>, <span class="number">1.0</span>, <span class="number">0.7</span>]   <span class="comment"># 看得出花&quot;变胖了&quot;</span></span><br><span class="line"></span><br><span class="line">imgs_resized = tf.image.crop_and_resize(images, </span><br><span class="line">                                        boxes=[china_box, flower_box], </span><br><span class="line">                                        box_indices=[<span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">                                        crop_size=[<span class="number">256</span>, <span class="number">256</span>])</span><br><span class="line"></span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_21_0.png" alt="png"></p>
<h2 id="Remark-3"><a href="#Remark-3" class="headerlink" title="Remark"></a>Remark</h2><ol>
<li>crop_and_resize() 需要自行指定<strong>裁剪框</strong>, 若选取不当, 则无法保持 resize 后的宽高比;</li>
<li>crop_size 用于指定输出图像的 shape.</li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>各种正则化方法之间的对比</title>
    <url>/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>❓ 如何在各种正则化方法中做出选择?</p>
<ol>
<li><p>EarlyStopping  </p>
</li>
<li><p>l1 &amp; l2 正则化  </p>
</li>
<li><p>Dropout (AlphaDropout)</p>
</li>
<li><p>Batch Normalization (有一定正则化效果)</p>
<span id="more"></span>
</li>
</ol>
<p>※ 下面针对(基本)相同的 &lt;网络架构 &amp; 优化器 &amp; 学习率调度&gt;, 对比不同正则化方法带给模型的影响.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>🔺 针对 Fashion MNIST 数据集, 开展下面的测试.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集 (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist 中数字标签对应的类别名称</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 展示部分训练集实例</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m 行 n 列</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_6_0.png" alt="png"></p>
<p>Remark: 基于训练速度考虑, 下面选用了 batch_size=128 以加速训练, 但使用更小的 batch_size (如 32) 可能获得更好的模型.</p>
<h1 id="No-Regularization"><a href="#No-Regularization" class="headerlink" title="No Regularization"></a>No Regularization</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">model = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">              optimizer=optimizer, </span><br><span class="line">              metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 8s 16ms/step - loss: 0.8197 - accuracy: 0.7210 - val_loss: 0.5812 - val_accuracy: 0.7936 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 6s 15ms/step - loss: 0.4344 - accuracy: 0.8399 - val_loss: 0.4085 - val_accuracy: 0.8498 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 7s 15ms/step - loss: 0.4009 - accuracy: 0.8525 - val_loss: 0.4098 - val_accuracy: 0.8544 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 6s 15ms/step - loss: 0.3585 - accuracy: 0.8691 - val_loss: 0.3402 - val_accuracy: 0.8764 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 7s 15ms/step - loss: 0.3367 - accuracy: 0.8767 - val_loss: 0.3698 - val_accuracy: 0.8662 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 7s 15ms/step - loss: 0.3209 - accuracy: 0.8835 - val_loss: 0.4160 - val_accuracy: 0.8668 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 7s 15ms/step - loss: 0.3122 - accuracy: 0.8874 - val_loss: 0.3409 - val_accuracy: 0.8800 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 7s 15ms/step - loss: 0.3012 - accuracy: 0.8907 - val_loss: 0.3483 - val_accuracy: 0.8762 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 7s 15ms/step - loss: 0.2365 - accuracy: 0.9120 - val_loss: 0.2883 - val_accuracy: 0.8986 - lr: 0.0010
Epoch 10/25
430/430 [==============================] - 7s 16ms/step - loss: 0.2210 - accuracy: 0.9175 - val_loss: 0.2918 - val_accuracy: 0.8984 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 7s 16ms/step - loss: 0.2133 - accuracy: 0.9199 - val_loss: 0.3090 - val_accuracy: 0.8972 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 7s 16ms/step - loss: 0.2070 - accuracy: 0.9219 - val_loss: 0.3103 - val_accuracy: 0.8938 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 7s 16ms/step - loss: 0.2002 - accuracy: 0.9249 - val_loss: 0.2996 - val_accuracy: 0.9030 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1657 - accuracy: 0.9371 - val_loss: 0.2945 - val_accuracy: 0.9042 - lr: 3.3333e-04
Epoch 15/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1547 - accuracy: 0.9411 - val_loss: 0.3044 - val_accuracy: 0.9060 - lr: 3.3333e-04
Epoch 16/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1481 - accuracy: 0.9445 - val_loss: 0.3096 - val_accuracy: 0.9036 - lr: 3.3333e-04
Epoch 17/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1420 - accuracy: 0.9459 - val_loss: 0.3186 - val_accuracy: 0.9050 - lr: 3.3333e-04
Epoch 18/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1235 - accuracy: 0.9539 - val_loss: 0.3218 - val_accuracy: 0.9092 - lr: 1.1111e-04
Epoch 19/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1184 - accuracy: 0.9554 - val_loss: 0.3327 - val_accuracy: 0.9084 - lr: 1.1111e-04
Epoch 20/25
430/430 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.95 - 7s 16ms/step - loss: 0.1143 - accuracy: 0.9576 - val_loss: 0.3411 - val_accuracy: 0.9074 - lr: 1.1111e-04
Epoch 21/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1107 - accuracy: 0.9591 - val_loss: 0.3515 - val_accuracy: 0.9052 - lr: 1.1111e-04
Epoch 22/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1022 - accuracy: 0.9624 - val_loss: 0.3601 - val_accuracy: 0.9072 - lr: 3.7037e-05
Epoch 23/25
430/430 [==============================] - 7s 16ms/step - loss: 0.0999 - accuracy: 0.9629 - val_loss: 0.3624 - val_accuracy: 0.9042 - lr: 3.7037e-05
Epoch 24/25
430/430 [==============================] - 7s 16ms/step - loss: 0.0984 - accuracy: 0.9639 - val_loss: 0.3709 - val_accuracy: 0.9060 - lr: 3.7037e-05
Epoch 25/25
430/430 [==============================] - 7s 16ms/step - loss: 0.0968 - accuracy: 0.9644 - val_loss: 0.3713 - val_accuracy: 0.9066 - lr: 3.7037e-05
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(model.evaluate(x_train, y_train))</span><br><span class="line">display(model.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_11_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 5s 3ms/step - loss: 0.0929 - accuracy: 0.9661

[0.09290006756782532, 0.9660909175872803]


313/313 [==============================] - 1s 3ms/step - loss: 0.4360 - accuracy: 0.8949

[0.4359566271305084, 0.8949000239372253]
</code></pre><p>Remark: 从上述学习曲线可以看出, 不带任何正则化的模型确实过拟合了训练集.</p>
<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">model_with_BN = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_BN.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_with_BN.add(keras.layers.BatchNormalization())</span><br><span class="line"></span><br><span class="line">model_with_BN.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_BN.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_BN = model_with_BN.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                    validation_data=(x_valid, y_valid),</span><br><span class="line">                                    callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 19s 37ms/step - loss: 0.4899 - accuracy: 0.8245 - val_loss: 0.3639 - val_accuracy: 0.8784 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 16s 37ms/step - loss: 0.3666 - accuracy: 0.8671 - val_loss: 0.3484 - val_accuracy: 0.8772 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 15s 36ms/step - loss: 0.3322 - accuracy: 0.8778 - val_loss: 0.4017 - val_accuracy: 0.8600 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 16s 37ms/step - loss: 0.3066 - accuracy: 0.8888 - val_loss: 0.3411 - val_accuracy: 0.8762 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 16s 37ms/step - loss: 0.2821 - accuracy: 0.8945 - val_loss: 0.2950 - val_accuracy: 0.8922 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 16s 37ms/step - loss: 0.2656 - accuracy: 0.9027 - val_loss: 0.3312 - val_accuracy: 0.8888 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 16s 38ms/step - loss: 0.2543 - accuracy: 0.9072 - val_loss: 0.3196 - val_accuracy: 0.8902 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 16s 37ms/step - loss: 0.2390 - accuracy: 0.9133 - val_loss: 0.3031 - val_accuracy: 0.8898 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 16s 37ms/step - loss: 0.2224 - accuracy: 0.9181 - val_loss: 0.3247 - val_accuracy: 0.8894 - lr: 0.0030
Epoch 10/25
430/430 [==============================] - 16s 38ms/step - loss: 0.1692 - accuracy: 0.9375 - val_loss: 0.2805 - val_accuracy: 0.9052 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 16s 38ms/step - loss: 0.1499 - accuracy: 0.9450 - val_loss: 0.2957 - val_accuracy: 0.9022 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 16s 38ms/step - loss: 0.1399 - accuracy: 0.9487 - val_loss: 0.3072 - val_accuracy: 0.8954 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 16s 38ms/step - loss: 0.1311 - accuracy: 0.9515 - val_loss: 0.2964 - val_accuracy: 0.9006 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 17s 40ms/step - loss: 0.1218 - accuracy: 0.9551 - val_loss: 0.3106 - val_accuracy: 0.9034 - lr: 0.0010
Epoch 15/25
430/430 [==============================] - 17s 39ms/step - loss: 0.0939 - accuracy: 0.9671 - val_loss: 0.3114 - val_accuracy: 0.9078 - lr: 3.3333e-04
Epoch 16/25
430/430 [==============================] - 17s 39ms/step - loss: 0.0830 - accuracy: 0.9705 - val_loss: 0.3309 - val_accuracy: 0.9030 - lr: 3.3333e-04
Epoch 17/25
430/430 [==============================] - 17s 39ms/step - loss: 0.0764 - accuracy: 0.9726 - val_loss: 0.3344 - val_accuracy: 0.9040 - lr: 3.3333e-04
Epoch 18/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0726 - accuracy: 0.9738 - val_loss: 0.3462 - val_accuracy: 0.9032 - lr: 3.3333e-04
Epoch 19/25
430/430 [==============================] - 17s 38ms/step - loss: 0.0603 - accuracy: 0.9789 - val_loss: 0.3595 - val_accuracy: 0.9058 - lr: 1.1111e-04
Epoch 20/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0578 - accuracy: 0.9797 - val_loss: 0.3658 - val_accuracy: 0.9062 - lr: 1.1111e-04
Epoch 21/25
430/430 [==============================] - 16s 37ms/step - loss: 0.0544 - accuracy: 0.9810 - val_loss: 0.3720 - val_accuracy: 0.9056 - lr: 1.1111e-04
Epoch 22/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0535 - accuracy: 0.9815 - val_loss: 0.3803 - val_accuracy: 0.9052 - lr: 1.1111e-04
Epoch 23/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0510 - accuracy: 0.9825 - val_loss: 0.3804 - val_accuracy: 0.9052 - lr: 3.7037e-05
Epoch 24/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0485 - accuracy: 0.9833 - val_loss: 0.3842 - val_accuracy: 0.9070 - lr: 3.7037e-05
Epoch 25/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0480 - accuracy: 0.9832 - val_loss: 0.3861 - val_accuracy: 0.9048 - lr: 3.7037e-05ss: 0.0481 -  - ETA: 0s - loss: 0.0480 - accuracy: 0.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_with_BN.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(model_with_BN.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_BN.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_16_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 10s 6ms/step - loss: 0.0295 - accuracy: 0.9914

[0.029543157666921616, 0.9914363622665405]


313/313 [==============================] - 2s 6ms/step - loss: 0.4273 - accuracy: 0.9016

[0.4273393452167511, 0.9016000032424927]
</code></pre><h1 id="EarlyStopping"><a href="#EarlyStopping" class="headerlink" title="EarlyStopping"></a>EarlyStopping</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">model_with_ES = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_ES.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_with_ES.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_ES.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">8</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">history_with_ES = model_with_ES.fit(x_train, y_train, epochs=<span class="number">25</span>*<span class="number">2</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                    validation_data=(x_valid, y_valid),</span><br><span class="line">                                    callbacks=[lr_scheduler, early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/50
430/430 [==============================] - 9s 17ms/step - loss: 0.8095 - accuracy: 0.7205 - val_loss: 0.5028 - val_accuracy: 0.8152 - lr: 0.0030
Epoch 2/50
430/430 [==============================] - 7s 17ms/step - loss: 0.4347 - accuracy: 0.8411 - val_loss: 0.4048 - val_accuracy: 0.8596 - lr: 0.0030
Epoch 3/50
430/430 [==============================] - 7s 17ms/step - loss: 0.3897 - accuracy: 0.8570 - val_loss: 0.3748 - val_accuracy: 0.8630 - lr: 0.0030
Epoch 4/50
430/430 [==============================] - 7s 16ms/step - loss: 0.3533 - accuracy: 0.8711 - val_loss: 0.3313 - val_accuracy: 0.8804 - lr: 0.0030
Epoch 5/50
430/430 [==============================] - 7s 16ms/step - loss: 0.3341 - accuracy: 0.8781 - val_loss: 0.3669 - val_accuracy: 0.8662 - lr: 0.0030
Epoch 6/50
430/430 [==============================] - 7s 16ms/step - loss: 0.3198 - accuracy: 0.8835 - val_loss: 0.3579 - val_accuracy: 0.8746 - lr: 0.0030
Epoch 7/50
430/430 [==============================] - 7s 16ms/step - loss: 0.3080 - accuracy: 0.8889 - val_loss: 0.3232 - val_accuracy: 0.8852 - lr: 0.0030
Epoch 8/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2951 - accuracy: 0.8922 - val_loss: 0.3478 - val_accuracy: 0.8726 - lr: 0.0030
Epoch 9/50
430/430 [==============================] - 7s 16ms/step - loss: 0.2905 - accuracy: 0.8959 - val_loss: 0.3328 - val_accuracy: 0.8818 - lr: 0.0030
Epoch 10/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2822 - accuracy: 0.8963 - val_loss: 0.3141 - val_accuracy: 0.8928 - lr: 0.0030
Epoch 11/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2712 - accuracy: 0.9028 - val_loss: 0.3353 - val_accuracy: 0.8810 - lr: 0.0030
Epoch 12/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2678 - accuracy: 0.9029 - val_loss: 0.3128 - val_accuracy: 0.8864 - lr: 0.0030
Epoch 13/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2572 - accuracy: 0.9064 - val_loss: 0.3247 - val_accuracy: 0.8894 - lr: 0.0030
Epoch 14/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2548 - accuracy: 0.9078 - val_loss: 0.3044 - val_accuracy: 0.8902 - lr: 0.0030
Epoch 15/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2513 - accuracy: 0.9106 - val_loss: 0.3109 - val_accuracy: 0.8954 - lr: 0.0030
Epoch 16/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2415 - accuracy: 0.9137 - val_loss: 0.3293 - val_accuracy: 0.8880 - lr: 0.0030
Epoch 17/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2472 - accuracy: 0.9120 - val_loss: 0.3558 - val_accuracy: 0.8794 - lr: 0.0030
Epoch 18/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2340 - accuracy: 0.9164 - val_loss: 0.3173 - val_accuracy: 0.8950 - lr: 0.0030
Epoch 19/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1830 - accuracy: 0.9321 - val_loss: 0.2955 - val_accuracy: 0.9052 - lr: 0.0010
Epoch 20/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1649 - accuracy: 0.9390 - val_loss: 0.3008 - val_accuracy: 0.9050 - lr: 0.0010
Epoch 21/50
430/430 [==============================] - 7s 16ms/step - loss: 0.1560 - accuracy: 0.9419 - val_loss: 0.3085 - val_accuracy: 0.9062 - lr: 0.0010
Epoch 22/50
430/430 [==============================] - 7s 16ms/step - loss: 0.1493 - accuracy: 0.9436 - val_loss: 0.3473 - val_accuracy: 0.9038 - lr: 0.0010
Epoch 23/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1445 - accuracy: 0.9462 - val_loss: 0.3221 - val_accuracy: 0.9048 - lr: 0.0010
Epoch 24/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1197 - accuracy: 0.9547 - val_loss: 0.3485 - val_accuracy: 0.9106 - lr: 3.3333e-04
Epoch 25/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1117 - accuracy: 0.9577 - val_loss: 0.3573 - val_accuracy: 0.9098 - lr: 3.3333e-04
Epoch 26/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1064 - accuracy: 0.9594 - val_loss: 0.3843 - val_accuracy: 0.9062 - lr: 3.3333e-04
Epoch 27/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1010 - accuracy: 0.9622 - val_loss: 0.3881 - val_accuracy: 0.9080 - lr: 3.3333e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_with_ES.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(model_with_ES.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_ES.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_20_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 6s 3ms/step - loss: 0.1625 - accuracy: 0.9382

[0.16253036260604858, 0.9381999969482422]


313/313 [==============================] - 1s 4ms/step - loss: 0.3310 - accuracy: 0.8977

[0.3310108780860901, 0.8977000117301941]
</code></pre><h1 id="l1-amp-l2-Regularization"><a href="#l1-amp-l2-Regularization" class="headerlink" title="l1 &amp; l2 Regularization"></a>l1 &amp; l2 Regularization</h1><h2 id="l1"><a href="#l1" class="headerlink" title="l1()"></a>l1()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">model_with_l1_reg = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_l1_reg.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, </span><br><span class="line">                                             kernel_regularizer=keras.regularizers.l1(l1=<span class="number">3e-6</span>)))</span><br><span class="line"></span><br><span class="line">model_with_l1_reg.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_l1_reg.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                          optimizer=optimizer, </span><br><span class="line">                          metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_l1_reg = model_with_l1_reg.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                            validation_data=(x_valid, y_valid),</span><br><span class="line">                                            callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 9s 18ms/step - loss: 0.7871 - accuracy: 0.7362 - val_loss: 0.4988 - val_accuracy: 0.8262 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 8s 18ms/step - loss: 0.4610 - accuracy: 0.8381 - val_loss: 0.3974 - val_accuracy: 0.8626 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 8s 18ms/step - loss: 0.4129 - accuracy: 0.8566 - val_loss: 0.4618 - val_accuracy: 0.8364 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3745 - accuracy: 0.8711 - val_loss: 0.4005 - val_accuracy: 0.8650 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3581 - accuracy: 0.8765 - val_loss: 0.3781 - val_accuracy: 0.8750 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3447 - accuracy: 0.8819 - val_loss: 0.3859 - val_accuracy: 0.8792 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3369 - accuracy: 0.8861 - val_loss: 0.3767 - val_accuracy: 0.8768 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3350 - accuracy: 0.8866 - val_loss: 0.4536 - val_accuracy: 0.8540 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3297 - accuracy: 0.8891 - val_loss: 0.3566 - val_accuracy: 0.8804 - lr: 0.0030
Epoch 10/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3097 - accuracy: 0.8958 - val_loss: 0.3362 - val_accuracy: 0.8928 - lr: 0.0030
Epoch 11/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2984 - accuracy: 0.8985 - val_loss: 0.3772 - val_accuracy: 0.8768 - lr: 0.0030
Epoch 12/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2943 - accuracy: 0.9015 - val_loss: 0.3510 - val_accuracy: 0.8840 - lr: 0.0030
Epoch 13/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2871 - accuracy: 0.9032 - val_loss: 0.3567 - val_accuracy: 0.8878 - lr: 0.0030
Epoch 14/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2776 - accuracy: 0.9068 - val_loss: 0.3540 - val_accuracy: 0.8824 - lr: 0.0030
Epoch 15/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2213 - accuracy: 0.9257 - val_loss: 0.3112 - val_accuracy: 0.8996 - lr: 0.0010
Epoch 16/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2056 - accuracy: 0.9315 - val_loss: 0.3219 - val_accuracy: 0.9006 - lr: 0.0010
Epoch 17/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1982 - accuracy: 0.9326 - val_loss: 0.3345 - val_accuracy: 0.8972 - lr: 0.0010
Epoch 18/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1934 - accuracy: 0.9350 - val_loss: 0.3232 - val_accuracy: 0.8968 - lr: 0.0010
Epoch 19/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1875 - accuracy: 0.9375 - val_loss: 0.3313 - val_accuracy: 0.9028 - lr: 0.0010
Epoch 20/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1584 - accuracy: 0.9486 - val_loss: 0.3293 - val_accuracy: 0.9068 - lr: 3.3333e-04
Epoch 21/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1479 - accuracy: 0.9521 - val_loss: 0.3383 - val_accuracy: 0.9060 - lr: 3.3333e-04
Epoch 22/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1422 - accuracy: 0.9536 - val_loss: 0.3450 - val_accuracy: 0.9058 - lr: 3.3333e-04
Epoch 23/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1371 - accuracy: 0.9558 - val_loss: 0.3588 - val_accuracy: 0.9070 - lr: 3.3333e-04
Epoch 24/25
430/430 [==============================] - 9s 21ms/step - loss: 0.1218 - accuracy: 0.9617 - val_loss: 0.3806 - val_accuracy: 0.9074 - lr: 1.1111e-04
Epoch 25/25
430/430 [==============================] - 8s 19ms/step - loss: 0.1173 - accuracy: 0.9640 - val_loss: 0.3880 - val_accuracy: 0.9064 - lr: 1.1111e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_with_l1_reg.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(model_with_l1_reg.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_l1_reg.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_25_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 7s 4ms/step - loss: 0.1113 - accuracy: 0.9666

[0.11131621152162552, 0.9665636420249939]


313/313 [==============================] - 1s 4ms/step - loss: 0.4361 - accuracy: 0.8969

[0.4360505938529968, 0.8968999981880188]
</code></pre><h2 id="l2"><a href="#l2" class="headerlink" title="l2()"></a>l2()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">model_with_l2_reg = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_l2_reg.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, </span><br><span class="line">                                             kernel_regularizer=keras.regularizers.l2(l2=<span class="number">3e-6</span>)))</span><br><span class="line"></span><br><span class="line">model_with_l2_reg.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_l2_reg.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                          optimizer=optimizer, </span><br><span class="line">                          metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_l2_reg = model_with_l2_reg.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                            validation_data=(x_valid, y_valid),</span><br><span class="line">                                            callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 9s 18ms/step - loss: 0.7842 - accuracy: 0.7315 - val_loss: 0.5412 - val_accuracy: 0.8034 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 7s 17ms/step - loss: 0.4384 - accuracy: 0.8405 - val_loss: 0.3967 - val_accuracy: 0.8544 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3980 - accuracy: 0.8554 - val_loss: 0.4102 - val_accuracy: 0.8536 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3568 - accuracy: 0.8709 - val_loss: 0.3396 - val_accuracy: 0.8790 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3413 - accuracy: 0.8748 - val_loss: 0.3545 - val_accuracy: 0.8774 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3269 - accuracy: 0.8816 - val_loss: 0.3683 - val_accuracy: 0.8756 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3185 - accuracy: 0.8860 - val_loss: 0.3836 - val_accuracy: 0.8744 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3023 - accuracy: 0.8915 - val_loss: 0.3632 - val_accuracy: 0.8698 - lr: 0.0030.3022 - accuracy
Epoch 9/25
430/430 [==============================] - 8s 17ms/step - loss: 0.2381 - accuracy: 0.9125 - val_loss: 0.2946 - val_accuracy: 0.8972 - lr: 0.0010
Epoch 10/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2228 - accuracy: 0.9172 - val_loss: 0.3077 - val_accuracy: 0.9002 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2150 - accuracy: 0.9195 - val_loss: 0.3079 - val_accuracy: 0.8986 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2092 - accuracy: 0.9221 - val_loss: 0.3226 - val_accuracy: 0.8910 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2049 - accuracy: 0.9240 - val_loss: 0.3332 - val_accuracy: 0.8926 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1692 - accuracy: 0.9374 - val_loss: 0.3160 - val_accuracy: 0.9040 - lr: 3.3333e-04
Epoch 15/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1574 - accuracy: 0.9415 - val_loss: 0.3203 - val_accuracy: 0.9002 - lr: 3.3333e-04
Epoch 16/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1520 - accuracy: 0.9444 - val_loss: 0.3274 - val_accuracy: 0.9008 - lr: 3.3333e-04
Epoch 17/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1448 - accuracy: 0.9466 - val_loss: 0.3430 - val_accuracy: 0.8996 - lr: 3.3333e-04
Epoch 18/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1267 - accuracy: 0.9539 - val_loss: 0.3502 - val_accuracy: 0.9046 - lr: 1.1111e-04
Epoch 19/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1215 - accuracy: 0.9550 - val_loss: 0.3652 - val_accuracy: 0.9016 - lr: 1.1111e-04
Epoch 20/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1176 - accuracy: 0.9575 - val_loss: 0.3744 - val_accuracy: 0.9002 - lr: 1.1111e-04
Epoch 21/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1138 - accuracy: 0.9583 - val_loss: 0.3721 - val_accuracy: 0.9020 - lr: 1.1111e-04
Epoch 22/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1055 - accuracy: 0.9619 - val_loss: 0.3887 - val_accuracy: 0.9000 - lr: 3.7037e-05
Epoch 23/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1035 - accuracy: 0.9632 - val_loss: 0.3942 - val_accuracy: 0.9012 - lr: 3.7037e-05
Epoch 24/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1019 - accuracy: 0.9634 - val_loss: 0.4113 - val_accuracy: 0.9028 - lr: 3.7037e-05
Epoch 25/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1003 - accuracy: 0.9637 - val_loss: 0.4053 - val_accuracy: 0.8996 - lr: 3.7037e-05
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_with_l2_reg.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(model_with_l2_reg.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_l2_reg.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_29_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 7s 4ms/step - loss: 0.0970 - accuracy: 0.9660

[0.09695354849100113, 0.9660181999206543]


313/313 [==============================] - 1s 4ms/step - loss: 0.4260 - accuracy: 0.8957

[0.42597758769989014, 0.8956999778747559]
</code></pre><h2 id="l1-l2"><a href="#l1-l2" class="headerlink" title="l1_l2()"></a>l1_l2()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">model_with_l1l2_reg = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_l1l2_reg.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, </span><br><span class="line">                                               kernel_regularizer=keras.regularizers.l1_l2(l1=<span class="number">3e-6</span>, l2=<span class="number">3e-6</span>)))</span><br><span class="line"></span><br><span class="line">model_with_l1l2_reg.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_l1l2_reg.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                            optimizer=optimizer, </span><br><span class="line">                            metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_l1l2_reg = model_with_l1l2_reg.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                                validation_data=(x_valid, y_valid),</span><br><span class="line">                                                callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 10s 19ms/step - loss: 0.8085 - accuracy: 0.7345 - val_loss: 0.5617 - val_accuracy: 0.8048 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 8s 18ms/step - loss: 0.4591 - accuracy: 0.8401 - val_loss: 0.3946 - val_accuracy: 0.8642 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 8s 18ms/step - loss: 0.4229 - accuracy: 0.8565 - val_loss: 0.4137 - val_accuracy: 0.8612 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3774 - accuracy: 0.8725 - val_loss: 0.3887 - val_accuracy: 0.8712 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3602 - accuracy: 0.8767 - val_loss: 0.3529 - val_accuracy: 0.8808 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3462 - accuracy: 0.8836 - val_loss: 0.3782 - val_accuracy: 0.8790 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3335 - accuracy: 0.8872 - val_loss: 0.3859 - val_accuracy: 0.8710 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3223 - accuracy: 0.8920 - val_loss: 0.3978 - val_accuracy: 0.8630 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3145 - accuracy: 0.8946 - val_loss: 0.4028 - val_accuracy: 0.8780 - lr: 0.0030
Epoch 10/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2519 - accuracy: 0.9149 - val_loss: 0.3222 - val_accuracy: 0.8966 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2350 - accuracy: 0.9201 - val_loss: 0.3229 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2277 - accuracy: 0.9229 - val_loss: 0.3259 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2204 - accuracy: 0.9254 - val_loss: 0.3195 - val_accuracy: 0.8978 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2134 - accuracy: 0.9278 - val_loss: 0.3226 - val_accuracy: 0.8994 - lr: 0.0010
Epoch 15/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2083 - accuracy: 0.9297 - val_loss: 0.3433 - val_accuracy: 0.8950 - lr: 0.0010
Epoch 16/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2026 - accuracy: 0.9326 - val_loss: 0.3542 - val_accuracy: 0.8972 - lr: 0.0010
Epoch 17/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1940 - accuracy: 0.9351 - val_loss: 0.3455 - val_accuracy: 0.8970 - lr: 0.0010
Epoch 18/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1590 - accuracy: 0.9479 - val_loss: 0.3532 - val_accuracy: 0.9030 - lr: 3.3333e-04
Epoch 19/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1476 - accuracy: 0.9527 - val_loss: 0.3635 - val_accuracy: 0.9056 - lr: 3.3333e-04
Epoch 20/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1410 - accuracy: 0.9547 - val_loss: 0.3677 - val_accuracy: 0.9030 - lr: 3.3333e-04
Epoch 21/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1349 - accuracy: 0.9567 - val_loss: 0.3878 - val_accuracy: 0.9004 - lr: 3.3333e-04
Epoch 22/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1174 - accuracy: 0.9647 - val_loss: 0.4038 - val_accuracy: 0.8996 - lr: 1.1111e-04
Epoch 23/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1119 - accuracy: 0.9663 - val_loss: 0.4112 - val_accuracy: 0.9028 - lr: 1.1111e-04
Epoch 24/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1082 - accuracy: 0.9682 - val_loss: 0.4321 - val_accuracy: 0.9012 - lr: 1.1111e-04
Epoch 25/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1048 - accuracy: 0.9696 - val_loss: 0.4387 - val_accuracy: 0.8994 - lr: 1.1111e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_with_l1l2_reg.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(model_with_l1l2_reg.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_l1l2_reg.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_33_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 8s 5ms/step - loss: 0.0980 - accuracy: 0.9726

[0.09802676737308502, 0.9726181626319885]


313/313 [==============================] - 1s 5ms/step - loss: 0.4885 - accuracy: 0.8947

[0.48848921060562134, 0.8946999907493591]
</code></pre><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><h2 id="Regular-Dropout"><a href="#Regular-Dropout" class="headerlink" title="Regular Dropout"></a>Regular Dropout</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">model_with_dropout = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    model_with_dropout.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    model_with_dropout.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_with_dropout.add(keras.layers.Dropout(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">model_with_dropout.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_dropout.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                           optimizer=optimizer, </span><br><span class="line">                           metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_dropout = model_with_dropout.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                              validation_data=(x_valid, y_valid),</span><br><span class="line">                                              callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 8s 16ms/step - loss: 0.8331 - accuracy: 0.7204 - val_loss: 0.4820 - val_accuracy: 0.8290 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 7s 17ms/step - loss: 0.4750 - accuracy: 0.8316 - val_loss: 0.4168 - val_accuracy: 0.8586 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 7s 16ms/step - loss: 0.4474 - accuracy: 0.8440 - val_loss: 0.4188 - val_accuracy: 0.8600 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3892 - accuracy: 0.8620 - val_loss: 0.3509 - val_accuracy: 0.8774 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3694 - accuracy: 0.8702 - val_loss: 0.3763 - val_accuracy: 0.8548 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3506 - accuracy: 0.8758 - val_loss: 0.3687 - val_accuracy: 0.8754 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3407 - accuracy: 0.8797 - val_loss: 0.3697 - val_accuracy: 0.8700 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3362 - accuracy: 0.8824 - val_loss: 0.3767 - val_accuracy: 0.8666 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2661 - accuracy: 0.9041 - val_loss: 0.2933 - val_accuracy: 0.8966 - lr: 0.0010
Epoch 10/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2487 - accuracy: 0.9090 - val_loss: 0.2946 - val_accuracy: 0.8970 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2408 - accuracy: 0.9118 - val_loss: 0.3200 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2335 - accuracy: 0.9145 - val_loss: 0.3137 - val_accuracy: 0.8896 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2277 - accuracy: 0.9167 - val_loss: 0.2923 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2213 - accuracy: 0.9182 - val_loss: 0.3001 - val_accuracy: 0.8968 - lr: 0.0010
Epoch 15/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2144 - accuracy: 0.9204 - val_loss: 0.3046 - val_accuracy: 0.8960 - lr: 0.0010
Epoch 16/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2048 - accuracy: 0.9255 - val_loss: 0.3116 - val_accuracy: 0.8984 - lr: 0.0010
Epoch 17/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2021 - accuracy: 0.9252 - val_loss: 0.3191 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 18/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1656 - accuracy: 0.9390 - val_loss: 0.3177 - val_accuracy: 0.9038 - lr: 3.3333e-04
Epoch 19/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1537 - accuracy: 0.9431 - val_loss: 0.3284 - val_accuracy: 0.9050 - lr: 3.3333e-04
Epoch 20/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1476 - accuracy: 0.9449 - val_loss: 0.3322 - val_accuracy: 0.9006 - lr: 3.3333e-04
Epoch 21/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1417 - accuracy: 0.9474 - val_loss: 0.3434 - val_accuracy: 0.8994 - lr: 3.3333e-04
Epoch 22/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1250 - accuracy: 0.9536 - val_loss: 0.3527 - val_accuracy: 0.9050 - lr: 1.1111e-04
Epoch 23/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1190 - accuracy: 0.9560 - val_loss: 0.3574 - val_accuracy: 0.9056 - lr: 1.1111e-04
Epoch 24/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1158 - accuracy: 0.9566 - val_loss: 0.3740 - val_accuracy: 0.9034 - lr: 1.1111e-04
Epoch 25/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1117 - accuracy: 0.9588 - val_loss: 0.3855 - val_accuracy: 0.9034 - lr: 1.1111e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_with_dropout.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(model_with_dropout.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_dropout.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_38_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 6s 3ms/step - loss: 0.1015 - accuracy: 0.9624

[0.10153431445360184, 0.9624181985855103]


313/313 [==============================] - 1s 3ms/step - loss: 0.4297 - accuracy: 0.8955

[0.4297321140766144, 0.8955000042915344]
</code></pre><h2 id="AlphaDropout-自归一化网络"><a href="#AlphaDropout-自归一化网络" class="headerlink" title="AlphaDropout (自归一化网络)"></a>AlphaDropout (自归一化网络)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">model_with_alpha_dropout = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    model_with_alpha_dropout.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    model_with_alpha_dropout.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line">    model_with_alpha_dropout.add(keras.layers.AlphaDropout(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">model_with_alpha_dropout.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_alpha_dropout.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                                 optimizer=optimizer, </span><br><span class="line">                                 metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入特征标准化 (μ=0, σ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_alpha_dropout = model_with_alpha_dropout.fit(x_train_scaled, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                                          validation_data=(x_valid_scaled, y_valid),</span><br><span class="line">                                                          callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 9s 18ms/step - loss: 0.6672 - accuracy: 0.7771 - val_loss: 0.5470 - val_accuracy: 0.8572 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 7s 17ms/step - loss: 0.4653 - accuracy: 0.8437 - val_loss: 0.5983 - val_accuracy: 0.8474 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 7s 17ms/step - loss: 0.4090 - accuracy: 0.8605 - val_loss: 0.5949 - val_accuracy: 0.8572 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3917 - accuracy: 0.8655 - val_loss: 0.5147 - val_accuracy: 0.8720 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3594 - accuracy: 0.8782 - val_loss: 0.4578 - val_accuracy: 0.8768 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3398 - accuracy: 0.8837 - val_loss: 0.5777 - val_accuracy: 0.8672 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3399 - accuracy: 0.8849 - val_loss: 0.5091 - val_accuracy: 0.8790 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3175 - accuracy: 0.8912 - val_loss: 0.5224 - val_accuracy: 0.8776 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3103 - accuracy: 0.8946 - val_loss: 0.5111 - val_accuracy: 0.8818 - lr: 0.0030
Epoch 10/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2435 - accuracy: 0.9147 - val_loss: 0.4527 - val_accuracy: 0.8938 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 8s 19ms/step - loss: 0.2196 - accuracy: 0.9221 - val_loss: 0.5114 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2074 - accuracy: 0.9256 - val_loss: 0.4812 - val_accuracy: 0.8868 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1983 - accuracy: 0.9283 - val_loss: 0.5023 - val_accuracy: 0.8938 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1901 - accuracy: 0.9319 - val_loss: 0.5339 - val_accuracy: 0.8906 - lr: 0.0010
Epoch 15/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1602 - accuracy: 0.9434 - val_loss: 0.5606 - val_accuracy: 0.8988 - lr: 3.3333e-04
Epoch 16/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1466 - accuracy: 0.9483 - val_loss: 0.5909 - val_accuracy: 0.8974 - lr: 3.3333e-04
Epoch 17/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1372 - accuracy: 0.9513 - val_loss: 0.6188 - val_accuracy: 0.8988 - lr: 3.3333e-04
Epoch 18/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1315 - accuracy: 0.9535 - val_loss: 0.6030 - val_accuracy: 0.8964 - lr: 3.3333e-04
Epoch 19/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1150 - accuracy: 0.9596 - val_loss: 0.6832 - val_accuracy: 0.9014 - lr: 1.1111e-04
Epoch 20/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1102 - accuracy: 0.9619 - val_loss: 0.7075 - val_accuracy: 0.8992 - lr: 1.1111e-04
Epoch 21/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1056 - accuracy: 0.9629 - val_loss: 0.7323 - val_accuracy: 0.8990 - lr: 1.1111e-04
Epoch 22/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1027 - accuracy: 0.9641 - val_loss: 0.7475 - val_accuracy: 0.8998 - lr: 1.1111e-04
Epoch 23/25
430/430 [==============================] - 7s 17ms/step - loss: 0.0959 - accuracy: 0.9673 - val_loss: 0.7682 - val_accuracy: 0.9000 - lr: 3.7037e-05
Epoch 24/25
430/430 [==============================] - 7s 17ms/step - loss: 0.0939 - accuracy: 0.9679 - val_loss: 0.7781 - val_accuracy: 0.9014 - lr: 3.7037e-05
Epoch 25/25
430/430 [==============================] - 7s 17ms/step - loss: 0.0921 - accuracy: 0.9686 - val_loss: 0.8001 - val_accuracy: 0.9000 - lr: 3.7037e-05
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_with_alpha_dropout.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(model_with_alpha_dropout.evaluate(x_train_scaled, y_train))</span><br><span class="line">display(model_with_alpha_dropout.evaluate(x_test_scaled, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_43_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 6s 4ms/step - loss: 0.1080 - accuracy: 0.9704

[0.10797912627458572, 0.9703999757766724]


313/313 [==============================] - 1s 4ms/step - loss: 0.9345 - accuracy: 0.8960

[0.9344537258148193, 0.8960000276565552]
</code></pre><h1 id="结论-1"><a href="#结论-1" class="headerlink" title="结论 1"></a>结论 1</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Regularization</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Accu on train</th>
<th>Accu on test</th>
</tr>
</thead>
<tbody>
<tr>
<td>No Reg</td>
<td>7s/epoch</td>
<td>/</td>
<td>0.9661</td>
<td>0.8949</td>
</tr>
<tr>
<td>Batch Normalization</td>
<td>17s/epoch</td>
<td>/</td>
<td>0.9914</td>
<td>0.9016</td>
</tr>
<tr>
<td>EarlyStopping</td>
<td>7s/epoch</td>
<td>19 epochs</td>
<td>0.9382</td>
<td>0.8977</td>
</tr>
<tr>
<td>L1 Reg</td>
<td>8s/epoch</td>
<td>/</td>
<td>0.9666</td>
<td>0.8969</td>
</tr>
<tr>
<td>L2 Reg</td>
<td>8s/epoch</td>
<td>/</td>
<td>0.9660</td>
<td>0.8957</td>
</tr>
<tr>
<td>L1_L2 Reg</td>
<td>8s/epoch</td>
<td>/</td>
<td>0.9726</td>
<td>0.8947</td>
</tr>
<tr>
<td>Regular Dropout</td>
<td>7s/epoch</td>
<td>/</td>
<td>0.9624</td>
<td>0.8955</td>
</tr>
<tr>
<td>Alpha Dropout</td>
<td>7s/epoch</td>
<td>/</td>
<td>0.9704</td>
<td>0.8960</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>单独使用 Batch Normalization 作为正则化手段, 不可避免地<strong>大幅降低了训练速度</strong>, 但也获得了<strong>性能最佳的模型</strong>.</li>
<li>单独使用 EarlyStopping, <strong>显著降低了训练集上的 Accuracy</strong>, 但模型性能提升甚微.</li>
<li>单独使用 L1 / L2 / L1_L2 Reg, 相比无正则化区别不大.</li>
<li>以上所有带正则化的模型仍存在过拟合现象.</li>
</ol>
<h1 id="结合多种正则化技术"><a href="#结合多种正则化技术" class="headerlink" title="结合多种正则化技术"></a>结合多种正则化技术</h1><h2 id="Regular-DNN"><a href="#Regular-DNN" class="headerlink" title="Regular DNN"></a>Regular DNN</h2><p>Early Stopping + Batch Normalization (对于较深的网络) + l2 regularization (必要时使用)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">regular_dnn = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    regular_dnn.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, </span><br><span class="line">                                       kernel_regularizer=keras.regularizers.l2(<span class="number">3e-5</span>)))</span><br><span class="line">    regular_dnn.add(keras.layers.BatchNormalization())</span><br><span class="line"></span><br><span class="line">regular_dnn.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">regular_dnn.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">4</span>, patience=<span class="number">4</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">regular_history = regular_dnn.fit(x_train, y_train, epochs=<span class="number">25</span>*<span class="number">2</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                  validation_data=(x_valid, y_valid),</span><br><span class="line">                                  callbacks=[lr_scheduler, early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/50
430/430 [==============================] - 21s 40ms/step - loss: 0.5284 - accuracy: 0.8247 - val_loss: 0.3989 - val_accuracy: 0.8758 - lr: 0.0030
Epoch 2/50
430/430 [==============================] - 17s 40ms/step - loss: 0.4063 - accuracy: 0.8664 - val_loss: 0.3888 - val_accuracy: 0.8742 - lr: 0.0030
Epoch 3/50
430/430 [==============================] - 18s 41ms/step - loss: 0.3723 - accuracy: 0.8772 - val_loss: 0.4385 - val_accuracy: 0.8558 - lr: 0.0030
Epoch 4/50
430/430 [==============================] - 17s 40ms/step - loss: 0.3454 - accuracy: 0.8876 - val_loss: 0.3718 - val_accuracy: 0.8792 - lr: 0.0030
Epoch 5/50
430/430 [==============================] - 17s 40ms/step - loss: 0.3201 - accuracy: 0.8955 - val_loss: 0.3504 - val_accuracy: 0.8848 - lr: 0.0030
Epoch 6/50
430/430 [==============================] - 17s 40ms/step - loss: 0.3059 - accuracy: 0.8998 - val_loss: 0.3587 - val_accuracy: 0.8874 - lr: 0.0030
Epoch 7/50
430/430 [==============================] - 17s 40ms/step - loss: 0.2921 - accuracy: 0.9046 - val_loss: 0.3691 - val_accuracy: 0.8848 - lr: 0.0030
Epoch 8/50
430/430 [==============================] - 17s 40ms/step - loss: 0.2774 - accuracy: 0.9110 - val_loss: 0.3565 - val_accuracy: 0.8834 - lr: 0.0030
Epoch 9/50
430/430 [==============================] - 17s 40ms/step - loss: 0.2620 - accuracy: 0.9150 - val_loss: 0.3554 - val_accuracy: 0.8844 - lr: 0.0030
Epoch 10/50
430/430 [==============================] - 17s 40ms/step - loss: 0.1988 - accuracy: 0.9384 - val_loss: 0.3103 - val_accuracy: 0.9054 - lr: 7.5000e-04
Epoch 11/50
430/430 [==============================] - 18s 41ms/step - loss: 0.1763 - accuracy: 0.9460 - val_loss: 0.3156 - val_accuracy: 0.9052 - lr: 7.5000e-04
Epoch 12/50
430/430 [==============================] - 18s 42ms/step - loss: 0.1659 - accuracy: 0.9502 - val_loss: 0.3260 - val_accuracy: 0.8986 - lr: 7.5000e-04
Epoch 13/50
430/430 [==============================] - 17s 40ms/step - loss: 0.1558 - accuracy: 0.9528 - val_loss: 0.3301 - val_accuracy: 0.9026 - lr: 7.5000e-04
Epoch 14/50
430/430 [==============================] - 17s 40ms/step - loss: 0.1461 - accuracy: 0.9561 - val_loss: 0.3395 - val_accuracy: 0.9020 - lr: 7.5000e-04
Epoch 15/50
430/430 [==============================] - 17s 40ms/step - loss: 0.1184 - accuracy: 0.9670 - val_loss: 0.3509 - val_accuracy: 0.9022 - lr: 1.8750e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(regular_history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(regular_dnn.evaluate(x_train, y_train))</span><br><span class="line">display(regular_dnn.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_52_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 12s 7ms/step - loss: 0.1546 - accuracy: 0.9559

[0.15455804765224457, 0.9559454321861267]


313/313 [==============================] - 2s 7ms/step - loss: 0.3264 - accuracy: 0.9036

[0.32639697194099426, 0.9035999774932861]
</code></pre><h2 id="Self-Normalize-DNN"><a href="#Self-Normalize-DNN" class="headerlink" title="Self-Normalize DNN"></a>Self-Normalize DNN</h2><p>Early Stopping + Alpha Dropout</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">self_norm_dnn = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    self_norm_dnn.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    self_norm_dnn.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line">    self_norm_dnn.add(keras.layers.AlphaDropout(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">self_norm_dnn.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">self_norm_dnn.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入特征标准化 (μ=0, σ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">4</span>, patience=<span class="number">4</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">self_norm_history = self_norm_dnn.fit(x_train_scaled, y_train, epochs=<span class="number">25</span>*<span class="number">2</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                      validation_data=(x_valid_scaled, y_valid),</span><br><span class="line">                                      callbacks=[lr_scheduler, early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/50
430/430 [==============================] - 9s 19ms/step - loss: 0.6619 - accuracy: 0.7798 - val_loss: 0.5637 - val_accuracy: 0.8402 - lr: 0.0030
Epoch 2/50
430/430 [==============================] - 8s 18ms/step - loss: 0.4664 - accuracy: 0.8421 - val_loss: 0.5877 - val_accuracy: 0.8522 - lr: 0.0030
Epoch 3/50
430/430 [==============================] - 8s 18ms/step - loss: 0.4183 - accuracy: 0.8574 - val_loss: 0.5682 - val_accuracy: 0.8634 - lr: 0.0030
Epoch 4/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3946 - accuracy: 0.8657 - val_loss: 0.5031 - val_accuracy: 0.8706 - lr: 0.0030
Epoch 5/50
430/430 [==============================] - 7s 17ms/step - loss: 0.3664 - accuracy: 0.8742 - val_loss: 0.4748 - val_accuracy: 0.8760 - lr: 0.0030
Epoch 6/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3489 - accuracy: 0.8811 - val_loss: 0.5445 - val_accuracy: 0.8758 - lr: 0.0030
Epoch 7/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3360 - accuracy: 0.8865 - val_loss: 0.5468 - val_accuracy: 0.8716 - lr: 0.0030
Epoch 8/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3409 - accuracy: 0.8835 - val_loss: 0.4911 - val_accuracy: 0.8716 - lr: 0.0030
Epoch 9/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3090 - accuracy: 0.8939 - val_loss: 0.5311 - val_accuracy: 0.8780 - lr: 0.0030
Epoch 10/50
430/430 [==============================] - 8s 18ms/step - loss: 0.2407 - accuracy: 0.9152 - val_loss: 0.4742 - val_accuracy: 0.8926 - lr: 7.5000e-04
Epoch 11/50
430/430 [==============================] - 8s 18ms/step - loss: 0.2180 - accuracy: 0.9215 - val_loss: 0.5036 - val_accuracy: 0.8944 - lr: 7.5000e-04
Epoch 12/50
430/430 [==============================] - 8s 18ms/step - loss: 0.2064 - accuracy: 0.9260 - val_loss: 0.4891 - val_accuracy: 0.8920 - lr: 7.5000e-04
Epoch 13/50
430/430 [==============================] - 8s 18ms/step - loss: 0.1953 - accuracy: 0.9291 - val_loss: 0.5147 - val_accuracy: 0.8980 - lr: 7.5000e-04
Epoch 14/50
430/430 [==============================] - 8s 18ms/step - loss: 0.1871 - accuracy: 0.9316 - val_loss: 0.5644 - val_accuracy: 0.8958 - lr: 7.5000e-04
Epoch 15/50
430/430 [==============================] - 8s 18ms/step - loss: 0.1595 - accuracy: 0.9420 - val_loss: 0.5549 - val_accuracy: 0.8994 - lr: 1.8750e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(self_norm_history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练/测试集上评估模型</span></span><br><span class="line">display(self_norm_dnn.evaluate(x_train_scaled, y_train))</span><br><span class="line">display(self_norm_dnn.evaluate(x_test_scaled, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_58_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 6s 3ms/step - loss: 0.2535 - accuracy: 0.9248

[0.25346049666404724, 0.9247636198997498]


313/313 [==============================] - 1s 3ms/step - loss: 0.5312 - accuracy: 0.8911

[0.5311788320541382, 0.8910999894142151]
</code></pre><h1 id="结论-2"><a href="#结论-2" class="headerlink" title="结论 2"></a>结论 2</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Regularization</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Accu on train</th>
<th>Accu on test</th>
</tr>
</thead>
<tbody>
<tr>
<td>No Reg</td>
<td>7s/epoch</td>
<td>/</td>
<td>0.9661</td>
<td>0.8949</td>
</tr>
<tr>
<td>EarlyStopping + Batch Normalization + L2 Reg</td>
<td>18s/epoch</td>
<td>10 epochs</td>
<td>0.9559</td>
<td>0.9036</td>
</tr>
<tr>
<td>EarlyStopping + Alpha Dropout</td>
<td>8s/epoch</td>
<td>10 epochs</td>
<td>0.9248</td>
<td>0.8911</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>对于非自归一化网络, 使用 EarlyStopping + Batch Normalization + L2 Reg 进行正则化, 由于 BN 层的存在, 必然大幅降低训练速度, 但<strong>收敛速度提升</strong>; 而 EarlyStopping 节约了不必要的计算; 最终: 结合多种正则化技术, 达到了模型的<strong>最佳性能</strong>.</li>
<li>对于自归一化网络, 使用 EarlyStopping + Alpha Dropout 进行正则化, <strong>训练速度得以保持在较快水平</strong>, 且收敛速度提升; EarlyStopping 节约了不必要的计算; <strong>训练集上的 Accuracy 显著降低</strong>, 但最终模型性能不甚理想(可能需要微调其他超参数以发挥该模型的真正性能).</li>
</ol>
<p>🐒 可以改进的点: 以上各种模型都涉及大量超参数, 而测试中的配置并非最优超参数组合, 因此模型之间的性能比较未必准确. 另外, 使用 Alpha Dropout 的两个模型的验证误差都很高, 但 accuracy 却没有大幅下降…其原因尚未找到…</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>生成对抗网络 (Generative Adversarial Networks)</title>
    <url>/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/</url>
    <content><![CDATA[<p><strong>Generative Adversarial Networks</strong></p>
<p>GAN 由两个神经网络组成:</p>
<ol>
<li><p><strong>生成器</strong>: 接受一个随机分布(一般为高斯分布), 并输出一些数据.</p>
</li>
<li><p><strong>判别器</strong>: 判断接收的数据是训练集中的 “真实数据”, 还是由生成器生成的 “假数据”.</p>
<span id="more"></span>
</li>
</ol>
<p>下图是一个 GAN 架构的示例.</p>
<p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/GAN.PNG" width="80%"></p>
<p>训练期间, generator 与 discriminator 的目标相反: </p>
<ol>
<li>discriminator 试图识别出 “假图片”.</li>
<li>generator 试图产生足够逼真的 “假图片” 以骗过判别器.</li>
</ol>
<hr>
<p>GAN 的每个 training iteration 有两个阶段:</p>
<ol>
<li><strong>第一阶段训练 discriminator</strong>:<br>从训练集中采样一批 “真图像”(标签为 1), 再由 generator 产生同样数量的 “假图像”(标签为 0). 然后 discriminator 使用 binary cross-entropy loss 在这个真假参半的图像批次上训练一个 step. <strong>这期间反向传播仅优化 discriminator 的权重</strong>. </li>
</ol>
<ol>
<li><strong>第二阶段训练 generator</strong>:<br>首先让 generator 产生一批 “假图像”(<strong>将标签设置为 1</strong>), 并让 discriminator 区分其真假 (这次没有 “真图像”). 这里故意将标签设置为 1 是为了让 generator 产生使 discriminator 信以为真的图像. <strong>这期间反向传播仅优化 generator 的权重 (discriminator 的权重被冻结).</strong></li>
</ol>
<p>下面基于 Fashion MNIST 来构建一个简单的 GAN.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<h2 id="加载-amp-划分数据集"><a href="#加载-amp-划分数据集" class="headerlink" title="加载 &amp; 划分数据集"></a>加载 &amp; 划分数据集</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line">X_train_full = X_train_full.astype(np.float32) / <span class="number">255</span></span><br><span class="line">X_test = X_test.astype(np.float32) / <span class="number">255</span></span><br><span class="line">X_train, X_valid = X_train_full[:-<span class="number">5000</span>], X_train_full[-<span class="number">5000</span>:]</span><br><span class="line">y_train, y_valid = y_train_full[:-<span class="number">5000</span>], y_train_full[-<span class="number">5000</span>:]</span><br></pre></td></tr></table></figure>
<h2 id="构建生成器"><a href="#构建生成器" class="headerlink" title="构建生成器"></a>构建生成器</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">codings_size = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">generator = keras.models.Sequential([</span><br><span class="line">    <span class="comment"># generator 接收的输入类似于 autoencoder 中 decoder 部分接收的输入, 即 &quot;潜在编码&quot;.</span></span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, input_shape=[codings_size]), </span><br><span class="line">    keras.layers.Dense(<span class="number">150</span>, activation=<span class="string">&quot;elu&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">28</span> * <span class="number">28</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    keras.layers.Reshape([<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 id="构建判别器"><a href="#构建判别器" class="headerlink" title="构建判别器"></a>构建判别器</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">discriminator = keras.models.Sequential([</span><br><span class="line">    <span class="comment"># discriminator 接收的输入就是真/假图像</span></span><br><span class="line">    keras.layers.Flatten(input_shape=[<span class="number">28</span>, <span class="number">28</span>]),</span><br><span class="line">    keras.layers.Dense(<span class="number">150</span>, activation=<span class="string">&quot;elu&quot;</span>), </span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)    <span class="comment"># 该激活对应图像二分类</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 id="组合为-GAN"><a href="#组合为-GAN" class="headerlink" title="组合为 GAN"></a>组合为 GAN</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gan = keras.models.Sequential([generator, discriminator])</span><br></pre></td></tr></table></figure>
<h2 id="编译模型"><a href="#编译模型" class="headerlink" title="编译模型"></a>编译模型</h2><ol>
<li>由于 discriminator 是一个二分类器, 故可用 binary cross-entropy 作为其损失函数.</li>
<li>🔺 generator 将只通过整个 GAN 模型来训练, 因此无需单独编译它. 🔺</li>
<li>整个 GAN 模型也是一个二分类器, 故也可用 binary cross-entropy 作为其损失函数.</li>
<li>重要的是, 第二阶段只训练 generator 的权重, 因此在编译 GAN 模型前将 discriminator 设置为不可训练的.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">discriminator.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&quot;rmsprop&quot;</span>)</span><br><span class="line">discriminator.trainable = <span class="literal">False</span>    <span class="comment"># 对应上面第四条</span></span><br><span class="line">    </span><br><span class="line">gan.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&quot;rmsprop&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Remark</strong> on <code>discriminator.trainable = False</code>: </p>
<ol>
<li>当对 discriminator 调用 fit() 或 train_on_batch() 时, discriminator 是可训练的; </li>
<li>当对 gan model     调用 fit() 或 train_on_batch() 时, discriminator 是不可训练的.</li>
</ol>
<h2 id="自定义训练循环"><a href="#自定义训练循环" class="headerlink" title="自定义训练循环"></a>自定义训练循环</h2><h3 id="首先创建-Dataset"><a href="#首先创建-Dataset" class="headerlink" title="首先创建 Dataset"></a>首先创建 Dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从 X_train 创建 Dataset 并乱序 + 分批 + 预取 </span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(<span class="number">1000</span>)</span><br><span class="line">dataset = dataset.batch(batch_size=batch_size, drop_remainder=<span class="literal">True</span>).prefetch(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="定义训练循环"><a href="#定义训练循环" class="headerlink" title="定义训练循环"></a>定义训练循环</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiple_images</span>(<span class="params">images, n_cols=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 用于绘图の辅助函数</span></span><br><span class="line">    n_cols = n_cols <span class="keyword">or</span> <span class="built_in">len</span>(images)</span><br><span class="line">    n_rows = (<span class="built_in">len</span>(images) - <span class="number">1</span>) // n_cols + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> images.shape[-<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        images = np.squeeze(images, axis=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    plt.figure(figsize=(n_cols * <span class="number">1.5</span>, n_rows * <span class="number">1.5</span>))</span><br><span class="line">    <span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        plt.subplot(n_rows, n_cols, index + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_gan</span>(<span class="params">gan, dataset, batch_size, codings_size, n_epochs=<span class="number">10</span></span>):</span></span><br><span class="line">    generator, discriminator = gan.layers</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;/&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>, n_epochs))</span><br><span class="line">        <span class="keyword">for</span> X_batch <span class="keyword">in</span> dataset:</span><br><span class="line">            <span class="comment"># 阶段 1: 训练判别器</span></span><br><span class="line">            noise = tf.random.normal(shape=[batch_size, codings_size])</span><br><span class="line">            generated_images = generator(noise)</span><br><span class="line">            X_fake_and_real = tf.concat([generated_images, X_batch], axis=<span class="number">0</span>)</span><br><span class="line">            y1 = tf.constant([[<span class="number">0.</span>]] * batch_size + [[<span class="number">1.</span>]] * batch_size)</span><br><span class="line">            discriminator.trainable = <span class="literal">True</span>     <span class="comment"># 避免 warnings</span></span><br><span class="line">            discriminator.train_on_batch(X_fake_and_real, y1)       </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 阶段 2: 训练生成器</span></span><br><span class="line">            noise = tf.random.normal(shape=[batch_size, codings_size])</span><br><span class="line">            y2 = tf.constant([[<span class="number">1.</span>]] * batch_size)</span><br><span class="line">            discriminator.trainable = <span class="literal">False</span>    <span class="comment"># 避免 warnings</span></span><br><span class="line">            gan.train_on_batch(noise, y2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每个轮次结束时展示生成的图像</span></span><br><span class="line">        plot_multiple_images(generated_images, <span class="number">8</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<p><strong>Remark</strong> on previous code:</p>
<ol>
<li>在训练的第一阶段, 将高斯噪声传递给 generator 以产生 “假图像”, 同时在训练集中采样相同数量的 “真图像”, 将真假图像合并为一个批次. 这个批次对应的标签 $y1$ 中 0 代表 “假图像”, 1 代表 “真图像”. 然后在这个<strong>真假参半的图像批次上</strong>训练 discriminator.</li>
</ol>
<ol>
<li>在训练的第二阶段, 将高斯噪声传递给 gan 模型, 其 generator 将产生一些 “假图像”, 然后其 discriminator 将尝试分辨这些图像的真假. 这批 “假图像” 的标签 $y2$ 被设置为 1 (即表示 “真图像”), 这是为了<strong>让 discriminator 误以为它们是训练集中采样的 “真图像”</strong>.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_gan(gan, dataset, batch_size, codings_size, n_epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/1
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_24_1.png" alt="png"></p>
<p><strong>Remark</strong> on the figure above:<br>由图可见, 生成图像已经开始与 Fashion MNIST 中的图像相似了, 但遗憾的是, generator 并不能产生比这更(明显地)好的图像了…</p>
<h2 id="训练-GAN-时的困难-amp-挑战"><a href="#训练-GAN-时的困难-amp-挑战" class="headerlink" title="训练 GAN 时的困难 &amp; 挑战"></a>训练 GAN 时的困难 &amp; 挑战</h2><ol>
<li>事实上, GAN 只能达到唯一的纳什均衡, 即当 generator 产生 “完全逼真” 的图像, 而 discriminator 不得不对每张接收到的图像进行单纯的猜测(50% 正确率)的情况.</li>
</ol>
<ol>
<li>然而, 这种均衡状态能否达到根本没有保证.</li>
</ol>
<hr>
<ol>
<li>最大的困难在于 <strong>模式崩溃(mode collapse)</strong> 的发生, 即 generator 的输出变得不再多样化. 举例来说, 假设 generator 能够更好地生成逼真的鞋子的图像, 为了骗过 discriminator, 它就会产生更多鞋子的图像. 慢慢地, generator 就会忘记如何生成其它类别的图像了; 同时, discriminator 只会接收到假的鞋子的图像, 于是它也会逐渐忘记如何区分其它类别的假图像. 最终, 当 discriminator 学会区分真假鞋子的图像后, generator 又被迫转而去生成其它类别的图像(比如说连衣裙)而忘记了如何生成逼真的鞋子的图像, 然后 discriminator 也跟着只会区分真假连衣裙的图像…最终的结果是, GAN 逐渐在少数几个类别中循环往复, 但每一个类别都不擅长.</li>
</ol>
<ol>
<li>另外, GAN 对超参数非常敏感, 可能需要花费大量精力来调参.</li>
</ol>
<ol>
<li>解决这些难题的方法有: <strong>experience replay</strong> 和 <strong>mini-batch discrimination</strong> 等.</li>
</ol>
<h2 id="Deep-Convolutional-GANs-DCGANs"><a href="#Deep-Convolutional-GANs-DCGANs" class="headerlink" title="Deep Convolutional GANs (DCGANs)"></a>Deep Convolutional GANs (DCGANs)</h2><p>如何构建稳定的 convolutional GANs? 见下述指南:</p>
<ol>
<li><p>将 discriminator 中的池化层替换为 <strong>strided Convolutions</strong>; 将 generator 中的池化层替换为 <strong>transposed Convolutions</strong>.</p>
</li>
<li><p>在 generator 和 discriminator 中使用 <strong>Batch Normalization</strong> (但 generator 的输出层和 discriminator 的输入层除外).</p>
</li>
<li><p>对较深的网络架构, 移除全连接的隐藏层.</p>
</li>
<li><p>在 generator 的所有层中使用 <strong>ReLU</strong> 激活, 但其输出层使用 <strong>tanh</strong> 激活.</p>
</li>
<li><p>在 discriminator 的所有层中使用 <strong>Leaky ReLU</strong> 激活, 但其输出层使用 <strong>sigmoid</strong> 激活.</p>
</li>
</ol>
<p><strong>Remark</strong>: </p>
<ol>
<li>上述指南在多数情况下能帮助构建一个稳定的 DCGAN, 但世事无绝对, 你仍可能需要尝试不同的超参数值; </li>
<li>有时候, 仅仅改变随机种子并重新训练完全相同的模型也可能成功.</li>
</ol>
<p>下面基于 Fashion MNIST 构建一个 DCGAN 作为演示:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建生成器</span></span><br><span class="line">codings_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">generator = keras.models.Sequential([</span><br><span class="line">    keras.layers.Dense(<span class="number">7</span> * <span class="number">7</span> * <span class="number">128</span>, input_shape=[codings_size]), </span><br><span class="line">    keras.layers.Reshape([<span class="number">7</span>, <span class="number">7</span>, <span class="number">128</span>]), </span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    keras.layers.Conv2DTranspose(<span class="number">64</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;selu&quot;</span>), </span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    keras.layers.Conv2DTranspose(<span class="number">1</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;tanh&quot;</span>) </span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.构建判别器</span></span><br><span class="line">discriminator = keras.models.Sequential([</span><br><span class="line">    keras.layers.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;same&quot;</span>, </span><br><span class="line">                        activation=keras.layers.LeakyReLU(<span class="number">0.2</span>), input_shape=[<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.4</span>), </span><br><span class="line">    keras.layers.Conv2D(<span class="number">128</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;same&quot;</span>, </span><br><span class="line">                        activation=keras.layers.LeakyReLU(<span class="number">0.2</span>)),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.4</span>), </span><br><span class="line">    keras.layers.Flatten(),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)  </span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.两者结合得到 GAN</span></span><br><span class="line">gan = keras.models.Sequential([generator, discriminator])</span><br></pre></td></tr></table></figure>
<p><strong>Remark</strong> on the code above:</p>
<ol>
<li>generator 接收 100 维的编码向量, 并将其映射为 (7, 7, 128) 的三阶张量, 该张量经过批量归一化后传递至一个转置卷积层(将图像的空间尺度上采样至 14 <em> 14, 同时将深度降至 64). 其结果再经批量归一化后被传递至第二个转置卷积层(空间尺度上采样至 28 </em> 28, 深度降至 1), 该层使用 tanh 激活, 故其输出在 [-1, 1] 中, 因此在训练 GAN 前需将训练集缩放至 [-1, 1] 内. 另外还需要对训练集做 reshape 以添加 channel 维度.</li>
</ol>
<ol>
<li>discriminator 与通常的二分类 CNN 相似, 但其下采样则通过 strides=2 的卷积层实现(而非最大池化层). 另外, 卷积层使用了 Leaky ReLU 激活.</li>
</ol>
<ol>
<li>上面的代码没有完全遵照指南, 以避免训练不稳定的情况.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加 channel 维度并缩放至 [-1, 1]</span></span><br><span class="line">X_train_dcgan = X_train.reshape(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>) * <span class="number">2.</span> - <span class="number">1.</span> </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型编译与 [5] 中一致</span></span><br><span class="line">discriminator.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&quot;rmsprop&quot;</span>)</span><br><span class="line">discriminator.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">gan.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&quot;rmsprop&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 Dataset 与 [6.1] 中一致</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)</span><br><span class="line">dataset = dataset.shuffle(<span class="number">1000</span>)</span><br><span class="line">dataset = dataset.batch(batch_size, drop_remainder=<span class="literal">True</span>).prefetch(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练 50 个轮次 (训练方法与 [6.2] 中一致)</span></span><br><span class="line">train_gan(gan, dataset, batch_size, codings_size, n_epochs=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_1.png" alt="png"></p>
<pre><code>Epoch 2/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_3.png" alt="png"></p>
<pre><code>Epoch 3/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_5.png" alt="png"></p>
<p>……</p>
<pre><code>Epoch 48/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_95.png" alt="png"></p>
<pre><code>Epoch 49/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_97.png" alt="png"></p>
<pre><code>Epoch 50/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_99.png" alt="png"></p>
<p><strong>Remark</strong>:</p>
<ol>
<li>可以看到, 经过 50 个轮次的训练, DCGAN 已经能生成与 Fashion MNIST 中较为相似的图像了, 但仍有部分图像难以辨认, 或者不够自然…</li>
</ol>
<ol>
<li>可以尝试改动上面 DCGAN 的架构, 你就能明白它对超参数值有多敏感了~</li>
</ol>
<ol>
<li>DCGANs 仍不完美, 若尝试使用它生成很大的图像, 则可能得到局部真实的特征, 但总体上却有违和感(比如一件衬衫两只袖子长短不一).</li>
</ol>
<h2 id="GANs-的渐进式增长"><a href="#GANs-的渐进式增长" class="headerlink" title="GANs 的渐进式增长"></a>GANs 的渐进式增长</h2><p><strong>一种重要的技术</strong>: 在训练初期生成较小的图像, 然后逐渐在 generator 和 discriminator 中添加卷积层以产生越来越大的图像. 这些额外的层将添加在 generator 的末尾以及 discriminator 的开头, 并且先前训练好的层仍是可训练的.</p>
<hr>
<ol>
<li>举例而言(参考下图), 在将 generator 的输出从 4x4 扩大至 8x8 时, 在现有卷积层上添加一个上采样层以输出 8x8 的特征图, 其输出再传递给新的卷积层, 而其输出再次被传递给一个输出卷积层, 以输出 8x8x3 的常规图像. </li>
</ol>
<ol>
<li>为避免破坏第一个卷积层的权重, 最终输出是 &lt;原始输出层&gt; 与 &lt;新输出层(以虚线框标识)&gt; 的加权和. 新输出的权重为 $α$, 而原始输出的权重为 $1-α$, 且 $α$ 从 0 慢慢增加到 1. 也就是说: 新输出层将<strong>逐渐取代</strong>原始输出层.</li>
</ol>
<ol>
<li>在向 discriminator 中添加新卷积层时, 也使用类似的逐渐取代的策略.</li>
</ol>
<p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/Progressive Growing of GANs.PNG" width="80%"></p>
<p>其它旨在 <strong>增加输出多样性</strong> &amp; <strong>使训练更稳定</strong> 的技术:</p>
<ol>
<li><strong>Minibatch standard deviation layer</strong>: 鼓励 generator 产生更多样化的输出, 从而降低模式崩溃的风险.</li>
</ol>
<ol>
<li><strong>Equalized learning rate</strong>:  既可加快训练速度又可提高训练稳定性. (当使用 RMSProp, Adam 或其他<strong>自适应梯度优化器</strong>时, 该技术能显著提高 GAN 的性能)</li>
</ol>
<ol>
<li><strong>Pixelwise normalization layer</strong>: 可避免由 generator 和 discriminator 间的过度竞争而导致的激活爆炸.</li>
</ol>
<h2 id="StyleGANs"><a href="#StyleGANs" class="headerlink" title="StyleGANs"></a>StyleGANs</h2><p>在 generator 中使用<strong>风格迁移</strong>技术, 确保生成图像与训练图像(在所有尺度上)有相同的局部结构, 这极大提升了生成图像的质量; 而 discriminator 与 loss func 则没有修改.</p>
<p>StyleGAN 有两个神经网络组成:</p>
<ol>
<li><strong>Mapping network</strong>: 这是一个 8 层的 MLP, 将潜在表征向量 $z$ 映射至向量 $w$, 后者再经仿射变换(下图中方块A)产生了多个向量, 这些向量控制了生成图像在不同 level 上的风格 (从精细的纹理到高阶的特征). 简而言之, <strong>Mapping network 将潜在表征映射至多个风格向量</strong>.</li>
</ol>
<ol>
<li><strong>Synthesis network</strong>: 负责生成图像. 它包含一个 (在训练后) 恒定的输入, 并使用多个卷积层 &amp; 上采样层处理该输入. 不同点在于: (1).一些噪声被添加到输入和卷积层的所有输出中; (2).每个噪声层后有一个<strong>自适应实例归一化层</strong>, 它独立地标准化每个特征图, 然后使用风格向量来确定每个特征图的尺度和偏移量.</li>
</ol>
<p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/StyleGAN.PNG" width="80%"></p>
<p>未完待续…</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>蒙特卡洛 Dropout 实践</title>
    <url>/2022/01/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B-Dropout-%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<p>❓ 如何使用 MC(蒙特卡洛) Dropout? 它能带来什么好处?</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>🔺 针对 Fashion MNIST 数据集, 开展下面的测试.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集 (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist 中数字标签对应的类别名称</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 展示部分训练集实例</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m 行 n 列</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B-Dropout-%E5%AE%9E%E8%B7%B5/output_5_0.png" alt="png"></p>
<h1 id="MC-Dropout"><a href="#MC-Dropout" class="headerlink" title="MC Dropout"></a>MC Dropout</h1><h2 id="首先构建一个常规的-Dropout-网络"><a href="#首先构建一个常规的-Dropout-网络" class="headerlink" title="首先构建一个常规的 Dropout 网络"></a>首先构建一个常规的 Dropout 网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.构建模型</span></span><br><span class="line">model = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model.add(keras.layers.Dropout(<span class="number">0.4</span>))</span><br><span class="line"></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.003</span>, decay=<span class="number">1</span>/(<span class="number">430</span>*<span class="number">4</span>))</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                           optimizer=optimizer, </span><br><span class="line">                           metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, batch_size=<span class="number">32</span>,</span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 9s 5ms/step - loss: 0.6985 - accuracy: 0.7602 - val_loss: 0.4135 - val_accuracy: 0.8502
Epoch 2/100
1719/1719 [==============================] - 7s 4ms/step - loss: 0.4509 - accuracy: 0.8418 - val_loss: 0.3817 - val_accuracy: 0.8614
Epoch 3/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.3922 - accuracy: 0.8609 - val_loss: 0.3501 - val_accuracy: 0.8770
Epoch 4/100
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3519 - accuracy: 0.8756 - val_loss: 0.3257 - val_accuracy: 0.8808
Epoch 5/100
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3251 - accuracy: 0.8841 - val_loss: 0.3115 - val_accuracy: 0.8836
Epoch 6/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.3049 - accuracy: 0.8908 - val_loss: 0.3256 - val_accuracy: 0.8862
Epoch 7/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2877 - accuracy: 0.8952 - val_loss: 0.3103 - val_accuracy: 0.8912
Epoch 8/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2707 - accuracy: 0.9030 - val_loss: 0.2940 - val_accuracy: 0.8970
Epoch 9/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2591 - accuracy: 0.9049 - val_loss: 0.2892 - val_accuracy: 0.8970
Epoch 10/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2474 - accuracy: 0.9085 - val_loss: 0.2985 - val_accuracy: 0.8988
Epoch 11/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2389 - accuracy: 0.9119 - val_loss: 0.2984 - val_accuracy: 0.8972
Epoch 12/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2291 - accuracy: 0.9157 - val_loss: 0.2889 - val_accuracy: 0.8994
Epoch 13/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2181 - accuracy: 0.9193 - val_loss: 0.2906 - val_accuracy: 0.9040
Epoch 14/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2133 - accuracy: 0.9212 - val_loss: 0.3011 - val_accuracy: 0.9002
Epoch 15/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2067 - accuracy: 0.9235 - val_loss: 0.3074 - val_accuracy: 0.9012
Epoch 16/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1995 - accuracy: 0.9269 - val_loss: 0.3019 - val_accuracy: 0.9034
Epoch 17/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1943 - accuracy: 0.9286 - val_loss: 0.2989 - val_accuracy: 0.9048
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.在训练/测试集上评估模型</span></span><br><span class="line">display(model.evaluate(x_train, y_train))</span><br><span class="line">display(model.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B-Dropout-%E5%AE%9E%E8%B7%B5/output_10_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 4s 3ms/step - loss: 0.1939 - accuracy: 0.9266

[0.19393619894981384, 0.9265636205673218]


313/313 [==============================] - 1s 2ms/step - loss: 0.3195 - accuracy: 0.8905

[0.31950753927230835, 0.890500009059906]
</code></pre><h2 id="实现-MC-Dropout"><a href="#实现-MC-Dropout" class="headerlink" title="实现 MC Dropout"></a>实现 MC Dropout</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 激活 Dropout 层并进行 sample 次预测, 并对预测求平均.</span></span><br><span class="line">y_probas = np.stack([model(x_test, training=<span class="literal">True</span>) <span class="keyword">for</span> sample <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>)])</span><br><span class="line">y_proba = y_probas.mean(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="对比常规-Dropout-模型和-MC-Dropout-的预测结果"><a href="#对比常规-Dropout-模型和-MC-Dropout-的预测结果" class="headerlink" title="对比常规 Dropout 模型和 MC Dropout 的预测结果"></a>对比常规 Dropout 模型和 MC Dropout 的预测结果</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.常规 Dropout 模型的预测结果</span></span><br><span class="line">display(np.<span class="built_in">round</span>(model.predict(x_test[:<span class="number">1</span>]), <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;~&quot;</span>*<span class="number">72</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.MC Dropout 的预测结果</span></span><br><span class="line">display(np.<span class="built_in">round</span>(y_probas[:<span class="number">3</span>, :<span class="number">1</span>], <span class="number">2</span>))    <span class="comment"># 对第一个实例的前三次预测</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;~&quot;</span>*<span class="number">72</span>)</span><br><span class="line"></span><br><span class="line">display(np.<span class="built_in">round</span>(y_proba[:<span class="number">1</span>], <span class="number">2</span>))         <span class="comment"># 对第一个实例的平均预测</span></span><br></pre></td></tr></table></figure>
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],
      dtype=float32)

array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]]],
      dtype=float32)

array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],
      dtype=float32)
</code></pre><h2 id="MC-Dropout-预测的-Accuracy"><a href="#MC-Dropout-预测的-Accuracy" class="headerlink" title="MC Dropout 预测的 Accuracy"></a>MC Dropout 预测的 Accuracy</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracy = np.<span class="built_in">sum</span>(np.argmax(y_proba, axis=<span class="number">1</span>)==y_test) / <span class="built_in">len</span>(y_test)</span><br><span class="line">accuracy</span><br></pre></td></tr></table></figure>
<pre><code>0.892
</code></pre><p>Remark: 由此可见 MC Dropout 预测的 Accuracy(=0.892) 要<strong>略高于</strong>常规 Dropout 模型预测的 Accuracy(=0.8905)</p>
<h2 id="上述-MC-Dropout-的实现仍有限制"><a href="#上述-MC-Dropout-的实现仍有限制" class="headerlink" title="上述 MC Dropout 的实现仍有限制"></a>上述 MC Dropout 的实现仍有限制</h2><p>若 model 中包含其他 &lt;在训练期间和推断期间行为不同&gt; 的层 (如 BatchNormalization 层), 则应使用下面的类实现 MC Dropout.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MCDropout</span>(<span class="params">keras.layers.Dropout</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().call(inputs, training=<span class="literal">True</span>)    <span class="comment"># 重载 call 方法, 强制 training 参数为 True</span></span><br></pre></td></tr></table></figure>
<p>可以使用上述 MCDropout 层代替 Dropout 层, 重新训练一个模型;<br>或者构建一个使用 MCDropout 层的新模型, 并将原模型的参数拷贝给它, 像下面一样:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在原 model 基础上，将 Dropout 层替换为 MCDropout 层, 得到一个新模型</span></span><br><span class="line">mc_model = keras.models.Sequential([</span><br><span class="line">    MCDropout(layer.rate) <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, keras.layers.Dropout) <span class="keyword">else</span> layer </span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.003</span>, decay=<span class="number">1</span>/(<span class="number">430</span>*<span class="number">4</span>))</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, optimizer=optimizer, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重用原 model 的连接权重</span></span><br><span class="line">mc_model.set_weights(model.get_weights())</span><br></pre></td></tr></table></figure>
<p>接下来就能使用这个 MC Dropout 模型了.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在测试集上预测 100 次后取平均</span></span><br><span class="line">y_proba_1 = np.mean([mc_model.predict(x_test[:<span class="number">1</span>]) <span class="keyword">for</span> sample <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>)], axis=<span class="number">0</span>)</span><br><span class="line">np.<span class="built_in">round</span>(y_proba_1, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],
      dtype=float32)
</code></pre><h1 id="MC-Alpha-Dropout"><a href="#MC-Alpha-Dropout" class="headerlink" title="MC Alpha Dropout"></a>MC Alpha Dropout</h1><p>仿照上面的流程, 容易实现 MC Alpha Dropout.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.定义 MCAlphaDropout Class (继承 AlphaDropout)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MCAlphaDropout</span>(<span class="params">keras.layers.AlphaDropout</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().call(inputs, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.构建自归一化网络</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">model_self_norm = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    model_self_norm.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line">    model_self_norm.add(MCAlphaDropout(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">model_self_norm.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.003</span>, decay=<span class="number">1</span>/(<span class="number">430</span>*<span class="number">4</span>))</span><br><span class="line">model_self_norm.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.输入特征标准化 (μ=0, σ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.训练模型</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_self_norm = model_self_norm.fit(x_train_scaled, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">32</span>,</span><br><span class="line">                                        validation_data=(x_valid_scaled, y_valid),</span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
1719/1719 [==============================] - 9s 5ms/step - loss: 0.5783 - accuracy: 0.8027 - val_loss: 0.4252 - val_accuracy: 0.8522
Epoch 2/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.3962 - accuracy: 0.8610 - val_loss: 0.3962 - val_accuracy: 0.8630
Epoch 3/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.3413 - accuracy: 0.8787 - val_loss: 0.3716 - val_accuracy: 0.8720
Epoch 4/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2987 - accuracy: 0.8921 - val_loss: 0.3438 - val_accuracy: 0.8820
Epoch 5/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2706 - accuracy: 0.9015 - val_loss: 0.3244 - val_accuracy: 0.8900
Epoch 6/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2451 - accuracy: 0.9107 - val_loss: 0.3190 - val_accuracy: 0.8918
Epoch 7/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2254 - accuracy: 0.9161 - val_loss: 0.3248 - val_accuracy: 0.8894
Epoch 8/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2052 - accuracy: 0.9242 - val_loss: 0.3284 - val_accuracy: 0.8930
Epoch 9/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1889 - accuracy: 0.9298 - val_loss: 0.3465 - val_accuracy: 0.8968
Epoch 10/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1764 - accuracy: 0.9351 - val_loss: 0.3469 - val_accuracy: 0.8968
Epoch 11/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1618 - accuracy: 0.9390 - val_loss: 0.3632 - val_accuracy: 0.8988
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_self_norm.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.在训练/测试集上评估模型</span></span><br><span class="line">display(model_self_norm.evaluate(x_train_scaled, y_train))</span><br><span class="line">display(model_self_norm.evaluate(x_test_scaled, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B-Dropout-%E5%AE%9E%E8%B7%B5/output_31_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 5s 3ms/step - loss: 0.2152 - accuracy: 0.9211

[0.21519158780574799, 0.9211272597312927]


313/313 [==============================] - 1s 3ms/step - loss: 0.3495 - accuracy: 0.8824

[0.3495039939880371, 0.8823999762535095]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 7.使用带有 (激活的)MCAlphaDropout 层的模型进行预测</span></span><br><span class="line">y_proba_2 = np.mean([model_self_norm.predict(x_test_scaled[:<span class="number">1</span>]) <span class="keyword">for</span> sample <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>)], axis=<span class="number">0</span>)</span><br><span class="line">np.<span class="built_in">round</span>(y_proba_2, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],
      dtype=float32)
</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>推荐使用 MCDropout &amp; MCAlphaDropout 类来构建模型, 这样模型可兼容一些在训练期间和推断期间行为不同的层.</li>
<li>MCDropout &amp; MCAlphaDropout 思想上是一致的, 只不过后者专门服务于自归一化网络.</li>
<li>MC Dropout 技术能够帮助获得更好的预测, 也就是说: (1).<strong>预测准确率更高</strong> (2).预测具有<strong>更好的不确定性评估</strong>. </li>
</ol>
<p>🐒 可以改进的点: MCDropout 中的 sample 也是一个超参数, 本文中没有测试不同的 sample 值带来的影响.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>隐藏层中不同 activation 的对比</title>
    <url>/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>❓ 对比不同的 activation + 对应的初始化策略, 它们如何影响梯度消失问题?</p>
<ol>
<li><p>ReLU / Leaky ReLU / Parametric Leaky ReLU + he initialization</p>
</li>
<li><p>ELU + he initialization</p>
</li>
<li><p>SELU + lecun_normal initialization (自归一化网络)</p>
<span id="more"></span>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>🔺 下面针对 Fashion MNIST 数据集, 测试以下三种(相同的)网络架构对该问题能达到怎样的性能.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备数据集 (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist 中数字标签对应的类别名称</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 展示部分训练集实例</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m 行 n 列</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_5_0.png" alt="png"></p>
<h1 id="原始的-Sigmoid"><a href="#原始的-Sigmoid" class="headerlink" title="原始的 Sigmoid"></a>原始的 Sigmoid</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型构建</span></span><br><span class="line">model_ori = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_ori.add(keras.layers.Dense(<span class="number">50</span>, activation=<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_ori.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_ori.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                  optimizer=optimizer, </span><br><span class="line">                  metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_ori = model_ori.fit(x_train, y_train, epochs=<span class="number">10</span>, </span><br><span class="line">                            validation_data=(x_valid, y_valid), </span><br><span class="line">                            callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/10
1719/1719 [==============================] - 12s 6ms/step - loss: 1.9044 - accuracy: 0.1749 - val_loss: 1.7585 - val_accuracy: 0.1914
Epoch 2/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7287 - accuracy: 0.2000 - val_loss: 1.7233 - val_accuracy: 0.1954
Epoch 3/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7402 - accuracy: 0.1976 - val_loss: 1.7459 - val_accuracy: 0.1888
Epoch 4/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7295 - accuracy: 0.1999 - val_loss: 1.7693 - val_accuracy: 0.2064
Epoch 5/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7627 - accuracy: 0.1972 - val_loss: 1.7482 - val_accuracy: 0.1938
Epoch 6/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7409 - accuracy: 0.1965 - val_loss: 1.7335 - val_accuracy: 0.2000
Epoch 7/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7254 - accuracy: 0.1985 - val_loss: 1.7321 - val_accuracy: 0.2120
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_ori.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_ori.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_9_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 3ms/step - loss: 1.7235 - accuracy: 0.1993

[1.7234939336776733, 0.19930000603199005]
</code></pre><h1 id="ReLU-及其变体"><a href="#ReLU-及其变体" class="headerlink" title="ReLU 及其变体"></a>ReLU 及其变体</h1><h2 id="ReLU-he-initialization"><a href="#ReLU-he-initialization" class="headerlink" title="ReLU + he initialization"></a>ReLU + he initialization</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型构建</span></span><br><span class="line">model = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;relu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">50</span>, activation=<span class="string">&quot;relu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">              optimizer=optimizer, </span><br><span class="line">              metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid), </span><br><span class="line">                    callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.5692 - accuracy: 0.7916 - val_loss: 0.4004 - val_accuracy: 0.8618
Epoch 2/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.4166 - accuracy: 0.8488 - val_loss: 0.4237 - val_accuracy: 0.8442
Epoch 3/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3775 - accuracy: 0.8631 - val_loss: 0.3526 - val_accuracy: 0.8750
Epoch 4/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3547 - accuracy: 0.8703 - val_loss: 0.3420 - val_accuracy: 0.8804
Epoch 5/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3367 - accuracy: 0.8794 - val_loss: 0.3465 - val_accuracy: 0.8736
Epoch 6/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3201 - accuracy: 0.8849 - val_loss: 0.3395 - val_accuracy: 0.8728
Epoch 7/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3109 - accuracy: 0.8885 - val_loss: 0.3303 - val_accuracy: 0.8844
Epoch 8/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2989 - accuracy: 0.8903 - val_loss: 0.3283 - val_accuracy: 0.8876
Epoch 9/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2898 - accuracy: 0.8934 - val_loss: 0.3251 - val_accuracy: 0.8874
Epoch 10/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2795 - accuracy: 0.8984 - val_loss: 0.3163 - val_accuracy: 0.8900
Epoch 11/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2726 - accuracy: 0.9004 - val_loss: 0.3581 - val_accuracy: 0.8870
Epoch 12/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2654 - accuracy: 0.9012 - val_loss: 0.3145 - val_accuracy: 0.8882
Epoch 13/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2575 - accuracy: 0.9063 - val_loss: 0.3099 - val_accuracy: 0.8906
Epoch 14/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2526 - accuracy: 0.9069 - val_loss: 0.3396 - val_accuracy: 0.8834
Epoch 15/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2417 - accuracy: 0.9106 - val_loss: 0.3241 - val_accuracy: 0.8928
Epoch 16/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2405 - accuracy: 0.9128 - val_loss: 0.3205 - val_accuracy: 0.8910
Epoch 17/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2322 - accuracy: 0.9142 - val_loss: 0.3449 - val_accuracy: 0.8892
Epoch 18/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2311 - accuracy: 0.9148 - val_loss: 0.3124 - val_accuracy: 0.8992
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_15_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 3ms/step - loss: 0.3455 - accuracy: 0.8789

[0.3454982042312622, 0.8788999915122986]
</code></pre><h2 id="Leaky-ReLU-he-initialization"><a href="#Leaky-ReLU-he-initialization" class="headerlink" title="Leaky ReLU + he initialization"></a>Leaky ReLU + he initialization</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型构建</span></span><br><span class="line">model_leaky = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_leaky.add(keras.layers.Dense(<span class="number">50</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_leaky.add(keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>))</span><br><span class="line"></span><br><span class="line">model_leaky.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_leaky.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                    optimizer=optimizer, </span><br><span class="line">                    metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_leaky = model_leaky.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                validation_data=(x_valid, y_valid), </span><br><span class="line">                                callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 12s 6ms/step - loss: 0.5612 - accuracy: 0.7937 - val_loss: 0.4269 - val_accuracy: 0.8432
Epoch 2/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.4181 - accuracy: 0.8481 - val_loss: 0.4394 - val_accuracy: 0.8424
Epoch 3/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3806 - accuracy: 0.8616 - val_loss: 0.3604 - val_accuracy: 0.8698
Epoch 4/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3558 - accuracy: 0.8713 - val_loss: 0.3439 - val_accuracy: 0.8780
Epoch 5/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3381 - accuracy: 0.8770 - val_loss: 0.3917 - val_accuracy: 0.8592
Epoch 6/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3279 - accuracy: 0.8811 - val_loss: 0.3685 - val_accuracy: 0.8720
Epoch 7/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.3173 - accuracy: 0.8852 - val_loss: 0.3416 - val_accuracy: 0.8824
Epoch 8/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3031 - accuracy: 0.8892 - val_loss: 0.3171 - val_accuracy: 0.8828
Epoch 9/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2932 - accuracy: 0.8927 - val_loss: 0.3086 - val_accuracy: 0.8910
Epoch 10/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.2869 - accuracy: 0.8954 - val_loss: 0.3413 - val_accuracy: 0.8900
Epoch 11/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.2762 - accuracy: 0.8988 - val_loss: 0.3319 - val_accuracy: 0.8876
Epoch 12/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2697 - accuracy: 0.9011 - val_loss: 0.3514 - val_accuracy: 0.8730
Epoch 13/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2663 - accuracy: 0.9026 - val_loss: 0.3334 - val_accuracy: 0.8914
Epoch 14/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2588 - accuracy: 0.9036 - val_loss: 0.3192 - val_accuracy: 0.8882
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_leaky.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_leaky.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_19_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3529 - accuracy: 0.8749

[0.3529128432273865, 0.8748999834060669]
</code></pre><h2 id="Parametric-Leaky-ReLU-he-initialization"><a href="#Parametric-Leaky-ReLU-he-initialization" class="headerlink" title="Parametric Leaky ReLU + he initialization"></a>Parametric Leaky ReLU + he initialization</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型构建</span></span><br><span class="line">model_leaky_p = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.PReLU(),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.PReLU()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_leaky_p.add(keras.layers.Dense(<span class="number">50</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_leaky_p.add(keras.layers.PReLU())</span><br><span class="line"></span><br><span class="line">model_leaky_p.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_leaky_p.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_leaky_p = model_leaky_p.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                    validation_data=(x_valid, y_valid), </span><br><span class="line">                                    callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 19s 10ms/step - loss: 0.5752 - accuracy: 0.7899 - val_loss: 0.3916 - val_accuracy: 0.8602
Epoch 2/100
1719/1719 [==============================] - 17s 10ms/step - loss: 0.4137 - accuracy: 0.8504 - val_loss: 0.4368 - val_accuracy: 0.8450
Epoch 3/100
1719/1719 [==============================] - 17s 10ms/step - loss: 0.3768 - accuracy: 0.8632 - val_loss: 0.3443 - val_accuracy: 0.8730
Epoch 4/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.3514 - accuracy: 0.8729 - val_loss: 0.3302 - val_accuracy: 0.8812
Epoch 5/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.3315 - accuracy: 0.8801 - val_loss: 0.3350 - val_accuracy: 0.8824
Epoch 6/100
1719/1719 [==============================] - 17s 10ms/step - loss: 0.3168 - accuracy: 0.8851 - val_loss: 0.3439 - val_accuracy: 0.8790
Epoch 7/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.3055 - accuracy: 0.8895 - val_loss: 0.3291 - val_accuracy: 0.8878
Epoch 8/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2954 - accuracy: 0.8927 - val_loss: 0.3318 - val_accuracy: 0.8832
Epoch 9/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2857 - accuracy: 0.8964 - val_loss: 0.3485 - val_accuracy: 0.8844
Epoch 10/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2763 - accuracy: 0.8991 - val_loss: 0.3158 - val_accuracy: 0.8922
Epoch 11/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2679 - accuracy: 0.9031 - val_loss: 0.3350 - val_accuracy: 0.8876
Epoch 12/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2625 - accuracy: 0.9042 - val_loss: 0.3068 - val_accuracy: 0.8946
Epoch 13/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2548 - accuracy: 0.9080 - val_loss: 0.3108 - val_accuracy: 0.8912
Epoch 14/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2463 - accuracy: 0.9086 - val_loss: 0.3020 - val_accuracy: 0.8958
Epoch 15/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2395 - accuracy: 0.9113 - val_loss: 0.3358 - val_accuracy: 0.8926
Epoch 16/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2375 - accuracy: 0.9133 - val_loss: 0.3174 - val_accuracy: 0.8868
Epoch 17/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2308 - accuracy: 0.9155 - val_loss: 0.3335 - val_accuracy: 0.8958
Epoch 18/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2262 - accuracy: 0.9177 - val_loss: 0.3179 - val_accuracy: 0.8938
Epoch 19/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2202 - accuracy: 0.9197 - val_loss: 0.3548 - val_accuracy: 0.8902
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_leaky_p.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_leaky_p.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_23_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3284 - accuracy: 0.8881

[0.3283936679363251, 0.8881000280380249]
</code></pre><h1 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型构建</span></span><br><span class="line">model_elu = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_elu.add(keras.layers.Dense(<span class="number">50</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_elu.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_elu.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                  optimizer=optimizer, </span><br><span class="line">                  metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_elu = model_elu.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                            validation_data=(x_valid, y_valid), </span><br><span class="line">                            callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.5414 - accuracy: 0.8028 - val_loss: 0.4075 - val_accuracy: 0.8502
Epoch 2/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.4151 - accuracy: 0.8495 - val_loss: 0.4342 - val_accuracy: 0.8462
Epoch 3/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3814 - accuracy: 0.8612 - val_loss: 0.3812 - val_accuracy: 0.8566
Epoch 4/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3545 - accuracy: 0.8722 - val_loss: 0.3407 - val_accuracy: 0.8804
Epoch 5/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3349 - accuracy: 0.8782 - val_loss: 0.3579 - val_accuracy: 0.8732
Epoch 6/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3205 - accuracy: 0.8838 - val_loss: 0.3499 - val_accuracy: 0.8782
Epoch 7/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3114 - accuracy: 0.8874 - val_loss: 0.3223 - val_accuracy: 0.8888
Epoch 8/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2922 - accuracy: 0.8933 - val_loss: 0.3475 - val_accuracy: 0.8760
Epoch 9/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2860 - accuracy: 0.8956 - val_loss: 0.3181 - val_accuracy: 0.8852
Epoch 10/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2766 - accuracy: 0.8986 - val_loss: 0.3325 - val_accuracy: 0.8852
Epoch 11/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2655 - accuracy: 0.9024 - val_loss: 0.3199 - val_accuracy: 0.8880
Epoch 12/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2593 - accuracy: 0.9039 - val_loss: 0.3135 - val_accuracy: 0.8876
Epoch 13/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2522 - accuracy: 0.9080 - val_loss: 0.3080 - val_accuracy: 0.8890
Epoch 14/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2443 - accuracy: 0.9104 - val_loss: 0.3121 - val_accuracy: 0.8892
Epoch 15/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2380 - accuracy: 0.9121 - val_loss: 0.3497 - val_accuracy: 0.8898
Epoch 16/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2332 - accuracy: 0.9140 - val_loss: 0.3162 - val_accuracy: 0.8920
Epoch 17/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.2257 - accuracy: 0.9153 - val_loss: 0.3139 - val_accuracy: 0.8924
Epoch 18/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2221 - accuracy: 0.9174 - val_loss: 0.3250 - val_accuracy: 0.8894
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_elu.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_elu.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_27_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3421 - accuracy: 0.8789

[0.3421170711517334, 0.8788999915122986]
</code></pre><h1 id="SELU-构建的自归一化网络"><a href="#SELU-构建的自归一化网络" class="headerlink" title="SELU 构建的自归一化网络"></a>SELU 构建的自归一化网络</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型构建</span></span><br><span class="line">model_self_norm = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_self_norm.add(keras.layers.Dense(<span class="number">50</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_self_norm.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_self_norm.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入特征标准化 (μ=0, σ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_self_norm = model_self_norm.fit(x_train_scaled, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                        validation_data=(x_valid_scaled, y_valid), </span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.5155 - accuracy: 0.8188 - val_loss: 0.4051 - val_accuracy: 0.8532
Epoch 2/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.4026 - accuracy: 0.8584 - val_loss: 0.4220 - val_accuracy: 0.8562
Epoch 3/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.3683 - accuracy: 0.8692 - val_loss: 0.3650 - val_accuracy: 0.8694
Epoch 4/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3436 - accuracy: 0.8798 - val_loss: 0.3531 - val_accuracy: 0.8732
Epoch 5/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3180 - accuracy: 0.8861 - val_loss: 0.3311 - val_accuracy: 0.8786
Epoch 6/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3016 - accuracy: 0.8924 - val_loss: 0.3715 - val_accuracy: 0.8780
Epoch 7/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2899 - accuracy: 0.8969 - val_loss: 0.3542 - val_accuracy: 0.8738
Epoch 8/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2741 - accuracy: 0.9012 - val_loss: 0.3532 - val_accuracy: 0.8796
Epoch 9/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2634 - accuracy: 0.9064 - val_loss: 0.3524 - val_accuracy: 0.8822
Epoch 10/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2500 - accuracy: 0.9102 - val_loss: 0.3428 - val_accuracy: 0.8910
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制学习曲线</span></span><br><span class="line">pd.DataFrame(history_self_norm.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估模型</span></span><br><span class="line">model_self_norm.evaluate(x_test_scaled, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_32_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3677 - accuracy: 0.8702

[0.3677000403404236, 0.870199978351593]
</code></pre><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Activation</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Evaluation on x_test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid</td>
<td>11s/epoch</td>
<td>2 epochs</td>
<td>accuracy=0.1993</td>
</tr>
<tr>
<td>ReLU</td>
<td>11s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8789</td>
</tr>
<tr>
<td>Leaky ReLU</td>
<td>11s/epoch</td>
<td>9 epochs</td>
<td>accuracy=0.8749</td>
</tr>
<tr>
<td>PReLU</td>
<td>18s/epoch</td>
<td>14 epochs</td>
<td>accuracy=0.8881</td>
</tr>
<tr>
<td>ELU</td>
<td>11s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8789</td>
</tr>
<tr>
<td>SELU</td>
<td>11s/epoch</td>
<td>15 epochs</td>
<td>accuracy=0.8702</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><p>使用 Sigmoid 作为激活函数, 训练时 loss 几乎不下降, 模型无法收敛到一个有价值的解. 其原因很可能是训练期间出现了<strong>梯度消失</strong>.</p>
</li>
<li><p>使用 ReLU 作为(默认)激活函数是个不错的选择, 模型训练速度较快, 收敛速度适中, 最终的模型性能也很不错.</p>
</li>
<li><p>使用 PReLU 作为激活函数, <strong>训练速度显著下降</strong>, <strong>收敛稍慢</strong>, 但模型<strong>性能更好</strong>.</p>
</li>
<li><p>使用 SELU 作为激活函数, 构建自归一化网络, <strong>收敛速度最慢</strong>, 且模型<strong>性能稍差</strong>.</p>
</li>
<li><p>在训练 &lt;使用 ReLU 激活的&gt; 模型时, 似乎没有出现神经元”死亡”的情况, 并且作为 ReLU 改进的 Leaky ReLU &amp; ELU 在表现上和 ReLU 相差不多.</p>
</li>
</ol>
<p>🐒 待改进的点: 如何实现观察训练期间梯度向量的变化? 以明确梯度消失问题是否发生.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
</search>
