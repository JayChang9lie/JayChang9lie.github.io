<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Batch Normalization Layer or Self-Normalization Network</title>
    <url>/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/</url>
    <content><![CDATA[<p>â“ ä½¿ç”¨ <strong>è‡ªå½’ä¸€åŒ–ç½‘ç»œ</strong> or <strong>Batch Normalization</strong>?</p>
<ol>
<li><p>å¯¹æ¯”è‡ªå½’ä¸€åŒ–ç½‘ç»œä¸ä½¿ç”¨ BN å±‚çš„ç½‘ç»œåœ¨æ€§èƒ½ä¸Šçš„å·®å¼‚.</p>
</li>
<li><p>å¯¹æ¯” BN å±‚æ·»åŠ åœ¨ activation å‰ / åçš„åŒºåˆ«.</p>
<span id="more"></span>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>ğŸ”º é’ˆå¯¹ Fashion MNIST æ•°æ®é›†, å¼€å±•ä¸‹é¢çš„æµ‹è¯•.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å‡†å¤‡æ•°æ®é›† (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist ä¸­æ•°å­—æ ‡ç­¾å¯¹åº”çš„ç±»åˆ«åç§°</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å±•ç¤ºéƒ¨åˆ†è®­ç»ƒé›†å®ä¾‹</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m è¡Œ n åˆ—</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/output_6_0.png" alt="png"></p>
<h1 id="Selu-lecun-normal-æ„å»ºè‡ªå½’ä¸€åŒ–ç½‘ç»œ"><a href="#Selu-lecun-normal-æ„å»ºè‡ªå½’ä¸€åŒ–ç½‘ç»œ" class="headerlink" title="Selu + lecun_normal æ„å»ºè‡ªå½’ä¸€åŒ–ç½‘ç»œ"></a>Selu + lecun_normal æ„å»ºè‡ªå½’ä¸€åŒ–ç½‘ç»œ</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹æ„å»º</span></span><br><span class="line">model_self_norm = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_self_norm.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_self_norm.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_self_norm.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¾“å…¥ç‰¹å¾æ ‡å‡†åŒ– (Î¼=0, Ïƒ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è®­ç»ƒ</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_self_norm = model_self_norm.fit(x_train_scaled, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                        validation_data=(x_valid_scaled, y_valid), </span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 12s 6ms/step - loss: 0.5500 - accuracy: 0.8066 - val_loss: 0.4003 - val_accuracy: 0.8604
Epoch 2/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.4220 - accuracy: 0.8532 - val_loss: 0.4499 - val_accuracy: 0.8514
Epoch 3/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3881 - accuracy: 0.8645 - val_loss: 0.3875 - val_accuracy: 0.8640
Epoch 4/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3569 - accuracy: 0.8762 - val_loss: 0.3643 - val_accuracy: 0.8748
Epoch 5/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3400 - accuracy: 0.8797 - val_loss: 0.3468 - val_accuracy: 0.8820
Epoch 6/100
1719/1719 [==============================] - 9s 5ms/step - loss: 0.3196 - accuracy: 0.8894 - val_loss: 0.3662 - val_accuracy: 0.8776
Epoch 7/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3066 - accuracy: 0.8928 - val_loss: 0.3506 - val_accuracy: 0.8782
Epoch 8/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2969 - accuracy: 0.8979 - val_loss: 0.3716 - val_accuracy: 0.8692
Epoch 9/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2764 - accuracy: 0.9018 - val_loss: 0.3561 - val_accuracy: 0.8790
Epoch 10/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2659 - accuracy: 0.9062 - val_loss: 0.3311 - val_accuracy: 0.8884
Epoch 11/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2591 - accuracy: 0.9072 - val_loss: 0.3548 - val_accuracy: 0.8880
Epoch 12/100
1719/1719 [==============================] - 9s 5ms/step - loss: 0.2515 - accuracy: 0.9108 - val_loss: 0.3629 - val_accuracy: 0.8888
Epoch 13/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2436 - accuracy: 0.9137 - val_loss: 0.3941 - val_accuracy: 0.8850
Epoch 14/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2332 - accuracy: 0.9179 - val_loss: 0.3579 - val_accuracy: 0.8834
Epoch 15/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3577 - accuracy: 0.9130 - val_loss: 0.3526 - val_accuracy: 0.8842
Epoch 16/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2254 - accuracy: 0.9193 - val_loss: 0.3466 - val_accuracy: 0.8890
Epoch 17/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2018 - accuracy: 0.9274 - val_loss: 0.3625 - val_accuracy: 0.8858
Epoch 18/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.1970 - accuracy: 0.9298 - val_loss: 0.3882 - val_accuracy: 0.8884
Epoch 19/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2006 - accuracy: 0.9291 - val_loss: 0.3566 - val_accuracy: 0.8986
Epoch 20/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.1947 - accuracy: 0.9314 - val_loss: 0.3617 - val_accuracy: 0.8892
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_self_norm.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_self_norm.evaluate(x_test_scaled, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/output_11_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 3ms/step - loss: 0.3794 - accuracy: 0.8743

[0.3793765604496002, 0.8743000030517578]
</code></pre><h1 id="åœ¨æ¨¡å‹ä¸­ä½¿ç”¨-Batch-Normalization-Layer"><a href="#åœ¨æ¨¡å‹ä¸­ä½¿ç”¨-Batch-Normalization-Layer" class="headerlink" title="åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ Batch Normalization Layer"></a>åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ Batch Normalization Layer</h1><h2 id="Bach-Normalization-before-Activation"><a href="#Bach-Normalization-before-Activation" class="headerlink" title="Bach Normalization before Activation"></a>Bach Normalization before Activation</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹æ„å»º</span></span><br><span class="line">model_with_BN_1 = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, use_bias=<span class="literal">False</span>),    <span class="comment"># æ³¨æ„è¿™ä¸ª use_bias</span></span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    keras.layers.Activation(<span class="string">&#x27;elu&#x27;</span>),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, use_bias=<span class="literal">False</span>),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    keras.layers.Activation(<span class="string">&#x27;elu&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_BN_1.add(keras.layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, use_bias=<span class="literal">False</span>))</span><br><span class="line">    model_with_BN_1.add(keras.layers.BatchNormalization())</span><br><span class="line">    model_with_BN_1.add(keras.layers.Activation(<span class="string">&#x27;elu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model_with_BN_1.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_BN_1.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è®­ç»ƒ</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_with_BN_1 = model_with_BN_1.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                        validation_data=(x_valid, y_valid), </span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 30s 16ms/step - loss: 0.5438 - accuracy: 0.8053 - val_loss: 0.3970 - val_accuracy: 0.8510
Epoch 2/100
1719/1719 [==============================] - 28s 16ms/step - loss: 0.4381 - accuracy: 0.8407 - val_loss: 0.4060 - val_accuracy: 0.8542
Epoch 3/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.4067 - accuracy: 0.8514 - val_loss: 0.3232 - val_accuracy: 0.8804
Epoch 4/100
1719/1719 [==============================] - 28s 16ms/step - loss: 0.3749 - accuracy: 0.8631 - val_loss: 0.3409 - val_accuracy: 0.8736
Epoch 5/100
1719/1719 [==============================] - 27s 16ms/step - loss: 0.3516 - accuracy: 0.8704 - val_loss: 0.3216 - val_accuracy: 0.8800
Epoch 6/100
1719/1719 [==============================] - 27s 16ms/step - loss: 0.3404 - accuracy: 0.8761 - val_loss: 0.3147 - val_accuracy: 0.8888
Epoch 7/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.3288 - accuracy: 0.8798 - val_loss: 0.3098 - val_accuracy: 0.8834
Epoch 8/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.3136 - accuracy: 0.8834 - val_loss: 0.3211 - val_accuracy: 0.8826
Epoch 9/100
1719/1719 [==============================] - 30s 17ms/step - loss: 0.3023 - accuracy: 0.8878 - val_loss: 0.2975 - val_accuracy: 0.8892
Epoch 10/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.2896 - accuracy: 0.8927 - val_loss: 0.2917 - val_accuracy: 0.8922
Epoch 11/100
1719/1719 [==============================] - 30s 18ms/step - loss: 0.2821 - accuracy: 0.8952 - val_loss: 0.2928 - val_accuracy: 0.8924
Epoch 12/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.2743 - accuracy: 0.8987 - val_loss: 0.3111 - val_accuracy: 0.8838
Epoch 13/100
1719/1719 [==============================] - 30s 17ms/step - loss: 0.2668 - accuracy: 0.9009 - val_loss: 0.3185 - val_accuracy: 0.8824
Epoch 14/100
1719/1719 [==============================] - 30s 18ms/step - loss: 0.2566 - accuracy: 0.9049 - val_loss: 0.2922 - val_accuracy: 0.8916
Epoch 15/100
1719/1719 [==============================] - 28s 16ms/step - loss: 0.2510 - accuracy: 0.9069 - val_loss: 0.3094 - val_accuracy: 0.8888
Epoch 16/100
1719/1719 [==============================] - 28s 16ms/step - loss: 0.2467 - accuracy: 0.9097 - val_loss: 0.3134 - val_accuracy: 0.8882
Epoch 17/100
1719/1719 [==============================] - 28s 17ms/step - loss: 0.2394 - accuracy: 0.9103 - val_loss: 0.2938 - val_accuracy: 0.8966
Epoch 18/100
1719/1719 [==============================] - 30s 17ms/step - loss: 0.2302 - accuracy: 0.9138 - val_loss: 0.3070 - val_accuracy: 0.8932
Epoch 19/100
1719/1719 [==============================] - 29s 17ms/step - loss: 0.2260 - accuracy: 0.9156 - val_loss: 0.3005 - val_accuracy: 0.8936
Epoch 20/100
1719/1719 [==============================] - 30s 17ms/step - loss: 0.2203 - accuracy: 0.9171 - val_loss: 0.3001 - val_accuracy: 0.8912
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_with_BN_1.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_BN_1.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/output_16_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 6ms/step - loss: 0.3210 - accuracy: 0.8871

[0.3209632933139801, 0.8870999813079834]
</code></pre><h2 id="Bach-Normalization-after-Activation"><a href="#Bach-Normalization-after-Activation" class="headerlink" title="Bach Normalization after Activation"></a>Bach Normalization after Activation</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹æ„å»º</span></span><br><span class="line">model_with_BN_2 = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_BN_2.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_with_BN_2.add(keras.layers.BatchNormalization())</span><br><span class="line"></span><br><span class="line">model_with_BN_2.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_BN_2.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è®­ç»ƒ</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_with_BN_2 = model_with_BN_2.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                        validation_data=(x_valid, y_valid), </span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 33s 18ms/step - loss: 0.5615 - accuracy: 0.8028 - val_loss: 0.3802 - val_accuracy: 0.8562
Epoch 2/100
1719/1719 [==============================] - 33s 19ms/step - loss: 0.4428 - accuracy: 0.8419 - val_loss: 0.3715 - val_accuracy: 0.8642
Epoch 3/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.4041 - accuracy: 0.8523 - val_loss: 0.3343 - val_accuracy: 0.8738
Epoch 4/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.3682 - accuracy: 0.8662 - val_loss: 0.3374 - val_accuracy: 0.8800
Epoch 5/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.3444 - accuracy: 0.8755 - val_loss: 0.3160 - val_accuracy: 0.8816
Epoch 6/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.3294 - accuracy: 0.8813 - val_loss: 0.3069 - val_accuracy: 0.8904
Epoch 7/100
1719/1719 [==============================] - 33s 19ms/step - loss: 0.3173 - accuracy: 0.8849 - val_loss: 0.3125 - val_accuracy: 0.8866
Epoch 8/100
1719/1719 [==============================] - 32s 18ms/step - loss: 0.3008 - accuracy: 0.8905 - val_loss: 0.3078 - val_accuracy: 0.8880
Epoch 9/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.2882 - accuracy: 0.8941 - val_loss: 0.3044 - val_accuracy: 0.8936
Epoch 10/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2722 - accuracy: 0.8997 - val_loss: 0.2909 - val_accuracy: 0.8932
Epoch 11/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2666 - accuracy: 0.9024 - val_loss: 0.2970 - val_accuracy: 0.8962
Epoch 12/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2571 - accuracy: 0.9056 - val_loss: 0.2990 - val_accuracy: 0.8880
Epoch 13/100
1719/1719 [==============================] - 33s 19ms/step - loss: 0.2473 - accuracy: 0.9099 - val_loss: 0.2929 - val_accuracy: 0.8948
Epoch 14/100
1719/1719 [==============================] - 31s 18ms/step - loss: 0.2359 - accuracy: 0.9130 - val_loss: 0.3205 - val_accuracy: 0.8880
Epoch 15/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2314 - accuracy: 0.9144 - val_loss: 0.3113 - val_accuracy: 0.8898
Epoch 16/100
1719/1719 [==============================] - 33s 19ms/step - loss: 0.2254 - accuracy: 0.9165 - val_loss: 0.2950 - val_accuracy: 0.8924
Epoch 17/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2210 - accuracy: 0.9190 - val_loss: 0.2970 - val_accuracy: 0.8972s - loss: 0.2189 - accu
Epoch 18/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2137 - accuracy: 0.9211 - val_loss: 0.3025 - val_accuracy: 0.8932
Epoch 19/100
1719/1719 [==============================] - 32s 19ms/step - loss: 0.2077 - accuracy: 0.9241 - val_loss: 0.3028 - val_accuracy: 0.8970
Epoch 20/100
1719/1719 [==============================] - 32s 18ms/step - loss: 0.2007 - accuracy: 0.9259 - val_loss: 0.3103 - val_accuracy: 0.8912
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_with_BN_2.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_BN_2.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/Batch-Normalization-Layer-or-Self-Normalization-Network/output_20_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 7ms/step - loss: 0.3176 - accuracy: 0.8904

[0.317624568939209, 0.8903999924659729]
</code></pre><h1 id="ç»“è®º"><a href="#ç»“è®º" class="headerlink" title="ç»“è®º"></a>ç»“è®º</h1><div class="table-container">
<table>
<thead>
<tr>
<th>ç½‘ç»œç±»å‹</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Evaluation on x_test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Selu + lecun normal æ„å»ºã®è‡ªå½’ä¸€åŒ–ç½‘ç»œ</td>
<td>10s/epoch</td>
<td>10 epochs</td>
<td>accuracy=0.8743</td>
</tr>
<tr>
<td>ELU + he normal (BN å±‚åœ¨æ¿€æ´»å‡½æ•°å‰)</td>
<td>29s/epoch</td>
<td>10 epochs</td>
<td>accuracy=0.8871</td>
</tr>
<tr>
<td>ELU + he normal (BN å±‚åœ¨æ¿€æ´»å‡½æ•°å)</td>
<td>32s/epoch</td>
<td>10 epochs</td>
<td>accuracy=0.8904</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>è‡ªå½’ä¸€åŒ–ç½‘ç»œ, <strong>è®­ç»ƒé€Ÿåº¦å¾ˆå¿«</strong>, æ”¶æ•›é€Ÿåº¦å¾ˆå¿«, æ¨¡å‹æ€§èƒ½æ¯”ä½¿ç”¨ BN å±‚çš„æ¨¡å‹ç¨å·®.</li>
<li>ä½¿ç”¨ BN å±‚çš„æ¨¡å‹, <strong>è®­ç»ƒé€Ÿåº¦æ˜¾è‘—é™ä½</strong>, æ”¶æ•›é€Ÿåº¦å¾ˆå¿«, è·å¾—çš„<strong>æ¨¡å‹ç•¥å¥½äºè‡ªå½’ä¸€åŒ–ç½‘ç»œæ¨¡å‹</strong>.</li>
<li>BN å±‚æ·»åŠ åœ¨æ¿€æ´»å‡½æ•°å‰ / å, åŒºåˆ«ä¸å¤ªæ˜æ˜¾.</li>
</ol>
<p>ğŸ’ å¾…æ”¹è¿›çš„ç‚¹: å¯ä»¥å°è¯•ä½¿ç”¨ä¸åŒçš„ learning rate</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Better Plots With pairplot and PairGrid</title>
    <url>/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/</url>
    <content><![CDATA[<p>ä½¿ç”¨ seaborn ç»˜åˆ¶ç¾è§‚çš„ pairplot  å’Œ PairGrid ~</p>
<p>åŸå¸–åœ°å€: <a href="https://www.pythonheidong.com/blog/article/493964/5fb6bf7e2dd6e667a6a3/">https://www.pythonheidong.com/blog/article/493964/5fb6bf7e2dd6e667a6a3/</a></p>
<p>ä¿®æ”¹äº†å…¶ä¸­çš„é”™è¯¯, å¹¶æ ¹æ®ç†è§£æ·»åŠ äº†äº›è®¸æ³¨é‡Š.</p>
<span id="more"></span>
<h1 id="æ•°æ®å‡†å¤‡"><a href="#æ•°æ®å‡†å¤‡" class="headerlink" title="æ•°æ®å‡†å¤‡"></a>æ•°æ®å‡†å¤‡</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 0.common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">x, y = iris.data, iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä½¿ç”¨ç±»åˆ«åç§°ä½œä¸ºæ ‡ç­¾</span></span><br><span class="line">y_1 = np.array([<span class="string">&#x27;setosa&#x27;</span> <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;versicolor&#x27;</span> <span class="keyword">if</span> i==<span class="number">1</span> <span class="keyword">else</span> <span class="string">&#x27;virginica&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> y])</span><br><span class="line"></span><br><span class="line">column_names = [</span><br><span class="line">    <span class="string">&#x27;sepal length(cm)&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;sepal width(cm)&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;petal length(cm)&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;petal width(cm)&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;class&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># å°†ç‰¹å¾å’Œæ ‡ç­¾æ•´ç†è‡³ DataFrame ä¸­</span></span><br><span class="line">pd_iris = pd.DataFrame(np.hstack([x, y_1.reshape(<span class="number">150</span>, <span class="number">1</span>)]), columns=column_names)</span><br><span class="line"></span><br><span class="line">display(pd_iris.head())</span><br><span class="line">display(pd_iris.info())</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¿®æ”¹æ•°å€¼ç‰¹å¾çš„ Dtype</span></span><br><span class="line"><span class="keyword">for</span> feature_name <span class="keyword">in</span> column_names[:<span class="number">4</span>]:</span><br><span class="line">    pd_iris[feature_name] = pd_iris[feature_name].astype(np.float64)</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length(cm)</th>
      <th>sepal width(cm)</th>
      <th>petal length(cm)</th>
      <th>petal width(cm)</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>

</div>


<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 150 entries, 0 to 149
Data columns (total 5 columns):
 #   Column            Non-Null Count  Dtype 
---  ------            --------------  ----- 
 0   sepal length(cm)  150 non-null    object
 1   sepal width(cm)   150 non-null    object
 2   petal length(cm)  150 non-null    object
 3   petal width(cm)   150 non-null    object
 4   class             150 non-null    object
dtypes: object(5)
memory usage: 6.0+ KB



None
</code></pre><h1 id="seaborn-pairplot-æ–¹æ³•"><a href="#seaborn-pairplot-æ–¹æ³•" class="headerlink" title="seaborn.pairplot æ–¹æ³•"></a>seaborn.pairplot æ–¹æ³•</h1><h2 id="åŸºç¡€ç”¨æ³•"><a href="#åŸºç¡€ç”¨æ³•" class="headerlink" title="åŸºç¡€ç”¨æ³•"></a>åŸºç¡€ç”¨æ³•</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris)    <span class="comment"># ç±»ä¼¼ pandas ä¸­çš„ scatter_matrix()</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)    <span class="comment"># è®¾ç½®ç”»å¸ƒå¤§å°</span></span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)    <span class="comment"># ç½‘æ ¼ + å­—ä½“ç¼©æ”¾å°ºåº¦</span></span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_6_0.png" alt="png"></p>
<h2 id="æŒ‰ç±»åˆ«è®¾ç½®æ•°æ®è‰²è°ƒ"><a href="#æŒ‰ç±»åˆ«è®¾ç½®æ•°æ®è‰²è°ƒ" class="headerlink" title="æŒ‰ç±»åˆ«è®¾ç½®æ•°æ®è‰²è°ƒ"></a>æŒ‰ç±»åˆ«è®¾ç½®æ•°æ®è‰²è°ƒ</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, </span><br><span class="line">                 hue=<span class="string">&#x27;class&#x27;</span>)    <span class="comment"># æ ¹æ® dataframe ä¸­çš„ class å­—æ®µè®¾ç½® hue(è‰²è°ƒ)</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_8_0.png" alt="png"></p>
<h2 id="ä¿®æ”¹è°ƒè‰²ç›˜"><a href="#ä¿®æ”¹è°ƒè‰²ç›˜" class="headerlink" title="ä¿®æ”¹è°ƒè‰²ç›˜"></a>ä¿®æ”¹è°ƒè‰²ç›˜</h2><p>å¯ä½¿ç”¨ Matplotlib, seaborn, é¢œè‰²å· list ç­‰è‰²ç›˜.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># !pip3 install palettable</span></span><br><span class="line"><span class="keyword">import</span> palettable </span><br><span class="line"></span><br><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, </span><br><span class="line">                palette=palettable.cartocolors.qualitative.Bold_9.mpl_colors[<span class="number">3</span>:<span class="number">6</span>])    <span class="comment"># palettable é¢œè‰²ç›˜</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_10_0.png" alt="png"></p>
<p><strong>Remark</strong>: åœ¨ pairplot() ä¸­è®¾ç½® hue å‚æ•°å, palette éœ€è¦æŒ‡å®šåŒæ ·æ•°é‡çš„è‰²å½©, ä¾‹å¦‚ä¸Šé¢ hue=â€™classâ€™, ç”±äº class ä¸­åªæœ‰ä¸‰ä¸ªç±»åˆ«, å› æ­¤ palette=palettable.cartocolors.qualitative.Bold_9.mpl_colors[3:6] åº”ä½¿ç”¨åˆ‡ç‰‡é€‰å–ä¸‰ç§é¢œè‰². </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>)    <span class="comment"># Matplotlib é¢œè‰²</span></span><br><span class="line">                </span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_12_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, </span><br><span class="line">                 palette=[<span class="string">&#x27;#dc2624&#x27;</span>, <span class="string">&#x27;#2b4750&#x27;</span>, <span class="string">&#x27;#45a0a2&#x27;</span>])    <span class="comment"># ä½¿ç”¨æŒ‡å®šé¢œè‰²åˆ—è¡¨</span></span><br><span class="line">                </span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_13_0.png" alt="png"></p>
<h2 id="x-y-è½´ä¸ŠæŒ‡å®šç›¸åŒç‰¹å¾è¿›è¡Œç»˜å›¾"><a href="#x-y-è½´ä¸ŠæŒ‡å®šç›¸åŒç‰¹å¾è¿›è¡Œç»˜å›¾" class="headerlink" title="x, y è½´ä¸ŠæŒ‡å®šç›¸åŒç‰¹å¾è¿›è¡Œç»˜å›¾"></a>x, y è½´ä¸ŠæŒ‡å®šç›¸åŒç‰¹å¾è¿›è¡Œç»˜å›¾</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 <span class="built_in">vars</span>=[<span class="string">&#x27;sepal length(cm)&#x27;</span>,<span class="string">&#x27;sepal width(cm)&#x27;</span>])    <span class="comment"># æŒ‡å®šè¦ä½¿ç”¨çš„ç‰¹å¾</span></span><br><span class="line">                </span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">6</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_15_0.png" alt="png"></p>
<h2 id="x-y-è½´ä¸ŠæŒ‡å®šä¸åŒç‰¹å¾è¿›è¡Œç»˜å›¾"><a href="#x-y-è½´ä¸ŠæŒ‡å®šä¸åŒç‰¹å¾è¿›è¡Œç»˜å›¾" class="headerlink" title="x, y è½´ä¸ŠæŒ‡å®šä¸åŒç‰¹å¾è¿›è¡Œç»˜å›¾"></a>x, y è½´ä¸ŠæŒ‡å®šä¸åŒç‰¹å¾è¿›è¡Œç»˜å›¾</h2><p>æ³¨æ„æ­¤æ—¶æ²¡æœ‰ä¸€ä¸ªç‰¹å¾å¯¹è‡ªå·±çš„æ•£ç‚¹å›¾äº†, å› æ­¤å¯¹è§’çº¿ä¸Šæ²¡æœ‰ kde æ›²çº¿äº†.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 x_vars=[<span class="string">&#x27;sepal length(cm)&#x27;</span>, <span class="string">&#x27;sepal width(cm)&#x27;</span>],    <span class="comment"># æŒ‡å®š x è½´ä¸Šè¦ä½¿ç”¨çš„ç‰¹å¾ </span></span><br><span class="line">                 y_vars=[<span class="string">&#x27;petal length(cm)&#x27;</span>, <span class="string">&#x27;petal width(cm)&#x27;</span>])    <span class="comment"># æŒ‡å®š y è½´ä¸Šè¦ä½¿ç”¨çš„ç‰¹å¾</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">6</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_17_0.png" alt="png"></p>
<h2 id="æ•£ç‚¹å›¾-å›å½’ç›´çº¿"><a href="#æ•£ç‚¹å›¾-å›å½’ç›´çº¿" class="headerlink" title="æ•£ç‚¹å›¾ + å›å½’ç›´çº¿"></a>æ•£ç‚¹å›¾ + å›å½’ç›´çº¿</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 kind=<span class="string">&#x27;reg&#x27;</span>)    <span class="comment"># åœ¨åŸæœ¬çš„æ•£ç‚¹å›¾ä¸Šç»˜åˆ¶å›å½’ç›´çº¿                 </span></span><br><span class="line">                </span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_19_0.png" alt="png"></p>
<h2 id="è®¾ç½®å¯¹è§’çº¿ä¸Šå›¾è¡¨ç±»å‹"><a href="#è®¾ç½®å¯¹è§’çº¿ä¸Šå›¾è¡¨ç±»å‹" class="headerlink" title="è®¾ç½®å¯¹è§’çº¿ä¸Šå›¾è¡¨ç±»å‹"></a>è®¾ç½®å¯¹è§’çº¿ä¸Šå›¾è¡¨ç±»å‹</h2><p>å¯é€‰å‚æ•°ä¸º: â€˜autoâ€™, â€˜histâ€™(é»˜è®¤), â€˜kdeâ€™, None.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 diag_kind=<span class="string">&#x27;hist&#x27;</span>)    <span class="comment"># å¯¹è§’çº¿ä¸Šç»˜åˆ¶å›¾è¡¨ç±»å‹: ç›´æ–¹å›¾               </span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_21_0.png" alt="png"></p>
<h2 id="åªæ˜¾ç¤ºå¯¹è§’çº¿ä»¥ä¸‹çš„æ•£ç‚¹å›¾"><a href="#åªæ˜¾ç¤ºå¯¹è§’çº¿ä»¥ä¸‹çš„æ•£ç‚¹å›¾" class="headerlink" title="åªæ˜¾ç¤ºå¯¹è§’çº¿ä»¥ä¸‹çš„æ•£ç‚¹å›¾"></a>åªæ˜¾ç¤ºå¯¹è§’çº¿ä»¥ä¸‹çš„æ•£ç‚¹å›¾</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 corner=<span class="literal">True</span>)    <span class="comment"># åªä¿ç•™å¯¹è§’çº¿ä»¥ä¸‹çš„æ•£ç‚¹å›¾</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_23_0.png" alt="png"></p>
<h2 id="å›¾å½¢å¤–è§‚è®¾ç½®"><a href="#å›¾å½¢å¤–è§‚è®¾ç½®" class="headerlink" title="å›¾å½¢å¤–è§‚è®¾ç½®"></a>å›¾å½¢å¤–è§‚è®¾ç½®</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(</span><br><span class="line">    pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">    markers=[<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;h&#x27;</span>],     <span class="comment"># æ•£ç‚¹å›¾ (å¡«å……å‹) marker</span></span><br><span class="line">    plot_kws=<span class="built_in">dict</span>(s=<span class="number">50</span>, edgecolor=<span class="string">&quot;k&quot;</span>, linewidth=<span class="number">0.6</span>),    <span class="comment"># æ•£ç‚¹å›¾ marker å¤§å°, å¤–æ¡†é¢œè‰², çº¿å®½</span></span><br><span class="line">    diag_kws=<span class="built_in">dict</span>(shade=<span class="literal">True</span>)    <span class="comment"># kde å›¾æ˜¯å¦å¡«å……</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_25_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.pairplot(</span><br><span class="line">    pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">    markers=[<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;+&#x27;</span>, <span class="string">&#x27;x&#x27;</span>],     <span class="comment"># æ•£ç‚¹å›¾ (çº¿å‹) marker</span></span><br><span class="line">    plot_kws=<span class="built_in">dict</span>(s=<span class="number">50</span>, linewidth=<span class="number">1.5</span>),    <span class="comment"># æ•£ç‚¹å›¾ marker å¤§å°, çº¿å®½</span></span><br><span class="line">    diag_kws=<span class="built_in">dict</span>(shade=<span class="literal">True</span>)    <span class="comment"># kde å›¾æ˜¯å¦å¡«å……</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_26_0.png" alt="png"></p>
<p><strong>Remark</strong>: åœ¨è®¾ç½® pairplot() ä¸­çš„ markers å‚æ•°æ—¶, ä¸å…è®¸å°† <strong>å¡«å……æ ‡è®°</strong> å’Œ <strong>çº¿æ ‡è®°</strong> æ··åˆä½¿ç”¨!</p>
<pre><code>1. å¡«å……æ ‡è®°æœ‰: &quot;o&quot;, &quot;v&quot;, &quot;^&quot;, &quot;&lt;&quot;, &quot;&gt;&quot;, &quot;8&quot;, &quot;s&quot;, &quot;p&quot;, &quot;P&quot;, &quot;*&quot;, &quot;h&quot;, &quot;H&quot;, &quot;X&quot;, &quot;D&quot;, &quot;d&quot;;
2. çº¿æ ‡è®°æœ‰: &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;+&quot;, &quot;x&quot;.
</code></pre><h1 id="seaborn-PairGrid-æ–¹æ³•"><a href="#seaborn-PairGrid-æ–¹æ³•" class="headerlink" title="seaborn.PairGrid æ–¹æ³•"></a>seaborn.PairGrid æ–¹æ³•</h1><h2 id="æ¯ä¸ªå­å›¾å‡ç»˜åˆ¶æ•£ç‚¹å›¾"><a href="#æ¯ä¸ªå­å›¾å‡ç»˜åˆ¶æ•£ç‚¹å›¾" class="headerlink" title="æ¯ä¸ªå­å›¾å‡ç»˜åˆ¶æ•£ç‚¹å›¾"></a>æ¯ä¸ªå­å›¾å‡ç»˜åˆ¶æ•£ç‚¹å›¾</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;husl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">g.<span class="built_in">map</span>(plt.scatter)    <span class="comment"># è®¾ç½®æ¯ä¸ªå­å›¾å‡ä¸ºæ•£ç‚¹å›¾</span></span><br><span class="line">g.add_legend()        <span class="comment"># æ·»åŠ å›¾ä¾‹</span></span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_30_0.png" alt="png"></p>
<h2 id="åˆ†åˆ«è®¾ç½®å¯¹è§’çº¿ä¸Šå­å›¾ç±»å‹-amp-éå¯¹è§’çº¿ä¸Šå­å›¾ç±»å‹"><a href="#åˆ†åˆ«è®¾ç½®å¯¹è§’çº¿ä¸Šå­å›¾ç±»å‹-amp-éå¯¹è§’çº¿ä¸Šå­å›¾ç±»å‹" class="headerlink" title="åˆ†åˆ«è®¾ç½®å¯¹è§’çº¿ä¸Šå­å›¾ç±»å‹ &amp; éå¯¹è§’çº¿ä¸Šå­å›¾ç±»å‹"></a>åˆ†åˆ«è®¾ç½®å¯¹è§’çº¿ä¸Šå­å›¾ç±»å‹ &amp; éå¯¹è§’çº¿ä¸Šå­å›¾ç±»å‹</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">g.map_diag(plt.hist)          <span class="comment"># å¯¹è§’çº¿ç»˜åˆ¶ç›´æ–¹å›¾</span></span><br><span class="line">g.map_offdiag(plt.scatter)    <span class="comment"># éå¯¹è§’çº¿ç»˜åˆ¶æ•£ç‚¹å›¾</span></span><br><span class="line">g.add_legend()</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_32_0.png" alt="png"></p>
<h2 id="å¯¹è§’çº¿ä¸Šæ–¹ã€å¯¹è§’çº¿ä¸‹æ–¹ã€å¯¹è§’çº¿ä¹‹ä¸Šåˆ†åˆ«æŒ‡å®šå­å›¾ç±»å‹"><a href="#å¯¹è§’çº¿ä¸Šæ–¹ã€å¯¹è§’çº¿ä¸‹æ–¹ã€å¯¹è§’çº¿ä¹‹ä¸Šåˆ†åˆ«æŒ‡å®šå­å›¾ç±»å‹" class="headerlink" title="å¯¹è§’çº¿ä¸Šæ–¹ã€å¯¹è§’çº¿ä¸‹æ–¹ã€å¯¹è§’çº¿ä¹‹ä¸Šåˆ†åˆ«æŒ‡å®šå­å›¾ç±»å‹"></a>å¯¹è§’çº¿ä¸Šæ–¹ã€å¯¹è§’çº¿ä¸‹æ–¹ã€å¯¹è§’çº¿ä¹‹ä¸Šåˆ†åˆ«æŒ‡å®šå­å›¾ç±»å‹</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>)</span><br><span class="line"></span><br><span class="line">g.map_upper(sns.scatterplot)    <span class="comment"># å¯¹è§’çº¿ä»¥ä¸Š, ç»˜åˆ¶æ•£ç‚¹å›¾</span></span><br><span class="line">g.map_lower(sns.kdeplot)        <span class="comment"># å¯¹è§’çº¿ä»¥ä¸‹, ç»˜åˆ¶ kde å›¾</span></span><br><span class="line">g.map_diag(sns.kdeplot, lw=<span class="number">2</span>)   <span class="comment"># å¯¹è§’çº¿ä¹‹ä¸Š, ç»˜åˆ¶ kde å›¾</span></span><br><span class="line">g.add_legend()</span><br><span class="line"></span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_34_0.png" alt="png"></p>
<h2 id="å…¶å®ƒä¸€äº›å‚æ•°ä¿®æ”¹"><a href="#å…¶å®ƒä¸€äº›å‚æ•°ä¿®æ”¹" class="headerlink" title="å…¶å®ƒä¸€äº›å‚æ•°ä¿®æ”¹"></a>å…¶å®ƒä¸€äº›å‚æ•°ä¿®æ”¹</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 hue_kws=&#123;<span class="string">&quot;marker&quot;</span>: [<span class="string">&quot;o&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;D&quot;</span>]&#125;,</span><br><span class="line">                 diag_sharey=<span class="literal">True</span>)    <span class="comment"># æ˜¯å¦å…±äº«å¯¹è§’çº¿ä¸Š kde å›¾çš„ y è½´å°ºåº¦</span></span><br><span class="line"></span><br><span class="line">g.map_upper(sns.scatterplot, edgecolor=<span class="string">&quot;k&quot;</span>, s=<span class="number">45</span>)      <span class="comment"># è®¾ç½®ç‚¹å¤§å°, è¾¹æ¡†é¢œè‰²</span></span><br><span class="line">g.map_lower(sns.kdeplot)</span><br><span class="line">g.map_diag(sns.kdeplot, lw=<span class="number">2.5</span>)                        <span class="comment"># è®¾ç½®å¯¹è§’çº¿ä¸Š kde å›¾çš„çº¿å®½</span></span><br><span class="line">g.add_legend()</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_36_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = sns.PairGrid(pd_iris, hue=<span class="string">&#x27;class&#x27;</span>, palette=<span class="string">&#x27;Set1&#x27;</span>,</span><br><span class="line">                 hue_kws=&#123;<span class="string">&quot;marker&quot;</span>: [<span class="string">&quot;o&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;D&quot;</span>]&#125;,</span><br><span class="line">                 diag_sharey=<span class="literal">False</span>)    <span class="comment"># ä¸å…±äº«å¯ä»¥æ›´&quot;é¥±æ»¡&quot;åœ°å±•ç¤ºæ¯æ¡ kde æ›²çº¿</span></span><br><span class="line"></span><br><span class="line">g.map_upper(sns.scatterplot, edgecolor=<span class="string">&quot;k&quot;</span>, s=<span class="number">45</span>)</span><br><span class="line">g.map_lower(sns.kdeplot)</span><br><span class="line">g.map_diag(sns.kdeplot, lw=<span class="number">2.5</span>)</span><br><span class="line">g.add_legend()</span><br><span class="line"></span><br><span class="line">g.fig.set_size_inches(<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/03/Better-Plots-With-pairplot-and-PairGrid/output_37_0.png" alt="png"></p>
<p><strong>Remark</strong>:</p>
<pre><code>1. diag_sharey ä»£è¡¨åœ¨å°è§’ç·šä¸Šçš„åœ–å…±ç”¨ y è»¸å”·ã€‚é è¨­æ‡‰è©²æ˜¯ True, æ‰€ä»¥ç•¶è®Šæˆ False å¾Œ, æ¯å€‹åœ–æœƒè‡ªå‹•èª¿æ•´æˆæ¯”è¼ƒå¥½çš„é¡¯ç¤ºç‹€æ…‹ã€‚
2. hue_kws=&#123;&quot;marker&quot;: [&quot;o&quot;, &quot;s&quot;, &quot;D&quot;]&#125; ä¸èµ·ä½œç”¨æ˜¯å› ä¸ºå¯¹è§’çº¿ä¸Šæ–¹ã€å¯¹è§’çº¿ä¸‹æ–¹å’Œå¯¹è§’çº¿ä¹‹ä¸Šçš„å­å›¾æ˜¯åˆ†åˆ«ç»˜åˆ¶çš„, å¦‚æœæ”¹æˆå•ä¸ª g.map(plt.scatter, linewidths=1, edgecolor=&quot;w&quot;, s=40) åˆ™å¯ä»¥çœ‹åˆ° marker çš„æ•ˆæœ.
</code></pre>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Ensemble Learning Methods</title>
    <url>/2023/03/08/Ensemble-Learning-Methods/</url>
    <content><![CDATA[<p><strong>Ensemble Learning Methods</strong></p>
<p><img src="/2023/03/08/Ensemble-Learning-Methods/Ensemble Learning and Random Forests.jpg" style="width: 1200px;" align="center"></p>
<span id="more"></span>
<h1 id="Voting-Classifiers"><a href="#Voting-Classifiers" class="headerlink" title="Voting Classifiers"></a>Voting Classifiers</h1><h2 id="Hard-Voting-Classifier"><a href="#Hard-Voting-Classifier" class="headerlink" title="Hard Voting Classifier"></a>Hard Voting Classifier</h2><ol>
<li>ä»¥<strong>å¾—ç¥¨æ•°æœ€é«˜</strong>çš„ç±»åˆ«æœ€ä¸ºæœ€ç»ˆé¢„æµ‹ç±»åˆ«ï¼›<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-2.png" style="width: 300px;" align="center"></li>
</ol>
<hr>
<ol>
<li>Remarkï¼š<ol>
<li>ç†è®ºä¸Šï¼Œåªè¦æœ‰è¶³å¤Ÿå¤šã€è¶³å¤Ÿä¸åŒçš„å¼±å­¦ä¹ å™¨ï¼Œå°†å®ƒä»¬é›†æˆåœ¨ä¸€èµ·å°±å¯èƒ½è·å¾—ä¸€ä¸ª<strong>å¼ºå­¦ä¹ å™¨</strong></li>
<li><strong>å¼±å­¦ä¹ å™¨</strong>æ˜¯æŒ‡æ¯”ççŒœå¼ºä¸€ç‚¹å„¿çš„å­¦ä¹ å™¨</li>
<li>å…³äº <strong>å°†å¼±å­¦ä¹ å™¨é›†æˆä¸ºå¼ºå­¦ä¹ å™¨</strong> çš„ç†è®ºä¸­æœ‰ä¸€ä¸ª<strong>å‡è®¾</strong>ï¼šå­¦ä¹ å™¨å¿…é¡»æ˜¯<strong>ç‹¬ç«‹çš„</strong>ï¼Œå®ƒä»¬æ‰€çŠ¯çš„é”™è¯¯äº’ä¸ç›¸å…³</li>
<li>å½“å„å­¦ä¹ å™¨åœ¨åŒä¸€æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶ï¼Œè¯¥å‡è®¾å¹¶ä¸æˆç«‹</li>
<li>å½“å­¦ä¹ å™¨å°½å¯èƒ½ç›¸äº’ç‹¬ç«‹æ—¶ï¼Œé›†æˆæ–¹æ³•çš„æ•ˆæœæœ€å¥½</li>
<li>ä½¿ç”¨å®Œå…¨ä¸åŒçš„ç®—æ³•è®­ç»ƒå¾—åˆ°çš„å­¦ä¹ å™¨å…·æœ‰ <strong>å¤šæ ·æ€§</strong>ï¼Œè¿™èƒ½å¤Ÿå¢åŠ å­¦ä¹ å™¨çŠ¯ä¸åŒç±»å‹é”™è¯¯çš„æ¦‚ç‡</li>
</ol>
</li>
</ol>
<h3 id="Imports"><a href="#Imports" class="headerlink" title="Imports"></a>Imports</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_openml</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> xgboost</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, ExtraTreesClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor, GradientBoostingClassifier</span><br></pre></td></tr></table></figure>
<h3 id="Code-Example-1"><a href="#Code-Example-1" class="headerlink" title="Code Example 1"></a>Code Example 1</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. å‡†å¤‡ &amp; å±•ç¤ºæ•°æ®é›†</span></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">500</span>, noise=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">42</span>)</span><br><span class="line">data = pd.DataFrame(np.c_[X, y])</span><br><span class="line">data.columns = [<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line">sns.scatterplot(x=<span class="string">&#x27;x&#x27;</span>, y=<span class="string">&#x27;y&#x27;</span>, data=data, hue=<span class="string">&#x27;label&#x27;</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. åˆ›å»º &amp; è®­ç»ƒï¼ˆç¡¬ï¼‰æŠ•ç¥¨åˆ†ç±»å™¨</span></span><br><span class="line">log_clf = LogisticRegression(solver=<span class="string">&quot;lbfgs&quot;</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rnd_clf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line">svm_clf = SVC(gamma=<span class="string">&quot;scale&quot;</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">voting_clf = VotingClassifier(</span><br><span class="line">    estimators=[(<span class="string">&#x27;lr&#x27;</span>, log_clf), (<span class="string">&#x27;rf&#x27;</span>, rnd_clf), (<span class="string">&#x27;svc&#x27;</span>, svm_clf)],</span><br><span class="line">    voting=<span class="string">&#x27;hard&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. è®­ç»ƒ &amp; è¯„ä¼°æ¨¡å‹</span></span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;clf.__class__.__name__&#125;</span>: <span class="subst">&#123;<span class="built_in">round</span>(accuracy_score(y_test, y_pred), <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2023/03/08/Ensemble-Learning-Methods/output_8_0.png" alt="png"></p>
<pre><code>LogisticRegression: 0.864
RandomForestClassifier: 0.896
SVC: 0.896
VotingClassifier: 0.912
</code></pre><p>Remarkï¼š</p>
<pre><code>  1. äº‹å®ä¸Šï¼Œè‹¥å°† make_moons() ä¸­çš„ noise å‚æ•°å€¼é™ä½ï¼Œä¹Ÿæœ‰å¯èƒ½å‡ºç° SVC åˆ†ç±»å‡†ç¡®ç‡æœ€é«˜çš„æƒ…å½¢
  2. å½“ N ä¸ªå­¦ä¹ å™¨ä¸­æœ‰ 1 ä¸ªçš„æ•ˆæœæ˜æ˜¾ä¼˜äºå…¶å®ƒå­¦ä¹ å™¨æ—¶ï¼Œé€šè¿‡ Voting å¾—åˆ°çš„å­¦ä¹ å™¨æœªå¿…æ€»ä¼˜äºæ‰€æœ‰åŸºç¡€å­¦ä¹ å™¨
  3. ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨æŠ•ç¥¨è¿‡ç¨‹ä¸­ï¼Œæ•ˆæœæœ€å¥½çš„å­¦ä¹ å™¨å¯èƒ½ä¼šå—åˆ°å…¶å®ƒå­¦ä¹ å™¨çš„ **æ‹–ç´¯**
  4. å½“å­¦ä¹ å™¨æ•ˆæœéƒ½å·®ä¸å¤šæ—¶ï¼Œé€šå¸¸ Voting å¾—åˆ°çš„å­¦ä¹ å™¨æ•ˆæœä¼šæ›´å¥½
</code></pre><h2 id="Soft-Voting-Classifier"><a href="#Soft-Voting-Classifier" class="headerlink" title="Soft Voting Classifier"></a>Soft Voting Classifier</h2><ol>
<li><strong>è½¯æŠ•ç¥¨</strong> æ˜¯æŒ‡åœ¨æ‰€æœ‰å­¦ä¹ å™¨é¢„æµ‹çš„<strong>æ¦‚ç‡</strong>ä¸Šæ±‚å¹³å‡ï¼Œç„¶åæŒ‰æœ€é«˜å¹³å‡æ¦‚ç‡ç»™å‡ºæœ€ç»ˆé¢„æµ‹çš„é›†æˆæ–¹æ³•</li>
</ol>
<hr>
<ol>
<li>Remarkï¼š<ol>
<li>ä½¿ç”¨ soft voting éœ€ç¡®ä¿æ‰€æœ‰å­¦ä¹ å™¨éƒ½èƒ½é¢„æµ‹ç±»åˆ«æ¦‚ç‡</li>
<li>soft voting é€šå¸¸æ¯” hard voting å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½</li>
</ol>
</li>
</ol>
<h3 id="Code-Example-2"><a href="#Code-Example-2" class="headerlink" title="Code Example 2"></a>Code Example 2</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†ï¼Œä½†æ˜¯ Soft Voting</span></span><br><span class="line">log_clf = LogisticRegression(solver=<span class="string">&quot;lbfgs&quot;</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rnd_clf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line">svm_clf = SVC(gamma=<span class="string">&quot;scale&quot;</span>, probability=<span class="literal">True</span>, random_state=<span class="number">42</span>)    <span class="comment"># probability=True å¯ä½¿ SVC è¾“å‡ºç±»æ¦‚ç‡</span></span><br><span class="line"></span><br><span class="line">voting_clf = VotingClassifier(</span><br><span class="line">    estimators=[(<span class="string">&#x27;lr&#x27;</span>, log_clf), (<span class="string">&#x27;rf&#x27;</span>, rnd_clf), (<span class="string">&#x27;svc&#x27;</span>, svm_clf)],</span><br><span class="line">    voting=<span class="string">&#x27;soft&#x27;</span>    <span class="comment"># è®¾ç½®ä¸º Soft Voting</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(clf.__class__.__name__, accuracy_score(y_test, y_pred))</span><br></pre></td></tr></table></figure>
<pre><code>LogisticRegression 0.864
RandomForestClassifier 0.896
SVC 0.896
VotingClassifier 0.92
</code></pre><p>Remarkï¼š</p>
<pre><code>  1. ä»ä¸Šä¾‹å‘ç°ï¼ŒSoft Voting çš„é¢„æµ‹å‡†ç¡®ç‡çš„ç¡®é«˜äº Hard Voting
  2. å³ä½¿æ˜¯ Soft Voting ä»å¯èƒ½å‡ºç° **è¢«æ‹–ç´¯** çš„ç°è±¡
</code></pre><h1 id="Bagging-and-Pasting"><a href="#Bagging-and-Pasting" class="headerlink" title="Bagging and Pasting"></a>Bagging and Pasting</h1><ol>
<li>è¦ä½¿å­¦ä¹ å™¨å…·å¤‡<strong>å¤šæ ·æ€§</strong>æœ‰ä¸¤ç§æ–¹æ³•ï¼š<ol>
<li>ä½¿ç”¨ä¸åŒæ¨¡å‹</li>
<li>åœ¨ä¸åŒéšæœºå­é›†ä¸Šè®­ç»ƒï¼ˆåŒä¸€ç§æ¨¡å‹ï¼‰</li>
<li>åŸºäº 2 çš„æ€è·¯å°±äº§ç”Ÿäº†ä¸¤ç§éšæœºå­é›†çš„<strong>æŠ½æ ·</strong>æ–¹æ³•ï¼š<ol>
<li><strong>æœ‰æ”¾å›</strong> â€”â€” <strong>Bagging</strong> </li>
<li><strong>æ— æ”¾å›</strong> â€”â€” <strong>Pasting</strong><br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-4.png" style="width: 360px;" align="center"></li>
</ol>
</li>
</ol>
</li>
</ol>
<ol>
<li>Bagging å’Œ Pasting æ¨¡å‹çš„<strong>æ¨æ–­</strong>ï¼š<ol>
<li>åˆ†ç±»é—®é¢˜ï¼šæŠ•ç¥¨æ³•</li>
<li>å›å½’é—®é¢˜ï¼šåŠ æƒå¹³å‡</li>
<li><span class="mark">Remark</span>ï¼šåœ¨ BaggingClassifier çš„æ¨æ–­ä¸­ï¼Œæœ‰ä¸¤ä¸ªæ–¹æ³•ï¼š<code>predict</code> å’Œ <code>predict_proba</code>ï¼š<ol>
<li>å‰è€…é»˜è®¤ä½¿ç”¨ Soft Voting é¢„æµ‹ç±»åˆ«ï¼Œä½†å½“åŸºç¡€åˆ†ç±»å™¨ä¸æ”¯æŒç±»æ¦‚ç‡å‹è¾“å‡ºæ—¶åˆ™ä½¿ç”¨ Hard Voting</li>
<li>åè€…é»˜è®¤ä½¿ç”¨ Soft Voting é¢„æµ‹ç±»æ¦‚ç‡ï¼Œä½†å½“åŸºç¡€åˆ†ç±»å™¨ä¸æ”¯æŒç±»æ¦‚ç‡å‹è¾“å‡ºæ—¶åˆ™ä½¿ç”¨ Hard Voting å¹¶æœ€ç»ˆè¾“å‡º <strong>é¢„æµ‹ç±»åˆ«é¢‘ç‡</strong> ä½œä¸ºæ¦‚ç‡çš„è¿‘ä¼¼</li>
</ol>
</li>
</ol>
</li>
</ol>
<hr>
<ol>
<li>Remarkï¼š<ol>
<li>ä»¥ bagging or pasting æ–¹æ³•å¾—åˆ°çš„å­¦ä¹ å™¨  <strong>VS</strong>  åœ¨åŸå§‹è®­ç»ƒé›†ä¸Šçš„å•ä¸ªå­¦ä¹ å™¨ï¼š<ol>
<li>ä¸¤è€… Bias ç›¸è¿‘ </li>
<li>å‰è€… Variance æ›´å°  </li>
<li>å‰è€…å¯<strong>å¹¶è¡Œåœ°</strong>è®­ç»ƒå’Œæ¨æ–­ï¼Œå› æ­¤è§„æ¨¡æ˜“äºæ‹“å±•</li>
</ol>
</li>
<li>Bagging  <strong>VS</strong>  Pasting:<ol>
<li>ä» bias è§’åº¦ï¼šbagging ç•¥å¤§äº pasting</li>
<li>ä» variance è§’åº¦ï¼šbagging ç•¥å°äº pasting</li>
<li>é€šå¸¸ bagging æ¨¡å‹æ³›åŒ–æ€§èƒ½æ›´å¥½</li>
<li>ç®—åŠ›å……è¶³æ—¶ï¼Œå¯é€šè¿‡äº¤å‰éªŒè¯æ¥ç¡®å®š bagging å’Œ pasting å“ªä¸ªæ›´é€‚åˆå½“å‰é—®é¢˜</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="Code-Example-3"><a href="#Code-Example-3" class="headerlink" title="Code Example 3"></a>Code Example 3</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.Bagging æ¨¡å‹çš„æ„å»ºã€è®­ç»ƒå’Œé¢„æµ‹</span></span><br><span class="line">bag_clf = BaggingClassifier(</span><br><span class="line">    base_estimator=DecisionTreeClassifier(), </span><br><span class="line">    n_estimators=<span class="number">500</span>,</span><br><span class="line">    max_samples=<span class="number">100</span>, </span><br><span class="line">    bootstrap=<span class="literal">True</span>,    <span class="comment"># bootstrap=True ä¸º Baggingï¼Œå¦åˆ™ä¸º Pasting </span></span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">bag_clf.fit(X_train, y_train)</span><br><span class="line">y_pred = bag_clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Bagging Classifier: <span class="subst">&#123;accuracy_score(y_test, y_pred)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.å•ä¸ªå†³ç­–æ ‘æ¨¡å‹çš„æ„å»ºã€è®­ç»ƒå’Œé¢„æµ‹</span></span><br><span class="line">tree_clf = DecisionTreeClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">tree_clf.fit(X_train, y_train)</span><br><span class="line">y_pred_tree = tree_clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tree Classifier: <span class="subst">&#123;accuracy_score(y_test, y_pred_tree)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Bagging Classifier: 0.904
Tree Classifier: 0.856
</code></pre><p>Remarkï¼š</p>
<pre><code>  1. ä»ä¸Šä¾‹å¾—åˆ°çš„**å¯ç¤º**â€”â€”é›†æˆæ¨¡å‹ä¸å•ä¸ªæ¨¡å‹ç›¸æ¯”ï¼š
     1. åå·®ç›¸è¿‘ï¼ˆè®­ç»ƒé›†ä¸­çš„ errors æ•°å·®ä¸å¤šï¼Œ&lt;span class=&quot;burk&quot;&gt;ä½†è¯´å®è¯æˆ‘è§‰å¾—æŒºå¤šçš„--!&lt;/span&gt;ï¼‰
     2. æ–¹å·®æ›´å°ï¼ˆå†³ç­–è¾¹ç•Œæ›´å¹³æ»‘ï¼Œæ³›åŒ–æ€§èƒ½å¯èƒ½æ›´å¥½ï¼‰
        &lt;img src=&quot;./Fig7-5.png&quot;  style=&quot;width: 600px;&quot;  align=&quot;center&quot;/&gt;
</code></pre><h2 id="Out-of-Bag-Evaluation"><a href="#Out-of-Bag-Evaluation" class="headerlink" title="Out-of-Bag Evaluation"></a>Out-of-Bag Evaluation</h2><ol>
<li>ç”±äº bagging çš„æŠ½æ ·æœºåˆ¶ï¼Œæ— éœ€é¢å¤–åˆ’åˆ†éªŒè¯é›†ï¼Œå³å¯åœ¨ <strong>åŒ…å¤–å®ä¾‹</strong> ä¸Šè¯„ä¼°æ¨¡å‹</li>
</ol>
<h3 id="Code-Example-4"><a href="#Code-Example-4" class="headerlink" title="Code Example 4"></a>Code Example 4</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.å¸¦åŒ…å¤–è¯„ä¼°çš„ Bagging æ¨¡å‹</span></span><br><span class="line">bag_clf = BaggingClassifier(</span><br><span class="line">    base_estimator=DecisionTreeClassifier(), </span><br><span class="line">    n_estimators=<span class="number">500</span>,</span><br><span class="line">    bootstrap=<span class="literal">True</span>, </span><br><span class="line">    oob_score=<span class="literal">True</span>,    <span class="comment"># å¼€å¯åŒ…å¤–è¯„ä¼°ï¼ˆæ³¨æ„ï¼šè¯¥å‚æ•°åœ¨ bootstrap=Falseï¼ˆå³ pastingï¼‰æ—¶ä¸å¯è®¾ä¸º Falseï¼‰</span></span><br><span class="line">    random_state=<span class="number">40</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">bag_clf.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;åŒ…å¤–è¯„ä¼°å‡†ç¡®ç‡: <span class="subst">&#123;<span class="built_in">round</span>(bag_clf.oob_score_, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;æµ‹è¯•é›†å‡†ç¡®ç‡: <span class="subst">&#123;accuracy_score(y_test, bag_clf.predict(X_test))&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># bag_clf.oob_decision_function_</span></span><br></pre></td></tr></table></figure>
<pre><code>åŒ…å¤–è¯„ä¼°å‡†ç¡®ç‡: 0.8987
æµ‹è¯•é›†å‡†ç¡®ç‡: 0.912
</code></pre><p>Remarkï¼š</p>
<pre><code>  1. `oob_decision_function_`å¹¶éä¸€ä¸ªå‡½æ•°ï¼Œå®ƒåªæ˜¯å­˜å‚¨äº†å­¦ä¹ å™¨åœ¨åŒ…å¤–å®ä¾‹ä¸Šçš„é¢„æµ‹ç»“æœ
  2. å½“åŸºå­¦ä¹ å™¨å¯é¢„æµ‹ç±»æ¦‚ç‡æ—¶ï¼Œ`oob_decision_function_`çš„è¿”å›ç»“æœä¹Ÿå°†æ˜¯æ¯ä¸ª**è®­ç»ƒ**å®ä¾‹çš„ç±»æ¦‚ç‡
</code></pre><h2 id="Random-Patches-and-Random-Subspaces"><a href="#Random-Patches-and-Random-Subspaces" class="headerlink" title="Random Patches and Random Subspaces"></a>Random Patches and Random Subspaces</h2><ol>
<li>BaggingClassifier è¿˜æ”¯æŒ<strong>å¯¹ç‰¹å¾çš„æŠ½æ ·</strong>ï¼Œæ­¤æŠ½æ ·ç”±ä¸¤ä¸ªå‚æ•°æ§åˆ¶ï¼šmax_features &amp; bootstrap_features </li>
<li>è¿™ç§æŠ½æ ·å¯¹ç‰¹å¾æ•°åºå¤§çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚å›¾åƒï¼‰éå¸¸æœ‰ç”¨ï¼Œå®ƒè¿›ä¸€æ­¥å¢åŠ äº†åŸºæ¨¡å‹çš„å¤šæ ·æ€§ï¼Œä»£ä»·åˆ™æ˜¯ä¸€ç‚¹å„¿ bias</li>
<li>ä¸åŒæŠ½æ ·ç­–ç•¥å½¢æˆä¸åŒçš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼š<ol>
<li><strong>éšæœºè¡¥ä¸</strong>ï¼šåŒæ—¶å¯¹æ ·æœ¬å’Œç‰¹å¾æŠ½æ ·</li>
<li><strong>éšæœºå­ç©ºé—´</strong>ï¼šä»…å¯¹ç‰¹å¾æŠ½æ ·</li>
</ol>
</li>
</ol>
<h2 id="Random-Forests"><a href="#Random-Forests" class="headerlink" title="Random Forests"></a>Random Forests</h2><ol>
<li>ä¸å¿…é€šè¿‡ BaggingClassifier åˆ›å»ºéšæœºæ£®æ—åˆ†ç±»å™¨ï¼Œç›´æ¥ä½¿ç”¨ RandomForestClassifier æ›´æ–¹ä¾¿</li>
<li>RandomForestClassifier å¯¹å†³ç­–æ ‘ä¸“é—¨ä¼˜åŒ–è¿‡</li>
<li>RandomForest é€šå¸¸é‡‡ç”¨ bagging ç­–ç•¥ï¼Œä¸”å®ƒé»˜è®¤ä½¿ç”¨å®Œæ•´è®­ç»ƒé›†</li>
<li>éšæœºæ£®æ—å¼•è¿›äº†<strong>é¢å¤–éšæœºæ€§</strong>ï¼šåœ¨åˆ†è£‚èŠ‚ç‚¹æ—¶ï¼Œé€‰å–ä¸€ä¸ªéšæœºç‰¹å¾å­é›†ï¼Œå¹¶ä»ä¸­å¯»æ‰¾æœ€ä½³ç‰¹å¾ä»¥åˆ’åˆ†èŠ‚ç‚¹</li>
<li>ä¸Šè¿°åšæ³•è¿˜æ˜¯ç”¨ç¨é«˜çš„ Bias æ¢å–æ›´ä½çš„ Varianceï¼Œé€šå¸¸æ¨¡å‹æ•ˆæœä¹Ÿæ›´å¥½</li>
</ol>
<h3 id="Code-Example-5-amp-6"><a href="#Code-Example-5-amp-6" class="headerlink" title="Code Example 5 &amp; 6"></a>Code Example 5 &amp; 6</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.æ„å»ºå¹¶è®­ç»ƒä¸€ä¸ªéšæœºæ£®æ—åˆ†ç±»å™¨</span></span><br><span class="line">rnd_clf = RandomForestClassifier(</span><br><span class="line">    n_estimators=<span class="number">500</span>, </span><br><span class="line">    max_leaf_nodes=<span class="number">16</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">rnd_clf.fit(X_train, y_train)</span><br><span class="line">y_pred_rf = rnd_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.æ„å»ºå¹¶è®­ç»ƒäº†ä¸€ä¸ªç­‰ä»·çš„ Bagging æ¨¡å‹</span></span><br><span class="line">bag_clf = BaggingClassifier(</span><br><span class="line">    DecisionTreeClassifier(</span><br><span class="line">        max_features=<span class="string">&quot;sqrt&quot;</span>, </span><br><span class="line">        max_leaf_nodes=<span class="number">16</span></span><br><span class="line">    ),</span><br><span class="line">    n_estimators=<span class="number">500</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">bag_clf.fit(X_train, y_train)</span><br><span class="line">y_pred = bag_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.å¯¹æ¯”ä¸Šé¢ä¸¤ä¸ªæ¨¡å‹çš„æ•ˆæœï¼Œå‘ç°é¢„æµ‹ç»“æœå‡ ä¹å®Œå…¨ä¸€è‡´</span></span><br><span class="line">np.<span class="built_in">sum</span>(y_pred == y_pred_rf) / <span class="built_in">len</span>(y_pred)</span><br></pre></td></tr></table></figure>
<pre><code>1.0
</code></pre><h2 id="Extra-Trees"><a href="#Extra-Trees" class="headerlink" title="Extra-Trees"></a>Extra-Trees</h2><ol>
<li><strong>Extra-Trees</strong>ï¼šä¸æœç´¢æœ€ä½³åˆ†è£‚é˜ˆå€¼ï¼Œè€Œé‡‡å–<strong>éšæœºåˆ†è£‚</strong>çš„æ–¹å¼ï¼Œä¸ºå†³ç­–æ ‘æ·»åŠ æ›´å¤šéšæœºæ€§</li>
<li>ä¸Šè¿°ç­–ç•¥ä»æ˜¯ä»¥ç¨é«˜çš„ bias æ¢å–æ›´ä½çš„ variance</li>
<li>ç”±äºèŠ‚ç‚¹åˆ†è£‚æ–¹å¼æ˜¯éšæœºçš„ï¼Œå› æ­¤ Extra-Trees çš„<strong>è®­ç»ƒé€Ÿåº¦æ›´å¿«</strong>ï¼ˆä¸ Random Forest ç›¸æ¯”ï¼‰</li>
<li>é€šå¸¸æ— æ³•é¢„è§éšæœºæ£®æ—å’Œæç«¯éšæœºæ ‘è°çš„æ€§èƒ½æ›´å¥½ï¼Œåªæœ‰å®é™…å°è¯•ä¸¤ç§æ¨¡å‹å¹¶é€šè¿‡äº¤å‰éªŒè¯æœç´¢æœ€ä½³è¶…å‚æ•°ç»„åˆï¼Œæ‰èƒ½æœ€ç»ˆç¡®å®šæ›´é€‚åˆå½“å‰ä»»åŠ¡çš„æ¨¡å‹</li>
</ol>
<h2 id="Feature-Importance"><a href="#Feature-Importance" class="headerlink" title="Feature Importance"></a>Feature Importance</h2><ol>
<li>éšæœºæ£®æ—çš„ä¸€ä¸ªä¼˜ç‚¹ï¼šå¯æ–¹ä¾¿åœ°åº¦é‡<strong>ç‰¹å¾çš„ç›¸å¯¹é‡è¦æ€§</strong></li>
<li>æ‰€æœ‰ç‰¹å¾çš„é‡è¦æ€§ä¹‹å’Œç­‰äº 1</li>
<li>æ³¨ï¼š<strong>æ’åˆ—é‡è¦æ€§</strong> å’Œ <strong>SHAP</strong> èƒ½æ›´å¥½åœ°å¯¹ç‰¹å¾é‡è¦æ€§åšå‡ºåˆ†æ</li>
</ol>
<h3 id="Code-Example-7"><a href="#Code-Example-7" class="headerlink" title="Code Example 7"></a>Code Example 7</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.åŸºäºé¸¢å°¾èŠ±æ•°æ®é›†æ„å»ºéšæœºæ£®æ—åˆ†ç±»å™¨</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">rnd_clf = RandomForestClassifier(</span><br><span class="line">    n_estimators=<span class="number">500</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">rnd_clf.fit(iris[<span class="string">&quot;data&quot;</span>], iris[<span class="string">&quot;target&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç‰¹å¾é‡è¦æ€§</span></span><br><span class="line"><span class="keyword">for</span> name, score <span class="keyword">in</span> <span class="built_in">zip</span>(iris[<span class="string">&quot;feature_names&quot;</span>], rnd_clf.feature_importances_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span> çš„ç‰¹å¾é‡è¦æ€§åˆ†æ•°ï¼š<span class="subst">&#123;<span class="built_in">round</span>(score, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>sepal length (cm) çš„ç‰¹å¾é‡è¦æ€§åˆ†æ•°ï¼š0.1125
sepal width (cm) çš„ç‰¹å¾é‡è¦æ€§åˆ†æ•°ï¼š0.0231
petal length (cm) çš„ç‰¹å¾é‡è¦æ€§åˆ†æ•°ï¼š0.441
petal width (cm) çš„ç‰¹å¾é‡è¦æ€§åˆ†æ•°ï¼š0.4234
</code></pre><h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><ol>
<li>Boosting çš„æ ¸å¿ƒæƒ³æ³•ï¼š<strong>åºåˆ—å¼</strong>åœ°è®­ç»ƒä¸€ç³»åˆ—é¢„æµ‹å™¨ï¼Œæ¯ä¸€ä¸ªéƒ½å°è¯•ä¿®æ­£å‰ä¸€ä¸ªé¢„æµ‹å™¨æ‰€çŠ¯é”™è¯¯</li>
<li>è¯´ç™½äº†å°±æ˜¯ä¸æ–­é€¼è¿‘ç›®æ ‡å€¼ or ä¸æ–­ä¿®æ­£è¯¯å·®çš„æ–¹æ³•</li>
<li>ç›®å‰æœ€ä¸»æµçš„ä¸¤ä¸ª Boosting æ–¹æ³•ï¼š <ol>
<li>AdaBoost</li>
<li>Gradient Boosting</li>
</ol>
</li>
</ol>
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><ol>
<li><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼ˆä»¥åˆ†ç±»å™¨è§’åº¦ä¸¾ä¾‹ï¼‰ï¼š<ol>
<li>å¯¹åˆ†ç±»å™¨  <strong>è¯¯åˆ†ç±»çš„å®ä¾‹</strong>  å¢åŠ æƒé‡</li>
<li>åœ¨æ›´æ–°æƒé‡åçš„è®­ç»ƒé›†ä¸Šè®­ç»ƒä¸‹ä¸€ä¸ªåˆ†ç±»å™¨</li>
<li>ä¸æ–­è¿­ä»£ï¼ˆè®­ç»ƒã€é¢„æµ‹ã€æ›´æ–°æƒé‡ã€è®­ç»ƒã€é¢„æµ‹ã€æ›´æ–°æƒé‡ã€â€¦â€¦ï¼‰ï¼Œä»è€Œæ–°åˆ†ç±»å™¨å°†æ›´å…³æ³¨é‚£äº›  <strong>è¾ƒéš¾åˆ†ç±»çš„å®ä¾‹</strong><br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-7.png" style="width: 400px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li>ä¸åŒ<strong>å­¦ä¹ ç‡</strong>å¯¹ AdaBoost å†³ç­–è¾¹ç•Œçš„å½±å“ï¼š<ol>
<li>è¾ƒå¤§çš„å­¦ä¹ ç‡ï¼šå†³ç­–è¾¹ç•Œå˜åŠ¨å‰§çƒˆ</li>
<li>è¾ƒå°çš„å­¦ä¹ ç‡ï¼šå†³ç­–è¾¹ç•Œå˜åŠ¨ç¼“å’Œ<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-8.png" style="width: 500px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li><strong>æ¨æ–­æ–¹å¼</strong>ï¼š<ol>
<li>åˆ†ç±»é—®é¢˜ï¼š<strong>ç±»ä¼¼æŠ•ç¥¨æ³•</strong></li>
<li>å›å½’é—®é¢˜ï¼šåŠ æƒå¹³å‡</li>
<li>AdaBoost ä½¿ç”¨æ‰€æœ‰å­¦ä¹ å™¨å¯¹æ–°å®ä¾‹è¿›è¡Œé¢„æµ‹ï¼Œç„¶åæŒ‰<strong>æ¯ä¸ªå­¦ä¹ å™¨çš„æƒé‡</strong>å¯¹é¢„æµ‹ç»“æœè¿›è¡ŒåŠ æƒï¼Œå³å¾—æœ€ç»ˆé¢„æµ‹ç»“æœ</li>
<li><span class="mark">Remark</span>ï¼šåˆ†ç±»é—®é¢˜çš„æ¨æ–­æ–¹å¼ä¸å†åŒºåˆ† Hard / Soft Votingï¼Œè€Œæ˜¯åœ¨ <code>AdaBoostClassifier</code> ä¸­ç»™å‡ºäº†ä¸¤ç§æ–¹æ³•ï¼š<code>predict</code> å’Œ <code>predict_proba</code></li>
</ol>
</li>
</ol>
<ol>
<li>Remarkï¼š<ol>
<li>ç”±äº AdaBoost çš„<strong>åºåˆ—å¼è®­ç»ƒæ–¹å¼</strong>ï¼Œå®ƒéš¾ä»¥è¢«<strong>å¹¶è¡ŒåŒ–</strong>å¤„ç†</li>
<li>AdaBoost <strong>è®­ç»ƒåœæ­¢æ¡ä»¶</strong>ï¼š<ol>
<li>å­¦ä¹ å™¨ä¸ªæ•°è¾¾åˆ°ä¸Šé™</li>
<li>å¾—åˆ°äº†å®Œç¾çš„å­¦ä¹ å™¨</li>
</ol>
</li>
<li>é˜²æ­¢è¿‡æ‹Ÿåˆã®æªæ–½ï¼š<ol>
<li>å‡å°‘åŸºå­¦ä¹ å™¨æ•°é‡</li>
<li>ä½¿ç”¨å¸¦å¼ºæ­£åˆ™åŒ–çš„åŸºå­¦ä¹ å™¨</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="Code-Example-8"><a href="#Code-Example-8" class="headerlink" title="Code Example 8"></a>Code Example 8</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.æ„å»ºå¹¶è®­ç»ƒ AdaBoost åˆ†ç±»å™¨</span></span><br><span class="line">ada_clf = AdaBoostClassifier(</span><br><span class="line">    base_estimator=DecisionTreeClassifier(max_depth=<span class="number">1</span>), </span><br><span class="line">    n_estimators=<span class="number">200</span>,</span><br><span class="line">    algorithm=<span class="string">&quot;SAMME.R&quot;</span>, </span><br><span class="line">    learning_rate=<span class="number">0.5</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">ada_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p><style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable<strong>label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable</strong>label-arrow:before {content: â€œâ–¸â€;float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable<strong>label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable</strong>label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable<strong>content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable</strong>content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable<strong>control:checked~div.sk-toggleable</strong>content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label-arrow:before {content: â€œâ–¾â€;}#sk-container-id-1 div.sk-estimator input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hiddenâ€”visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: â€œâ€;width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: â€œâ€;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: â€œâ€;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/<em> jupyterâ€™s <code>normalize.less</code> sets <code>[hidden] &#123; display: none; &#125;</code> but bootstrap.min.css set <code>[hidden] &#123; display: none !important; &#125;</code> so we also need the <code>!important</code> here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: <a href="https://github.com/scikit-learn/scikit-learn/issues/21755">https://github.com/scikit-learn/scikit-learn/issues/21755</a> </em>/display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),</pre></div></div></p>
<pre><code>               learning_rate=0.5, n_estimators=200, random_state=42)&lt;/pre&gt;&lt;b&gt;In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. &lt;br /&gt;On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;sk-container&quot; hidden&gt;&lt;div class=&quot;sk-item sk-dashed-wrapped&quot;&gt;&lt;div class=&quot;sk-label-container&quot;&gt;&lt;div class=&quot;sk-label sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-1&quot; type=&quot;checkbox&quot; &gt;&lt;label for=&quot;sk-estimator-id-1&quot; class=&quot;sk-toggleable__label sk-toggleable__label-arrow&quot;&gt;AdaBoostClassifier&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content&quot;&gt;&lt;pre&gt;AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),
               learning_rate=0.5, n_estimators=200, random_state=42)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;sk-parallel&quot;&gt;&lt;div class=&quot;sk-parallel-item&quot;&gt;&lt;div class=&quot;sk-item&quot;&gt;&lt;div class=&quot;sk-label-container&quot;&gt;&lt;div class=&quot;sk-label sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-2&quot; type=&quot;checkbox&quot; &gt;&lt;label for=&quot;sk-estimator-id-2&quot; class=&quot;sk-toggleable__label sk-toggleable__label-arrow&quot;&gt;base_estimator: DecisionTreeClassifier&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content&quot;&gt;&lt;pre&gt;DecisionTreeClassifier(max_depth=1)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;sk-serial&quot;&gt;&lt;div class=&quot;sk-item&quot;&gt;&lt;div class=&quot;sk-estimator sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-3&quot; type=&quot;checkbox&quot; &gt;&lt;label for=&quot;sk-estimator-id-3&quot; class=&quot;sk-toggleable__label sk-toggleable__label-arrow&quot;&gt;DecisionTreeClassifier&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content&quot;&gt;&lt;pre&gt;DecisionTreeClassifier(max_depth=1)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</code></pre><h2 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h2><ol>
<li><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼š<ol>
<li>Gradient Boosting ä¹Ÿæ˜¯<strong>åºåˆ—å¼</strong>åœ°è®­ç»ƒä¸€ç³»åˆ—å­¦ä¹ å™¨</li>
<li>ä¸ AdaBoost çš„åŒºåˆ«ï¼šGradient Boosting ä¸­çš„æ¯ä¸ªå­¦ä¹ å™¨éƒ½æ˜¯å¯¹å‰ä¸€ä¸ªå­¦ä¹ å™¨çš„ <strong>æ®‹å·®</strong> è¿›è¡Œæ‹Ÿåˆ</li>
</ol>
</li>
</ol>
<ol>
<li><strong>æ¨æ–­æ–¹æ³•</strong>ï¼šåœ¨æ–°å®ä¾‹ä¸Šå°†æ¯ä¸ªå­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœ<strong>æ±‚å’Œ</strong><br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-9.png" style="width: 600px;" align="center"></li>
</ol>
<ol>
<li>Gradient Boosting ä¸­çš„<strong>å­¦ä¹ ç‡</strong>ï¼š<ol>
<li>å–å€¼å¾ˆå°æ—¶ï¼Œéœ€è¦æ›´å¤šæ ‘æ¥æ‹Ÿåˆè®­ç»ƒé›†ï¼Œä½†æ³›åŒ–æ€§èƒ½é€šå¸¸æ›´å¥½</li>
<li>ä¸‹å›¾æ¯”è¾ƒäº†æ ‘å¤ªå¤š / å¤ªå°‘çš„æƒ…å†µ</li>
<li><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-10.png" style="width: 600px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li>é’ˆå¯¹è®¾å®šå¥½å­¦ä¹ ç‡çš„ Gradient Boosting æ¨¡å‹<strong>é€‰å–æœ€ä½³æ ‘æ•°é‡</strong>çš„æ–¹æ³•ï¼š<ol>
<li>ä½¿ç”¨è¿‡é‡çš„æ ‘å»ºæ¨¡å¹¶è®­ç»ƒ</li>
<li>åœ¨éªŒè¯è¿‡ç¨‹ä¸­é€šè¿‡ <strong>æå‰åœæ­¢</strong> æ‰¾åˆ°æœ€ä½³æ ‘æ•°é‡</li>
<li>Code Example 10</li>
</ol>
</li>
</ol>
<ol>
<li><p>å®ç°æå‰åœæ­¢çš„å¦ä¸€ç§æ€è·¯ï¼š</p>
<ol>
<li><strong>å¢é‡å¼è®­ç»ƒ</strong></li>
<li>Code Example 11</li>
</ol>
</li>
<li><p><strong>éšæœºæ¢¯åº¦æå‡</strong>ï¼š</p>
<ol>
<li>åœ¨ Gradient Boosting ä¸­ï¼Œä½¿æ¯ä¸ªå­¦ä¹ å™¨åŸºäº <strong>å®Œæ•´è®­ç»ƒé›†çš„ä¸€ä¸ªéšæœºå­é›†</strong> æ‹Ÿåˆæ®‹å·®</li>
<li>é€šè¿‡è®¾ç½® subsample è¶…å‚æ•°å®ç°</li>
<li>å¥½å¤„ï¼š<ol>
<li>ç”¨é«˜ä¸€ç‚¹å„¿çš„åå·®æ¢å–æ›´ä½çš„æ–¹å·®ï¼Œå¢åŠ æ¨¡å‹æ³›åŒ–æ€§èƒ½</li>
<li>åŠ å¿«è®­ç»ƒé€Ÿåº¦</li>
</ol>
</li>
</ol>
</li>
</ol>
<ol>
<li>Remarkï¼š<ol>
<li>Gradient Boosting æ¨¡å‹ä¸­å¯è‡ªå®šä¹‰æŸå¤±å‡½æ•°</li>
<li><strong>XGBoost</strong>ï¼šGradient Boosting çš„ä¸€ä¸ªä¼˜åŒ–å®ç° â€”â€” Code Example 12</li>
</ol>
</li>
</ol>
<h3 id="Code-Example-9"><a href="#Code-Example-9" class="headerlink" title="Code Example 9"></a>Code Example 9</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 0.å‡†å¤‡æ•°æ®é›†</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>) - <span class="number">0.5</span></span><br><span class="line">y = <span class="number">3</span>*X[:, <span class="number">0</span>]**<span class="number">2</span> + <span class="number">0.05</span> * np.random.randn(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºå¹¶è®­ç»ƒä¸€ä¸ª GradientBoosting å›å½’å™¨</span></span><br><span class="line">gbrt = GradientBoostingRegressor(</span><br><span class="line">    max_depth=<span class="number">2</span>, </span><br><span class="line">    n_estimators=<span class="number">3</span>, </span><br><span class="line">    learning_rate=<span class="number">1.0</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">gbrt.fit(X, y)</span><br></pre></td></tr></table></figure>
<p><style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable<strong>label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable</strong>label-arrow:before {content: â€œâ–¸â€;float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable<strong>label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable</strong>label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable<strong>content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable</strong>content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable<strong>control:checked~div.sk-toggleable</strong>content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label-arrow:before {content: â€œâ–¾â€;}#sk-container-id-2 div.sk-estimator input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hiddenâ€”visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: â€œâ€;width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: â€œâ€;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: â€œâ€;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/<em> jupyterâ€™s <code>normalize.less</code> sets <code>[hidden] &#123; display: none; &#125;</code> but bootstrap.min.css set <code>[hidden] &#123; display: none !important; &#125;</code> so we also need the <code>!important</code> here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: <a href="https://github.com/scikit-learn/scikit-learn/issues/21755">https://github.com/scikit-learn/scikit-learn/issues/21755</a> </em>/display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,</pre></div></div></p>
<pre><code>                      random_state=42)&lt;/pre&gt;&lt;b&gt;In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. &lt;br /&gt;On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;sk-container&quot; hidden&gt;&lt;div class=&quot;sk-item&quot;&gt;&lt;div class=&quot;sk-estimator sk-toggleable&quot;&gt;&lt;input class=&quot;sk-toggleable__control sk-hidden--visually&quot; id=&quot;sk-estimator-id-4&quot; type=&quot;checkbox&quot; checked&gt;&lt;label for=&quot;sk-estimator-id-4&quot; class=&quot;sk-toggleable__label sk-toggleable__label-arrow&quot;&gt;GradientBoostingRegressor&lt;/label&gt;&lt;div class=&quot;sk-toggleable__content&quot;&gt;&lt;pre&gt;GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,
                      random_state=42)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</code></pre><h3 id="Code-Example-10"><a href="#Code-Example-10" class="headerlink" title="Code Example 10"></a>Code Example 10</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.è®¾ç½®è¿‡é‡çš„æ ‘æ¥è®­ç»ƒ</span></span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=<span class="number">49</span>)</span><br><span class="line"></span><br><span class="line">gbrt = GradientBoostingRegressor(</span><br><span class="line">    max_depth=<span class="number">2</span>, </span><br><span class="line">    n_estimators=<span class="number">120</span>, </span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">gbrt.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.åŸºäºéªŒè¯é›†å¯»æ‰¾æœ€ä½³æ ‘æ•°é‡</span></span><br><span class="line">errors = [mean_squared_error(y_val, y_pred) <span class="keyword">for</span> y_pred <span class="keyword">in</span> gbrt.staged_predict(X_val)]    <span class="comment"># staged_predict</span></span><br><span class="line">bst_n_estimators = np.argmin(errors) + <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;æœ€ä½³æ ‘æ•°é‡ï¼š<span class="subst">&#123;bst_n_estimators&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.ä»¥æœ€ä½³æ ‘æ•°é‡é‡æ–°å»ºæ¨¡ã€è®­ç»ƒ</span></span><br><span class="line">gbrt_best = GradientBoostingRegressor(</span><br><span class="line">    max_depth=<span class="number">2</span>, </span><br><span class="line">    n_estimators=bst_n_estimators,    <span class="comment"># æœ€ä½³æ ‘æ•°é‡</span></span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">gbrt_best.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<pre><code>æœ€ä½³æ ‘æ•°é‡ï¼š56
</code></pre><p><style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable<strong>label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable</strong>label-arrow:before {content: â€œâ–¸â€;float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable<strong>label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable</strong>label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable<strong>content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable</strong>content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable<strong>control:checked~div.sk-toggleable</strong>content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label-arrow:before {content: â€œâ–¾â€;}#sk-container-id-3 div.sk-estimator input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable<strong>control:checked~label.sk-toggleable</strong>label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hiddenâ€”visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: â€œâ€;width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: â€œâ€;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: â€œâ€;position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/<em> jupyterâ€™s <code>normalize.less</code> sets <code>[hidden] &#123; display: none; &#125;</code> but bootstrap.min.css set <code>[hidden] &#123; display: none !important; &#125;</code> so we also need the <code>!important</code> here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: <a href="https://github.com/scikit-learn/scikit-learn/issues/21755">https://github.com/scikit-learn/scikit-learn/issues/21755</a> </em>/display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" checked><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">GradientBoostingRegressor</label><div class="sk-toggleable__content"><pre>GradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=42)</pre></div>&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</div></div></div></div></p>
<p>Remarkï¼š</p>
<pre><code>  1. éªŒè¯è¯¯å·® vs æ ‘æ•°é‡
  2. æœ€ä½³æ ‘æ•°é‡ä¸‹çš„ GradientBoosting æ¨¡å‹
     &lt;img src=&quot;./Fig7-11.png&quot;  style=&quot;width: 600px;&quot;  align=&quot;center&quot;/&gt;
</code></pre><h3 id="Code-Example-11"><a href="#Code-Example-11" class="headerlink" title="Code Example 11"></a>Code Example 11</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.æ„å»º GradientBoosting æ¨¡å‹</span></span><br><span class="line">gbrt = GradientBoostingRegressor(</span><br><span class="line">    max_depth=<span class="number">2</span>, </span><br><span class="line">    warm_start=<span class="literal">True</span>,    <span class="comment"># warm_start=True  </span></span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.åˆå§‹åŒ–éªŒè¯è¯¯å·® &amp; æ¯æ¬¡è¿­ä»£éƒ½è®­ç»ƒæ¨¡å‹å¹¶è®¡ç®—éªŒè¯è¯¯å·®</span></span><br><span class="line">min_val_error = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">error_going_up = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> n_estimators <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">120</span>):</span><br><span class="line">    gbrt.n_estimators = n_estimators    <span class="comment"># æ¯æ¬¡è¿­ä»£æ›´æ–°æ ‘æ•°é‡</span></span><br><span class="line">    gbrt.fit(X_train, y_train)</span><br><span class="line">    y_pred = gbrt.predict(X_val)</span><br><span class="line">    val_error = mean_squared_error(y_val, y_pred)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3.æ£€éªŒéªŒè¯è¯¯å·®æ˜¯å¦ä¸‹é™ </span></span><br><span class="line">    <span class="keyword">if</span> val_error &lt; min_val_error:</span><br><span class="line">        min_val_error = val_error</span><br><span class="line">        error_going_up = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        error_going_up += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> error_going_up == <span class="number">5</span>:    <span class="comment"># early stopping with patience = 5</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;æœ€ä½³æ ‘æ•°é‡ï¼š<span class="subst">&#123;gbrt.n_estimators&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;æœ€ä½éªŒè¯è¯¯å·®ï¼š<span class="subst">&#123;<span class="built_in">round</span>(min_val_error, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>æœ€ä½³æ ‘æ•°é‡ï¼š61
æœ€ä½éªŒè¯è¯¯å·®ï¼š0.0027
</code></pre><h3 id="Code-Example-12"><a href="#Code-Example-12" class="headerlink" title="Code Example 12"></a>Code Example 12</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šæ‹Ÿåˆ XGBoost</span></span><br><span class="line">xgb_reg = xgboost.XGBRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">xgb_reg.fit(X_train, y_train)</span><br><span class="line">y_pred = xgb_reg.predict(X_val)</span><br><span class="line">val_error = mean_squared_error(y_val, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šæ‹Ÿåˆ XGBoost æ—¶çš„éªŒè¯è¯¯å·®ï¼š<span class="subst">&#123;<span class="built_in">round</span>(val_error, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.åŸºäºæå‰åœæ­¢è®­ç»ƒ XGBoost</span></span><br><span class="line">xgb_reg = xgboost.XGBRegressor(</span><br><span class="line">    early_stopping_rounds=<span class="number">2</span>,    <span class="comment"># patience = 2 </span></span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">xgb_reg.fit(</span><br><span class="line">    X_train, y_train,</span><br><span class="line">    eval_set=[(X_val, y_val)],    <span class="comment"># éªŒè¯é›†</span></span><br><span class="line">)</span><br><span class="line">y_pred = xgb_reg.predict(X_val)</span><br><span class="line">val_error = mean_squared_error(y_val, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;åŸºäºæå‰åœæ­¢æ‹Ÿåˆ XGBoost æ—¶çš„éªŒè¯è¯¯å·®ï¼š<span class="subst">&#123;<span class="built_in">round</span>(val_error, <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šæ‹Ÿåˆ XGBoost æ—¶çš„éªŒè¯è¯¯å·®ï¼š0.004
[0]    validation_0-rmse:0.22834
[1]    validation_0-rmse:0.16224
[2]    validation_0-rmse:0.11843
[3]    validation_0-rmse:0.08760
[4]    validation_0-rmse:0.06848
[5]    validation_0-rmse:0.05709
[6]    validation_0-rmse:0.05297
[7]    validation_0-rmse:0.05129
[8]    validation_0-rmse:0.05155
åŸºäºæå‰åœæ­¢æ‹Ÿåˆ XGBoost æ—¶çš„éªŒè¯è¯¯å·®ï¼š0.0026
</code></pre><h1 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h1><ol>
<li><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼š<ol>
<li>ä¸å†ä½¿ç”¨ç®€å•å‡½æ•°è¿›è¡Œèšåˆï¼ˆä¾‹å¦‚ hard votingï¼‰</li>
<li>è€Œæ˜¯è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥èšåˆåŸºå­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœ</li>
<li>è¿™æ˜¯ä¸€ç§<strong>å¤åˆå‡½æ•°</strong>ï¼Œåªä¸è¿‡å¤–å±‚å‡½æ•°æ˜¯ç”±æ¨¡å‹æ‹Ÿåˆå¾—åˆ°çš„</li>
</ol>
</li>
</ol>
<ol>
<li><strong>æ¨æ–­æ–¹æ³•</strong>ï¼š<ol>
<li>å…ˆç”±åŸºå­¦ä¹ å™¨å¯¹æ–°å®ä¾‹è¿›è¡Œé¢„æµ‹</li>
<li><strong>å…ƒå­¦ä¹ å™¨</strong>å†åŸºäºè¿™äº›é¢„æµ‹å€¼é¢„æµ‹å‡ºæœ€ç»ˆç»“æœ</li>
<li>å…ƒå­¦ä¹ å™¨ï¼šå³ Stacking çš„å¤–å±‚èšåˆå‡½æ•°<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-12.png" style="width: 500px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li><strong>è®­ç»ƒæ–¹æ³•</strong>ï¼š<ol>
<li>å°†åˆå§‹è®­ç»ƒé›†åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ† A å’Œ B</li>
<li>åœ¨å­é›† A ä¸Šè®­ç»ƒåŸºå­¦ä¹ å™¨</li>
<li>ä»¥è®­ç»ƒå¥½çš„åŸºå­¦ä¹ å™¨åœ¨å­é›† B ä¸Šé¢„æµ‹ï¼Œå¾—åˆ°å­é›† C</li>
<li>åœ¨å­é›† C ä¸Šè®­ç»ƒå…ƒå­¦ä¹ å™¨<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-13.png" style="width: 500px;" align="center"><br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-14.png" style="width: 500px;" align="center"></li>
</ol>
</li>
</ol>
<ol>
<li>æ›´è¿›ä¸€æ­¥ï¼Œå…ƒå­¦ä¹ å™¨å¯ä»¥å½¢æˆä¸€ä¸ªä¸­é—´å±‚ï¼š<br><img src="/2023/03/08/Ensemble-Learning-Methods/Fig7-15.png" style="width: 500px;" align="center"></li>
</ol>
<h3 id="Code-Example-13"><a href="#Code-Example-13" class="headerlink" title="Code Example 13"></a>Code Example 13</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 0.è¯»å–æ•°æ®é›†</span></span><br><span class="line">mnist = fetch_openml(<span class="string">&#x27;mnist_784&#x27;</span>, version=<span class="number">1</span>, as_frame=<span class="literal">False</span>)</span><br><span class="line">mnist.target = mnist.target.astype(np.uint8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.åˆ’åˆ†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†</span></span><br><span class="line">X_train_val, X_test, y_train_val, y_test = train_test_split(mnist.data, mnist.target, test_size=<span class="number">10000</span>, random_state=<span class="number">42</span>)</span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=<span class="number">10000</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.5ç®€åŒ–æ•°æ®é›†</span></span><br><span class="line">pca = PCA(n_components=<span class="number">0.90</span>)</span><br><span class="line">pca.fit_transform(X_train)</span><br><span class="line">pca.transform(X_val)</span><br><span class="line">pca.transform(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;é™ç»´åï¼Œæ•°æ®é›†ç»´æ•°ï¼š<span class="subst">&#123;pca.n_components_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.æ„å»ºä¸åŒåˆ†ç±»å™¨</span></span><br><span class="line">clf_1 = Pipeline([(<span class="string">&#x27;rdf&#x27;</span>, RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>))])</span><br><span class="line">clf_2 = Pipeline([(<span class="string">&#x27;extra_trees&#x27;</span>, ExtraTreesClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>))])</span><br><span class="line">clf_3 = Pipeline([(<span class="string">&#x27;scaler&#x27;</span>, StandardScaler()), (<span class="string">&#x27;mlp&#x27;</span>, MLPClassifier(random_state=<span class="number">42</span>))])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.è®­ç»ƒä¸åŒåˆ†ç±»å™¨</span></span><br><span class="line">estimators = [clf_1, clf_2, clf_3]</span><br><span class="line"><span class="keyword">for</span> estimator <span class="keyword">in</span> estimators:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training the&quot;</span>, estimator[-<span class="number">1</span>])</span><br><span class="line">    estimator.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 4.éªŒè¯é›†ä¸Šè¯„ä¼°å„åˆ†ç±»å™¨</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;æ¯ä¸ªåˆ†ç±»å™¨åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š<span class="subst">&#123;[estimator.score(X_val, y_val) <span class="keyword">for</span> estimator <span class="keyword">in</span> estimators]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.æ„å»º Voting åˆ†ç±»å™¨</span></span><br><span class="line">voting_clf = VotingClassifier([</span><br><span class="line">    (<span class="string">&quot;random_forest&quot;</span>, clf_1),</span><br><span class="line">    (<span class="string">&quot;extra_trees&quot;</span>, clf_2),</span><br><span class="line">    (<span class="string">&quot;mlp&quot;</span>, clf_3)], </span><br><span class="line">    voting=<span class="string">&#x27;hard&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">voting_clf.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Voting åˆ†ç±»å™¨åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š<span class="subst">&#123;voting_clf.score(X_val, y_val)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Voting åˆ†ç±»å™¨ä¸­æ¯ä¸ªå­åˆ†ç±»å™¨åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š<span class="subst">&#123;[estimator.score(X_val, y_val) <span class="keyword">for</span> estimator <span class="keyword">in</span> voting_clf.estimators_]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>é™ç»´åï¼Œæ•°æ®é›†ç»´æ•°ï¼š87
---------------------------------------------------------------
Training the RandomForestClassifier(random_state=42)
Training the ExtraTreesClassifier(random_state=42)
Training the MLPClassifier(random_state=42)
---------------------------------------------------------------
æ¯ä¸ªåˆ†ç±»å™¨åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š[0.9692, 0.9715, 0.9747]
---------------------------------------------------------------
Voting åˆ†ç±»å™¨åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š0.9751
Voting åˆ†ç±»å™¨ä¸­æ¯ä¸ªå­åˆ†ç±»å™¨åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š[0.9692, 0.9715, 0.9747]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 6.å°è¯•è½¯æŠ•ç¥¨çš„å‡†ç¡®ç‡</span></span><br><span class="line">voting_clf.voting = <span class="string">&quot;soft&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;è½¯æŠ•ç¥¨åˆ†ç±»å™¨çš„éªŒè¯å‡†ç¡®ç‡ï¼š<span class="subst">&#123;voting_clf.score(X_val, y_val)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.åœ¨æµ‹è¯•é›†ä¸Šå„åˆ†ç±»å™¨çš„å‡†ç¡®ç‡</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;è½¯æŠ•ç¥¨åˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š<span class="subst">&#123;voting_clf.score(X_test, y_test)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;å„å­åˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š<span class="subst">&#123;[estimator.score(X_test, y_test) <span class="keyword">for</span> estimator <span class="keyword">in</span> voting_clf.estimators_]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">63</span>)</span><br></pre></td></tr></table></figure>
<pre><code>è½¯æŠ•ç¥¨åˆ†ç±»å™¨çš„éªŒè¯å‡†ç¡®ç‡ï¼š0.9783
---------------------------------------------------------------
è½¯æŠ•ç¥¨åˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š0.9757
å„å­åˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š[0.9645, 0.9691, 0.9719]
---------------------------------------------------------------
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 8.ä½¿ç”¨ä¸Šè¿°å­åˆ†ç±»å™¨åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œç”±æ­¤å¾—åˆ°ä¸€ä¸ªæ–°è®­ç»ƒé›†</span></span><br><span class="line">X_val_predictions = np.empty(shape=(<span class="built_in">len</span>(X_val), <span class="built_in">len</span>(estimators)), dtype=np.float32)</span><br><span class="line"><span class="keyword">for</span> index, estimator <span class="keyword">in</span> <span class="built_in">enumerate</span>(estimators):</span><br><span class="line">    X_val_predictions[:, index] = estimator.predict(X_val)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.åœ¨ä¸Šè¿°æ–°è®­ç»ƒé›†ä¸Šï¼Œè®­ç»ƒå…ƒå­¦ä¹ å™¨</span></span><br><span class="line">blender_1 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;sclaer&#x27;</span>, StandardScaler()), </span><br><span class="line">    (<span class="string">&#x27;LR&#x27;</span>, LogisticRegression(max_iter=<span class="number">10000</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">blender_2 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;sclaer&#x27;</span>, StandardScaler()), </span><br><span class="line">    (<span class="string">&#x27;SVC&#x27;</span>, SVC())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">blender_3 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;RF&#x27;</span>, RandomForestClassifier(max_depth=<span class="number">5</span>, n_estimators=<span class="number">500</span>, random_state=<span class="number">42</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">blender_4 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;GradientBoosting&#x27;</span>, GradientBoostingClassifier(max_depth=<span class="number">5</span>, n_estimators=<span class="number">500</span>, random_state=<span class="number">42</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">blender_5 = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;sclaer&#x27;</span>, StandardScaler()), </span><br><span class="line">    (<span class="string">&#x27;MLP&#x27;</span>, MLPClassifier(hidden_layer_sizes=(<span class="number">30</span>, ), max_iter=<span class="number">10000</span>, random_state=<span class="number">42</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 10.é€šè¿‡äº¤å‰éªŒè¯ï¼Œé€‰æ‹©æœ€å¥½çš„å…ƒå­¦ä¹ å™¨</span></span><br><span class="line"><span class="keyword">for</span> blender <span class="keyword">in</span> [blender_1, blender_2, blender_3, blender_4, blender_5]:</span><br><span class="line">    cv_scores = cross_val_score(blender, X_val_predictions, y_val, cv=<span class="number">4</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;ğŸ¬<span class="subst">&#123;blender[-<span class="number">1</span>]&#125;</span> çš„äº¤å‰éªŒè¯å‡†ç¡®ç‡ä¸ºï¼š<span class="subst">&#123;<span class="built_in">round</span>(cv_scores.mean(), <span class="number">3</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>ğŸ¬LogisticRegression(max_iter=10000) çš„äº¤å‰éªŒè¯å‡†ç¡®ç‡ä¸ºï¼š0.959
ğŸ¬SVC() çš„äº¤å‰éªŒè¯å‡†ç¡®ç‡ä¸ºï¼š0.966
ğŸ¬RandomForestClassifier(max_depth=5, n_estimators=500, random_state=42) çš„äº¤å‰éªŒè¯å‡†ç¡®ç‡ä¸ºï¼š0.976
ğŸ¬GradientBoostingClassifier(max_depth=5, n_estimators=500, random_state=42) çš„äº¤å‰éªŒè¯å‡†ç¡®ç‡ä¸ºï¼š0.975
ğŸ¬MLPClassifier(hidden_layer_sizes=(30,), max_iter=10000, random_state=42) çš„äº¤å‰éªŒè¯å‡†ç¡®ç‡ä¸ºï¼š0.972
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 11.æ‹Ÿåˆæœ€ä½³ Blender</span></span><br><span class="line">blender_3.fit(X_val_predictions, y_val)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 12.åŸºäºæœ€ä½³ Blender ç»™å‡º Stacking æ¨¡å‹çš„é¢„æµ‹</span></span><br><span class="line">X_test_predictions = np.empty((<span class="built_in">len</span>(X_test), <span class="built_in">len</span>(estimators)), dtype=np.float32)</span><br><span class="line"><span class="keyword">for</span> index, estimator <span class="keyword">in</span> <span class="built_in">enumerate</span>(estimators):</span><br><span class="line">    X_test_predictions[:, index] = estimator.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 12.è¯„ä¼° Stacking æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡</span></span><br><span class="line">y_pred = blender_3.predict(X_test_predictions)    <span class="comment"># è¡¨ç°æœ€å¥½çš„ blender</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Stacking æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š<span class="subst">&#123;accuracy_score(y_test, y_pred)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Stacking ä¸­çš„å„åŸºåˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š<span class="subst">&#123;[estimator.score(X_test, y_test) <span class="keyword">for</span> estimator <span class="keyword">in</span> estimators]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Stacking æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š0.9717
Stacking ä¸­çš„å„åŸºåˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š[0.9645, 0.9691, 0.9719]
</code></pre><p>Remarkï¼š</p>
<pre><code>  1. æœ¬ä¾‹ä¸­ï¼ŒVoting æ¨¡å‹åœ¨æ³›åŒ–æ€§èƒ½ä¸Šç•¥ä¼˜äº Stacking æ¨¡å‹
  2. Stacking æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ç”šè‡³ä½äºæœ€ä½³åŸºåˆ†ç±»å™¨ (MLPClassifier) çš„æ³›åŒ–æ€§èƒ½
  3. æ•°æ®é›†åˆ’åˆ†æŒ‰ç…§ label è¿›è¡Œåˆ†å±‚æŠ½æ ·æ›´åˆç†
  4. åŸºåˆ†ç±»å™¨å’Œå…ƒå­¦ä¹ å™¨éƒ½æ²¡æœ‰ç»è¿‡è¶…å‚ä¼˜åŒ–ï¼Œè¿™å¯¼è‡´æœ€ç»ˆçš„ Voting &amp; Stacking æ¨¡å‹éƒ½æœªå¿…æ˜¯æœ€ä¼˜çš„
</code></pre>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/01/15/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Neural Style Transfer</title>
    <url>/2022/01/25/Neural-Style-Transfer/</url>
    <content><![CDATA[<p>ä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œå°†å›¾åƒé£æ ¼è¿ç§»åˆ°ä½ å–œæ¬¢çš„å›¾ç‰‡ä¸Š~</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">12</span>, <span class="number">12</span>)</span><br><span class="line">mpl.rcParams[<span class="string">&#x27;axes.grid&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> PIL.Image</span><br><span class="line"><span class="keyword">import</span> IPython.display <span class="keyword">as</span> display</span><br></pre></td></tr></table></figure>
<h2 id="åŠ è½½å¹¶å±•ç¤º-style-image-å’Œ-content-image"><a href="#åŠ è½½å¹¶å±•ç¤º-style-image-å’Œ-content-image" class="headerlink" title="åŠ è½½å¹¶å±•ç¤º style image å’Œ content image"></a>åŠ è½½å¹¶å±•ç¤º style image å’Œ content image</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># content image å’Œ style image çš„è·¯å¾„</span></span><br><span class="line">content_path = <span class="string">r&#x27;.\images\content\content_4.jpg&#x27;</span> </span><br><span class="line">style_path = <span class="string">r&#x27;.\images\style\style_1.jpg&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_img</span>(<span class="params">path_to_img</span>):</span></span><br><span class="line">    max_dim = <span class="number">512</span></span><br><span class="line">    img = tf.io.read_file(path_to_img)</span><br><span class="line">    img = tf.image.decode_image(img, channels=<span class="number">3</span>)</span><br><span class="line">    img = tf.image.convert_image_dtype(img, tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># è·å– img çš„å®½é«˜ &amp; è®¡ç®—ç¼©æ”¾åçš„ shape</span></span><br><span class="line">    shape = tf.cast(tf.shape(img)[:-<span class="number">1</span>], tf.float32)    </span><br><span class="line">    long_dim = <span class="built_in">max</span>(shape)</span><br><span class="line">    scale = max_dim / long_dim</span><br><span class="line">    new_shape = tf.cast(shape * scale, tf.int32)</span><br><span class="line"></span><br><span class="line">    img = tf.image.resize(img, new_shape)</span><br><span class="line">    img = img[tf.newaxis, :]    <span class="comment"># è¿˜åŸ batch ç»´åº¦</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span>(<span class="params">image, title=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(image.shape) &gt; <span class="number">3</span>:</span><br><span class="line">        image = tf.squeeze(image, axis=<span class="number">0</span>)</span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    <span class="keyword">if</span> title:</span><br><span class="line">        plt.title(title)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åŠ è½½å›¾åƒå¹¶å±•ç¤º</span></span><br><span class="line">content_image = load_img(content_path)</span><br><span class="line">style_image = load_img(style_path)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">imshow(content_image, <span class="string">&#x27;Content Image&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">imshow(style_image, <span class="string">&#x27;Style Image&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/25/Neural-Style-Transfer/output_5_0.png" alt="png"></p>
<h2 id="åŠ è½½é¢„è®­ç»ƒçš„-VGG19-å¹¶é€‰æ‹©è¡¨å¾-content-å’Œ-style-çš„å±‚"><a href="#åŠ è½½é¢„è®­ç»ƒçš„-VGG19-å¹¶é€‰æ‹©è¡¨å¾-content-å’Œ-style-çš„å±‚" class="headerlink" title="åŠ è½½é¢„è®­ç»ƒçš„ VGG19 å¹¶é€‰æ‹©è¡¨å¾ content å’Œ style çš„å±‚"></a>åŠ è½½é¢„è®­ç»ƒçš„ VGG19 å¹¶é€‰æ‹©è¡¨å¾ content å’Œ style çš„å±‚</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åŠ è½½ VGG19 (ä¸å«å…¨è¿æ¥å±‚)</span></span><br><span class="line">vgg = tf.keras.applications.VGG19(include_top=<span class="literal">False</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ˜¾ç¤º VGG19 ä¸­å±‚çš„åç§°</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> vgg.layers:</span><br><span class="line">    <span class="built_in">print</span>(layer.name)</span><br></pre></td></tr></table></figure>
<pre><code>input_1
block1_conv1
block1_conv2
block1_pool
block2_conv1
block2_conv2
block2_pool
block3_conv1
block3_conv2
block3_conv3
block3_conv4
block3_pool
block4_conv1
block4_conv2
block4_conv3
block4_conv4
block4_pool
block5_conv1
block5_conv2
block5_conv3
block5_conv4
block5_pool
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># é€‰å–å†…å®¹å±‚</span></span><br><span class="line">content_layers = [<span class="string">&#x27;block5_conv3&#x27;</span>, <span class="string">&#x27;block5_conv4&#x27;</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># é€‰å–é£æ ¼å±‚</span></span><br><span class="line">style_layers = [</span><br><span class="line">    <span class="string">&#x27;block1_conv2&#x27;</span>,    </span><br><span class="line">    <span class="string">&#x27;block2_conv2&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;block3_conv2&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;block4_conv1&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;block4_conv2&#x27;</span>, </span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">num_content_layers = <span class="built_in">len</span>(content_layers)</span><br><span class="line">num_style_layers = <span class="built_in">len</span>(style_layers)</span><br></pre></td></tr></table></figure>
<h2 id="è·å–-style-layers-çš„è¾“å‡º-è¿›è€Œè®¡ç®—-style"><a href="#è·å–-style-layers-çš„è¾“å‡º-è¿›è€Œè®¡ç®—-style" class="headerlink" title="è·å– style_layers çš„è¾“å‡º, è¿›è€Œè®¡ç®— style"></a>è·å– style_layers çš„è¾“å‡º, è¿›è€Œè®¡ç®— style</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_layers</span>(<span class="params">layer_names</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;  </span></span><br><span class="line"><span class="string">        è¯¥å‡½æ•°ç”Ÿæˆä¸€ä¸ªæ¨¡å‹, ä»¥è¿”å› VGG19 çš„ä¸­é—´å±‚çš„è¾“å‡º</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># åŠ è½½ä¸å«å…¨è¿æ¥å±‚çš„ VGG19 å¹¶è®¾ç½®ä¸ºä¸å¯è®­ç»ƒ</span></span><br><span class="line">    vgg = tf.keras.applications.VGG19(include_top=<span class="literal">False</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line">    vgg.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ ¹æ® &lt;å±‚åç§°&gt; è·å– &lt;å±‚è¾“å‡º&gt;, å¹¶æ„å»ºæ¨¡å‹</span></span><br><span class="line">    outputs = [vgg.get_layer(name).output <span class="keyword">for</span> name <span class="keyword">in</span> layer_names]</span><br><span class="line">    model = tf.keras.Model([vgg.<span class="built_in">input</span>], outputs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è·å– style_layers åœ¨ style_image ä¸Šçš„è¾“å‡º</span></span><br><span class="line">style_extractor = vgg_layers(style_layers)</span><br><span class="line">style_outputs = style_extractor(style_image * <span class="number">255</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span>(<span class="params">input_tensor</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        è¯¥å‡½æ•°ç”¨äºä» style_layers ä¸Šçš„è¾“å‡ºè®¡ç®—å‡º style</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    result = tf.linalg.einsum(<span class="string">&#x27;bijc,bijd-&gt;bcd&#x27;</span>, input_tensor, input_tensor)</span><br><span class="line">    input_shape = tf.shape(input_tensor)</span><br><span class="line">    num_locations = tf.cast(input_shape[<span class="number">1</span>] * input_shape[<span class="number">2</span>], tf.float32)</span><br><span class="line">    <span class="keyword">return</span> result/(num_locations)</span><br></pre></td></tr></table></figure>
<p>Remark: å›¾åƒçš„ style (å¦‚çº¹ç†ç­‰) å¯ä½¿ç”¨ CNN ä¸­ä¸åŒå±‚çš„ç›¸äº’å…³ç³»è¡¨ç¤º, å› æ­¤å¯ä»¥ä½¿ç”¨ä¸åŒå±‚è¾“å‡ºçš„ Gram Matrix å®šä¹‰ style.</p>
<h2 id="æå–-style-å’Œ-content"><a href="#æå–-style-å’Œ-content" class="headerlink" title="æå– style å’Œ content"></a>æå– style å’Œ content</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StyleContentModel</span>(<span class="params">tf.keras.models.Model</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        è¯¥ç±»ç”¨äºæå– content(content_layers çš„è¾“å‡º) å’Œ style(ç”¨ style_layers çš„è¾“å‡ºè®¡ç®—å¾—åˆ°)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, style_layers, content_layers</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(StyleContentModel, self).__init__()</span><br><span class="line">        <span class="comment"># åˆ›å»ºä¸€ä¸ªæå– style_layers å’Œ content_layers çš„è¾“å‡ºçš„(ä¸å¯è®­ç»ƒçš„)æ¨¡å‹</span></span><br><span class="line">        self.vgg = vgg_layers(style_layers + content_layers)</span><br><span class="line">        self.style_layers = style_layers</span><br><span class="line">        self.content_layers = content_layers</span><br><span class="line">        self.num_style_layers = <span class="built_in">len</span>(style_layers)</span><br><span class="line">        self.vgg.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="string">&quot;Expects float input in [0,1]&quot;</span></span><br><span class="line">        <span class="comment"># åˆ†åˆ«è·å– style_layers å’Œ content_layers çš„è¾“å‡º</span></span><br><span class="line">        inputs = inputs * <span class="number">255.0</span></span><br><span class="line">        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)</span><br><span class="line">        outputs = self.vgg(preprocessed_input)</span><br><span class="line">        style_outputs, content_outputs = (outputs[:self.num_style_layers],</span><br><span class="line">                                          outputs[self.num_style_layers:])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># æ ¹æ® style_outputs è®¡ç®— style</span></span><br><span class="line">        style_outputs = [gram_matrix(style_output)</span><br><span class="line">                         <span class="keyword">for</span> style_output <span class="keyword">in</span> style_outputs]</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># content_layers åˆ° content çš„æ˜ å°„</span></span><br><span class="line">        content_dict = &#123;content_name: value <span class="keyword">for</span> content_name, value</span><br><span class="line">                        <span class="keyword">in</span> <span class="built_in">zip</span>(self.content_layers, content_outputs)&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># style_layers åˆ° style çš„æ˜ å°„</span></span><br><span class="line">        style_dict = &#123;style_name: value <span class="keyword">for</span> style_name, value</span><br><span class="line">                      <span class="keyword">in</span> <span class="built_in">zip</span>(self.style_layers, style_outputs)&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;content&#x27;</span>: content_dict, <span class="string">&#x27;style&#x27;</span>: style_dict&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æå– style_image çš„ style å’Œ content_image çš„ content</span></span><br><span class="line"><span class="comment"># è¿™å°±æ˜¯ &lt;ç›®æ ‡å†…å®¹&gt; å’Œ &lt;ç›®æ ‡é£æ ¼&gt;, ç”¨æ¥æ„é€  loss func</span></span><br><span class="line">extractor = StyleContentModel(style_layers, content_layers)</span><br><span class="line"></span><br><span class="line">style_targets = extractor(style_image)[<span class="string">&#x27;style&#x27;</span>]</span><br><span class="line">content_targets = extractor(content_image)[<span class="string">&#x27;content&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="å¼€å§‹é£æ ¼è¿ç§»"><a href="#å¼€å§‹é£æ ¼è¿ç§»" class="headerlink" title="å¼€å§‹é£æ ¼è¿ç§»"></a>å¼€å§‹é£æ ¼è¿ç§»</h2><p>ä½¿ç”¨ Gradient Descent å°† style_image çš„ style è½¬ç§»åˆ° content_image ä¸Š, åŒæ—¶ä¿ç•™å…¶åŸæœ¬çš„ content</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¯¥å˜é‡å­˜å‚¨è¦ç”Ÿæˆçš„å›¾åƒ</span></span><br><span class="line">image = tf.Variable(content_image)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip_0_1</span>(<span class="params">image</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        è¯¥å‡½æ•°ä¿è¯ç”Ÿæˆå›¾åƒçš„åƒç´ å¼ºåº¦åœ¨ [0, 1] ä¸­</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> tf.clip_by_value(image, clip_value_min=<span class="number">0.0</span>, clip_value_max=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adam ä¼˜åŒ–å™¨</span></span><br><span class="line">opt = tf.optimizers.Adam(learning_rate=<span class="number">0.02</span>, beta_1=<span class="number">0.99</span>, epsilon=<span class="number">1e-1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è®¾ç½® style loss &amp; content loss çš„æƒé‡</span></span><br><span class="line">style_weight = <span class="number">1e-2</span></span><br><span class="line">content_weight = <span class="number">1e4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># è®¾ç½® total_variation_loss çš„æƒé‡</span></span><br><span class="line">total_variation_weight = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_content_loss</span>(<span class="params">outputs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        å®šä¹‰å®Œæ•´çš„ loss func</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># ç”Ÿæˆå›¾åƒçš„ style å’Œ content</span></span><br><span class="line">    style_outputs = outputs[<span class="string">&#x27;style&#x27;</span>]</span><br><span class="line">    content_outputs = outputs[<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># style loss = mean((current_style - target_style)^2) * corresponding_weight</span></span><br><span class="line">    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name])**<span class="number">2</span>) </span><br><span class="line">                           <span class="keyword">for</span> name <span class="keyword">in</span> style_outputs.keys()])</span><br><span class="line">    style_loss *= style_weight / num_style_layers</span><br><span class="line"></span><br><span class="line">    <span class="comment"># content loss = mean((current_content - target_content)^2) * corresponding_weight</span></span><br><span class="line">    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_targets[name])**<span class="number">2</span>) </span><br><span class="line">                             <span class="keyword">for</span> name <span class="keyword">in</span> content_outputs.keys()])</span><br><span class="line">    content_loss *= content_weight / num_content_layers</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># actual loss </span></span><br><span class="line">    loss = style_loss + content_loss</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">image</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        è¯¥å‡½æ•°æ‰§è¡Œ Gradient Descent  </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        outputs = extractor(image)</span><br><span class="line">        loss = style_content_loss(outputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># æ·»åŠ ä¸€ä¸ªæ­£åˆ™åŒ–æŸå¤±é¡¹, ä»¥é™ä½&quot;é«˜é¢‘ä¼ªå½±&quot;</span></span><br><span class="line">        loss += total_variation_weight * tf.image.total_variation(image)</span><br><span class="line"></span><br><span class="line">    grad = tape.gradient(loss, image)</span><br><span class="line">    opt.apply_gradients([(grad, image)])</span><br><span class="line">    image.assign(clip_0_1(image))</span><br></pre></td></tr></table></figure>
<p>Remark: The total variation is the sum of the absolute differences for neighboring pixel-values in the input images. This measures how much noise is in the images.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensor_to_image</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        ä» tensor ç”Ÿæˆ PIL.Image</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tensor = tensor * <span class="number">255</span></span><br><span class="line">    tensor = np.array(tensor, dtype=np.uint8)</span><br><span class="line">    <span class="keyword">if</span> np.ndim(tensor) &gt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">assert</span> tensor.shape[<span class="number">0</span>] == <span class="number">1</span></span><br><span class="line">        tensor = tensor[<span class="number">0</span>]    <span class="comment"># æ¶ˆé™¤ batch ç»´åº¦</span></span><br><span class="line">    <span class="keyword">return</span> PIL.Image.fromarray(tensor)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»“åˆä¸Šè¿°å†…å®¹å³å¯å¼€å§‹é£æ ¼è¿ç§»</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">steps_per_epoch = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(steps_per_epoch):</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line">        train_step(image)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;.&quot;</span>, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line">    display.display(tensor_to_image(image))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Train step: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(step))</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total time: &#123;:.1f&#125;&quot;</span>.<span class="built_in">format</span>(end-start))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/25/Neural-Style-Transfer/output_23_0.png" alt="png"></p>
<pre><code>Train step: 1000
Total time: 192.5
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ•ˆæœå±•ç¤º</span></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">18</span>, <span class="number">24</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">imshow(content_image, <span class="string">&#x27;Content Image&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">imshow(style_image, <span class="string">&#x27;Style Image&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">imshow(image, <span class="string">&#x27;Generated Image&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/25/Neural-Style-Transfer/output_24_0.png" alt="png"></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ã€ç¬”è®°ã€‘æ•°å­¦åˆ†ææ–°è®²-ç¬¬1ç¯‡</title>
    <url>/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/</url>
    <content><![CDATA[<h1 align="center" style="font-size:50px">ç¬¬ä¸€ç¯‡ &nbsp &nbsp åˆ†æåŸºç¡€</h1>

<span id="more"></span>
<p>[TOC]</p>
<h1 id="â„ç¬¬é›¶ç« -é¢„å¤‡çŸ¥è¯†"><a href="#â„ç¬¬é›¶ç« -é¢„å¤‡çŸ¥è¯†" class="headerlink" title="â„ç¬¬é›¶ç«         é¢„å¤‡çŸ¥è¯†"></a>â„ç¬¬é›¶ç«         é¢„å¤‡çŸ¥è¯†</h1><h2 id="â—-æ ¸å¿ƒæ¦‚å¿µ-amp-ç»“è®º"><a href="#â—-æ ¸å¿ƒæ¦‚å¿µ-amp-ç»“è®º" class="headerlink" title="â— æ ¸å¿ƒæ¦‚å¿µ &amp; ç»“è®º"></a>â— æ ¸å¿ƒæ¦‚å¿µ &amp; ç»“è®º</h2><p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Chapter 0 é¢„å¤‡çŸ¥è¯†.jpg" style="zoom:100%;">  </p>
<ol>
<li><p>$ \sum_{k=1}^{n}k = \frac{1}{2} n(n+1) $ </p>
</li>
<li><p>$ \sum_{k=1}^{n}k^2 = \frac{1}{6} n(n+1)(2n+1) $ </p>
</li>
<li><p>$ \sum_{k=1}^{n}k^3 = [\frac{1}{2} n(n+1)]^2 $ </p>
</li>
</ol>
<hr>
<h1 id="â„ç¬¬ä¸€ç« -å®æ•°"><a href="#â„ç¬¬ä¸€ç« -å®æ•°" class="headerlink" title="â„ç¬¬ä¸€ç«         å®æ•°"></a>â„ç¬¬ä¸€ç«         å®æ•°</h1><h2 id="â—-æ ¸å¿ƒæ¦‚å¿µ-amp-ç»“è®º-1"><a href="#â—-æ ¸å¿ƒæ¦‚å¿µ-amp-ç»“è®º-1" class="headerlink" title="â— æ ¸å¿ƒæ¦‚å¿µ &amp; ç»“è®º"></a>â— æ ¸å¿ƒæ¦‚å¿µ &amp; ç»“è®º</h2><p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\Chapter 1 å®æ•°.jpg" width="90%"> </p>
<ol>
<li><p><strong>ç¡®ç•ŒåŸç†</strong>ï¼š $\R$ çš„ä»»æ„éç©ºæœ‰ä¸Šç•Œçš„å­é›†åœ¨ $\R$ ä¸­æœ‰ä¸Šç¡®ç•Œï¼›</p>
</li>
<li><p>å¸¸ç”¨ä¸ç­‰å¼ï¼š</p>
<p>$<br>â‘ \quad \big||a|-|b|\big|\leq|a \pm b|\leq|a|+|b| \\<br>â‘¡\quad |a_{1} + a_{2} + \dots + a_{n}| \leq |a_{1}| + |a_{2}| + \dots + |a_{n}| \\<br>â‘¢\quad \text{Bernoulli ä¸ç­‰å¼ï¼š} (1 + x)^{n} \geq 1 + nx, \; \forall \; x \geq -1\\<br>â‘£\quad \text{ç®—æœ¯å¹³å‡-å‡ ä½•å¹³å‡ä¸ç­‰å¼ï¼š} \frac{x_{1} + x_{2} + \dots + x_{n}}{n} \geq \sqrt[n]            </p>
<pre><code>&#123;x_&#123;1&#125;x_&#123;2&#125; \dots x_&#123;n&#125;&#125;, \; \forall \; x_&#123;1&#125;, x_&#123;2&#125;, \dots,x_&#123;n&#125; \geq 0 \\
</code></pre><p>â‘¤\quad \sin x &lt; x &lt; \tan x, \; \forall \; x \in (0, \frac{\pi}{2}) \\<br>â‘¥\quad |\sin x| \leq |x|, \; \forall \; x \in \R<br>$ </p>
</li>
</ol>
<hr>
<h1 id="â„ç¬¬äºŒç« -æé™"><a href="#â„ç¬¬äºŒç« -æé™" class="headerlink" title="â„ç¬¬äºŒç«         æé™"></a>â„ç¬¬äºŒç«         æé™</h1><h2 id="â—-æ ¸å¿ƒæ¦‚å¿µ-amp-ç»“è®º-2"><a href="#â—-æ ¸å¿ƒæ¦‚å¿µ-amp-ç»“è®º-2" class="headerlink" title="â— æ ¸å¿ƒæ¦‚å¿µ &amp; ç»“è®º"></a>â— æ ¸å¿ƒæ¦‚å¿µ &amp; ç»“è®º</h2><p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\Chapter 2 æé™.jpg" width="100%">  </p>
<ol>
<li>Relation â‘ ï¼š</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2thm3.png" width="50%" align="left"></p>
<ol>
<li>Relation â‘¡ï¼š</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s4thm4.png" width="50%" align="left"></p>
<ol>
<li>Relation â‘¢ï¼š</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s6thm1.png" width="50%" align="left"></p>
<ol>
<li>æœ‰ç•Œåºåˆ—ä¸æ— ç©·å°åºåˆ—ï¼š</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1thm1.png" width="50%" align="left"></p>
<ol>
<li>å…³äº â€œæ”¶æ•›åºåˆ—ä¿åºæ€§ã®æ¨è®ºâ€ çš„æ³¨è®°ï¼š</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2remark.png" width="50%" align="left"></p>
<ol>
<li>å…³äº â€œé—­åŒºé—´å¥—åŸç†â€ çš„æ³¨è®°ï¼š</li>
</ol>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3remark.png" width="50%" align="left"></p>
<h2 id="â—-æœ¬ç« ã®æ³¨è®°"><a href="#â—-æœ¬ç« ã®æ³¨è®°" class="headerlink" title="â— æœ¬ç« ã®æ³¨è®°"></a>â— æœ¬ç« ã®æ³¨è®°</h2><ol>
<li>æœ¬ç« ä»‹ç»äº†åˆ†æåŸºç¡€ä¸­æœ€åŸºæœ¬çš„ä¸¤ä¸ªæ¦‚å¿µï¼š<strong>æ”¶æ•›åºåˆ—</strong> å’Œ <strong>å‡½æ•°æé™</strong> </li>
<li>åºåˆ—æ”¶æ•›æ€§å¸¸ç”± $\epsilon-N$ å®šä¹‰æè¿°</li>
<li>å‡½æ•°äºä¸€ç‚¹çš„æ”¶æ•›æ€§å¸¸ç”± $\epsilon-\delta$ å®šä¹‰ or åºåˆ—å¼å®šä¹‰æè¿° </li>
<li>æŸ¯è¥¿æ”¶æ•›åŸç†ä»…é€‚ç”¨äºæœ‰ç©·æé™çš„æƒ…å†µ</li>
<li>å•è°ƒæ”¶æ•›åŸç†å¯é€‚ç”¨äºæ— ç©·æé™çš„æƒ…å†µ</li>
</ol>
<h2 id="â—-å…¸å‹é—®é¢˜è§£æ"><a href="#â—-å…¸å‹é—®é¢˜è§£æ" class="headerlink" title="â— å…¸å‹é—®é¢˜è§£æ"></a>â— å…¸å‹é—®é¢˜è§£æ</h2><h3 id="ç¬¬ä¸€èŠ‚"><a href="#ç¬¬ä¸€èŠ‚" class="headerlink" title="@ ç¬¬ä¸€èŠ‚"></a><strong>@ ç¬¬ä¸€èŠ‚</strong></h3><p>1ã€æŒ‰å®šä¹‰è¯æ˜åºåˆ—æœ‰ç•Œ / æ— ç•Œï¼ˆåŸºäºäºŒé¡¹å±•å¼€çš„ä¸ç­‰å¼ç¼©æ”¾ã€åŸºäºåˆ†é¡¹åˆå¹¶çš„ä¸ç­‰å¼ç¼©æ”¾ï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e3.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e5.png" width="400px" align="left"> </p>
<p>2ã€æŒ‰ $\epsilon-N$ å®šä¹‰è¯æ˜åºåˆ—ä¸ºæ— ç©·å°ï¼ˆåŸºäº Bernoulli ä¸ç­‰å¼çš„ç¼©æ”¾ã€åŸºäºäºŒé¡¹å±•å¼€çš„ä¸ç­‰å¼ç¼©æ”¾ï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e7.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e8.png" width="400px" align="left"> </p>
<p>3ã€æŒ‰ Squeeze Theorem è¯æ˜åºåˆ—ä¸ºæ— ç©·å°ï¼ˆé€é¡¹ç¼©æ”¾ï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e9.png" width="400px" align="left"> </p>
<p>4ã€æœ‰é™ä¸ªæ— ç©·å°åºåˆ—çš„å’Œä¸ç§¯ä¹Ÿæ˜¯æ— ç©·å°åºåˆ—ã€æœ‰ç•Œé‡ä¸æ— ç©·å°åºåˆ—çš„ç§¯ä¹Ÿæ˜¯æ— ç©·å°åºåˆ—</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e10.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e11.png" width="400px" align="left">  </p>
<p>5ã€æŒ‰ $\epsilon-N$ å®šä¹‰è¯æ˜åºåˆ—ä¸ºæ— ç©·å°ï¼ˆn é¡¹å’Œçš„å½¢å¼ï¼Œæ‹†ä¸ºä¸¤éƒ¨åˆ†ï¼Œåˆ†åˆ«ç¼©æ”¾ï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e12.png" width="400px" align="left"></p>
<p>6ã€ æŒ‰ Squeeze Theorem è¯æ˜åºåˆ—ä¸ºæ— ç©·å°ï¼ˆç®—æ•°å¹³å‡-å‡ ä½•å¹³å‡ä¸ç­‰å¼ï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e13.png" width="400px" align="left"></p>
<p><strong>Remark</strong>ï¼šç”±ä¸Šé¢ä¸¤ä¸ªä¾‹å­ï¼Œæœ‰ï¼šä»¥ â€œç”±æ— ç©·å°åºåˆ—çš„å‰ n é¡¹æ„é€ çš„ç®—æ•°å¹³å‡æ•°ã€å‡ ä½•å¹³å‡æ•°â€ ä¸ºé€šé¡¹çš„åºåˆ—ä¹Ÿæ˜¯æ— ç©·å°åºåˆ— </p>
<p>7ã€å°† $z_{n}$ çœ‹ä½œæ— ç©·å°åºåˆ— $\{\frac{1}{n}\}$ çš„å‰ n é¡¹æ‰€æ„é€ çš„å‡ ä½•å¹³å‡æ•° </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s1e14.png" width="400px" align="left"> </p>
<h3 id="ç¬¬äºŒèŠ‚"><a href="#ç¬¬äºŒèŠ‚" class="headerlink" title="@ ç¬¬äºŒèŠ‚"></a><strong>@ ç¬¬äºŒèŠ‚</strong></h3><p>1ã€è¯æ˜ $x_{n}\to x$ <strong>ç­‰ä»·äºè¯æ˜</strong> $(x_{n}-x) \to 0$ <strong>ç­‰ä»·äºè¯æ˜</strong> $x_{n} = x + \alpha_{n}$ï¼Œå…¶ä¸­ $\alpha_{n}$ æ˜¯æ— ç©·å°åºåˆ—</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e3.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e4.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e6.png" width="400px" align="left"></p>
<p>2ã€åˆ©ç”¨åºåˆ—æé™çš„å››åˆ™è¿ç®—æ€§è´¨è®¡ç®—æ–°åºåˆ—æé™</p>
<p> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e7.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e9.png" width="400px" align="left"></p>
<p><strong>Remark</strong>ï¼š</p>
<ol>
<li><p>ç”±ä¾‹ 3 å’Œä¾‹ 7 çŸ¥ï¼š$\forall \; a &gt; 0, \; \sqrt[n]{a} \to 1, n \to \infty$  </p>
</li>
<li><p>ç”±ä¾‹ 6 çŸ¥ï¼šå½“ $x_{n} \to x$ æ—¶ï¼Œ$\dfrac{x_{1} + x_{2} + \dots + x_{n}}{n} \to x, \; n \to \infty$ </p>
</li>
<li><p>ç”±ä¸‹é¢ä¾‹ 10 çŸ¥ï¼šå½“æ­£å®æ•°åºåˆ— $x_{n} \to x(&gt;0)$ æ—¶ï¼Œ$\sqrt[n]{x_{1}x_{2} \dots x_{n}} \to x, \; n \to \infty$ </p>
</li>
</ol>
<p>3ã€éœ€è¦å¯¹ $c$ çš„å–å€¼åˆ†ç±»è®¨è®ºï¼Œå…¶ä¸­å½“ $c&gt;0$ æ—¶ï¼Œå¯é€šè¿‡ç¼©æ”¾ã€ä¸Šè¿° Remark 1ã€Squeeze Theorem å¾—åˆ°ç»“è®º </p>
<p> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e8.png" width="400px" align="left"> </p>
<p>4ã€åˆ©ç”¨ Squeeze Theorem è¯æ˜çš„åºåˆ—æ”¶æ•›æ€§ã€ç®—æœ¯å¹³å‡-å‡ ä½•å¹³å‡ä¸ç­‰å¼ï¼ˆæ‰©å±•ï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e10_1.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e10_2.png" width="400px" align="left"></p>
<p>5ã€å°† $x_{n} \to x$ ç­‰ä»·è½¬åŒ–ä¸º $x_{n} = x + \alpha_{n}$ï¼Œå…¶ä¸­ $\alpha_{n}$ æ˜¯æ— ç©·å°åºåˆ—åï¼Œè¯æ˜åºåˆ—æ”¶æ•›çš„é—®é¢˜å¾—åˆ°ç®€åŒ–</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e11_1.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e11_2.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e12_1.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e12_2.png" width="400px" align="left"></p>
<p>6ã€é€šè¿‡æ¯”å€¼æé™çš„ä¿åºæ€§ï¼Œæ¨å‡ºåˆ†å­ä¸åˆ†æ¯çš„å¤§å°å…³ç³»</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s2e13.png" width="400px" align="left"></p>
<h3 id="ç¬¬ä¸‰èŠ‚"><a href="#ç¬¬ä¸‰èŠ‚" class="headerlink" title="@ ç¬¬ä¸‰èŠ‚"></a><strong>@ ç¬¬ä¸‰èŠ‚</strong></h3><p>1ã€åˆ©ç”¨<strong>å•è°ƒæ”¶æ•›åŸç†</strong>æ±‚åºåˆ—æé™çš„ä¸‰ä¸ªä¾‹å­ï¼ˆé€’æ¨å…¬å¼ä¸¤è¾¹å–æé™ï¼Œå¾—åˆ°å…³äºæé™å€¼çš„æ–¹ç¨‹ï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3e1.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3e2_1.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3e2_2.png" width="400px" align="left">  </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3e3_1.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3e3_2.png" width="400px" align="left"></p>
<p>2ã€é‡è¦æé™ $(1 + \dfrac{1}{n})^{n} \to e, \; n \to \infty$ çš„å‡ºå¤„</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3e4.png" width="400PX" align="left"></p>
<p>3ã€åˆ©ç”¨<strong>æŸ¯è¥¿æ”¶æ•›åŸç†</strong>è¯æ˜åºåˆ—å‘æ•£ or æ”¶æ•›</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3e6.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3e7.png" width="400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s3e8.png" width="400px" align="left"></p>
<h3 id="ç¬¬å››èŠ‚"><a href="#ç¬¬å››èŠ‚" class="headerlink" title="@ ç¬¬å››èŠ‚"></a><strong>@ ç¬¬å››èŠ‚</strong></h3><p>1ã€åˆ©ç”¨ Stolz å®šç†æ±‚ $\dfrac{*}{\infty}$ å‹æé™</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s4e5.png" width="400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s4e6.png" width="400px" align="left"></p>
<h3 id="ç¬¬äº”èŠ‚"><a href="#ç¬¬äº”èŠ‚" class="headerlink" title="@ ç¬¬äº”èŠ‚"></a><strong>@ ç¬¬äº”èŠ‚</strong></h3><p>1ã€ æŒ‰ Heine åºåˆ—å¼å®šä¹‰è¯æ˜å‡½æ•°ï¼ˆåœ¨è¿ç»­ç‚¹çš„ï¼‰æé™</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e3.png" style="width:400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e5.png" style="width:400px" align="left"></p>
<p>2ã€æŒ‰  Heine åºåˆ—å¼å®šä¹‰è¯æ˜å‡½æ•°ï¼ˆåœ¨éè¿ç»­ç‚¹çš„ï¼‰æé™</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e6.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e7_1.png" style="width:400px" align="left"></p>
<p> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e7_2.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e8.png" style="width:400px" align="left"></p>
<p>3ã€<strong>å¤šé¡¹å¼å‡½æ•°</strong> ä¸ <strong>æœ‰ç†å‡½æ•°</strong> çš„æé™ã€ç»è¿‡<strong>æ¢å…ƒ</strong>å¾—åˆ°çš„æœ‰ç†å‡½æ•°æé™</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e9_1.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e9_2.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e10.png" style="width:400px" align="left"></p>
<p>Remarkï¼š</p>
<ol>
<li>ä¾‹ 7 ä¸ºé‡è¦æé™ï¼š$\lim_\limits{x\to0}\dfrac{\sin x}{x}=1$ </li>
<li>ä¾‹ 9 ä»‹ç»äº†å¤šé¡¹å¼åŠæœ‰ç†å‡½æ•°çš„æé™</li>
</ol>
<p>4ã€åˆ©ç”¨é‡è¦æé™è®¡ç®—æé™çš„ä¾‹å­ï¼ˆå‡½æ•°æé™çš„å››åˆ™è¿ç®—ã€$1 - \cos x = 2(\sin(\frac{x}{2}))^2$ï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e12.png" style="width:400px" align="left"></p>
<p>5ã€æŒ‰ $\epsilon-\delta$ å®šä¹‰è¯æ˜å‡½æ•°æé™</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e13.png" style="width:400px" align="left"> <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c2s5e14.png" style="width:400px" align="left"></p>
<h3 id="Final-Remark"><a href="#Final-Remark" class="headerlink" title="Final Remark"></a><strong>Final Remark</strong></h3><p><strong>â‘  è¯æ˜åºåˆ—æ”¶æ•›ã€æ±‚æ”¶æ•›åºåˆ—æé™</strong>  </p>
<ol>
<li><p>$\epsilon-N$ å®šä¹‰ &amp; ä¸ç­‰å¼ç¼©æ”¾</p>
</li>
<li><p>Squeeze Theorem &amp; ä¸ç­‰å¼ç¼©æ”¾</p>
</li>
<li><p>åºåˆ—æé™ã®å››åˆ™è¿ç®—</p>
</li>
<li><p>$(x_{n} \to x) \Leftrightarrow \; (x_{n} - x \to 0) \Leftrightarrow x_{n} = x + \alpha_{n}$ï¼Œå…¶ä¸­ $\alpha_{n}$ ä¸ºæ— ç©·å°åºåˆ—</p>
</li>
<li><p>å•è°ƒæ”¶æ•›åŸç† &amp; é€’æ¨å¼ä¸¤è¾¹å–æé™</p>
</li>
<li><p>Cauchy æ”¶æ•›åŸç† &amp; ä¸ç­‰å¼ç¼©æ”¾ ï¼ˆåªèƒ½ç”¨äºè¯æ˜åºåˆ—æ”¶æ•›ä¸å¦ï¼‰</p>
</li>
<li><p>Stolz Theorem ï¼ˆ$\dfrac{\ast}{+\infty}$ å‹æé™ï¼‰</p>
</li>
<li><p>ä¸ç­‰å¼ç¼©æ”¾æŠ€å·§ï¼šäºŒé¡¹å¼å®šç†ã€Bernoulli ä¸ç­‰å¼ã€é€é¡¹ç¼©æ”¾ã€æ‹†åˆ† N é¡¹å’Œï¼Œåˆ†åˆ«ç¼©æ”¾ã€ç®—æœ¯å¹³å‡-å‡ ä½•å¹³å‡ä¸ç­‰å¼</p>
</li>
<li><p>ä¸€äº›å·²è¯æ˜çš„ç»“è®ºï¼š</p>
<p>| $\dfrac{1}{a^{n}}, \dfrac{n}{a^{n}} \to 0, \; |a| &gt; 1$       | $\dfrac{n^{k}}{a^{n}} \to 0, \; a &gt; 1, k \in \N$             | $\dfrac{a^{n}}{n!} \to 0, \; a &gt; 0$                          |<br>| :â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”- | :â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”- | :â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”- |<br>| $\dfrac{\sum_{k=1}^{n} \alpha_{k}}{n} \to \alpha \in \R, \; \alpha_{n} \to \alpha$ | $\sqrt[n]{\prod_{k=1}^{n} \alpha_{k}} \to \alpha &gt; 0, \; \alpha_{n} \to \alpha, \; \alpha_{n} &gt; 0$ | $\sqrt[n]{\prod_{k=1}^{n} \alpha_{k}} \to 0, \; \alpha_{n} \to 0, \alpha_{n} \geq 0$ |<br>| $\sqrt[n]{a} \to 1, \; a &gt; 0$                                | $\sqrt[n]{n} \to 1$                                          |                                                              |<br>| $(1 + \dfrac{1}{n})^{n} \to e$                               | $x_{n} = \dfrac{1}{2}(x_{n-1} + \dfrac{a}{x_{n-1}}) \to \sqrt{a}, \; x_{0} &gt; 0, \; a &gt; 0$ |                                                              |</p>
</li>
</ol>
<p><strong>â‘¡ å¯¹å……åˆ†å¤§çš„ nï¼Œåˆ¤åˆ«åºåˆ—é€šé¡¹çš„å¤§å°å…³ç³»</strong></p>
<ol>
<li>æ”¶æ•›åºåˆ—ä¿åºæ€§ï¼ˆ$n^{k} &lt; k^{n} &lt; n!$ï¼Œè‡ªç„¶æ•° $k \geq 2$ï¼‰</li>
</ol>
<p><strong>â‘¢ è¯æ˜å‡½æ•°äºæŸç‚¹æ”¶æ•›</strong></p>
<ol>
<li>Heine åºåˆ—å¼å®šä¹‰ &amp; ä¸ç­‰å¼ç¼©æ”¾ï¼ˆ&amp; Squeeze Theoremï¼‰</li>
<li>$\epsilon-\delta$ å®šä¹‰ &amp; ä¸ç­‰å¼ç¼©æ”¾</li>
<li>Cauchy æ”¶æ•›åŸç†</li>
</ol>
<p><strong>â‘£ è®¡ç®—å‡½æ•°æé™</strong></p>
<ol>
<li>å››åˆ™è¿ç®—</li>
<li>é‡è¦æé™ï¼ˆ$\lim\limits_{x \to 0}\dfrac{\sin x}{x} = 1$ï¼‰</li>
<li>å¤šé¡¹å¼ã€æœ‰ç†å‡½æ•°ã®æé™</li>
</ol>
<hr>
<h1 id="â„ç¬¬ä¸‰ç« -è¿ç»­å‡½æ•°"><a href="#â„ç¬¬ä¸‰ç« -è¿ç»­å‡½æ•°" class="headerlink" title="â„ç¬¬ä¸‰ç«         è¿ç»­å‡½æ•°"></a>â„ç¬¬ä¸‰ç«         è¿ç»­å‡½æ•°</h1><h2 id="â—-æ ¸å¿ƒæ¦‚å¿µ-amp-ç»“è®º-3"><a href="#â—-æ ¸å¿ƒæ¦‚å¿µ-amp-ç»“è®º-3" class="headerlink" title="â— æ ¸å¿ƒæ¦‚å¿µ &amp; ç»“è®º"></a>â— æ ¸å¿ƒæ¦‚å¿µ &amp; ç»“è®º</h2><p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\Chapter 3 è¿ç»­å‡½æ•°.jpg" align="center">   </p>
<ol>
<li><p>Relation â‘ ï¼š</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s1thm6.png" width="50%" align="left"></p>
</li>
</ol>
<ol>
<li>Remarkï¼šæœ€å¤§å€¼ã€æœ€å°å€¼å®šç†ä¸­çš„æœ€å€¼å³ä¸º $f$ åœ¨ $[a, b]$ ä¸­çš„ä¸Šã€ä¸‹ç¡®ç•Œ</li>
</ol>
<ol>
<li><p><strong>è¿ç»­ä¸ä¸€è‡´è¿ç»­çš„åŒºåˆ«</strong>ï¼ˆæŒ‰ Îµ-Î´ å®šä¹‰ï¼‰ï¼š</p>
<ol>
<li><p>è¿ç»­ï¼šå¯¹ç»™å®š $\epsilon &gt; 0$ï¼Œåœ¨ä¸åŒ $x_{0}$ å¤„ç›¸åº”çš„ $\delta$ æœªå¿…ç›¸åŒ</p>
</li>
<li><p>ä¸€è‡´è¿ç»­ï¼šå¯¹ä»»æ„ $\epsilon &gt; 0$ï¼Œå­˜åœ¨é€‚ç”¨äºä¸€åˆ‡ $x_{0}$ çš„ $\delta &gt; 0$ï¼Œåªè¦ $|x - x_{0}| &lt; \delta$ï¼Œå°±æœ‰ $|f(x) - f(x_{0})| &lt; \epsilon$ </p>
</li>
</ol>
</li>
</ol>
<ol>
<li><p>â‘  ä»‹å€¼å®šç†</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s2thm2.png" width="50%" align="left"></p>
<p>â‘¡ å‡ ä½•å¼é™ˆè¿°</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s3thm1.png" width="50%" align="left"></p>
<p>â‘¢ ä¸Šè¿°å®šç†çš„é€†å‘½é¢˜ä¸€èˆ¬è€Œè¨€æ˜¯ä¸æˆç«‹çš„â€¦â€¦</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s3thm2.png" width="70%" align="left"></p>
</li>
</ol>
<ol>
<li><p>æŒ‡æ•°å‡½æ•°ã®å®šä¹‰è¿‡ç¨‹ï¼š</p>
<p>â€‹    â‘  æœ¬ç« ç¬¬ä¸‰èŠ‚ä¾‹ 5 ä¸­å®šä¹‰äº†<strong>ç®—æœ¯æ ¹</strong>ã€‚</p>
<p>â€‹    â‘¡ åŸºäºç®—æœ¯æ ¹å®šä¹‰<strong>åˆ†æ•°æ¬¡æ–¹å¹‚</strong>å¦‚ä¸‹ï¼š</p>
<p>â€‹            è®¾ $m, n \in \N$ï¼š</p>
<p>â€‹            â… ã€ å®šä¹‰  $a^{\frac{m}{n}}=(\sqrt[n]{a})^m, \; a \geq 0$ </p>
<p>â€‹            â…¡ã€ å®šä¹‰  $a^{-\frac{m}{n}} = \dfrac{1}{a^{\frac{m}{n}}} = \dfrac{1}{(\sqrt[n]{a})^m}, \; a &gt; 0$ </p>
<p>â€‹            â…¢ã€ å®šä¹‰  $a^{0} = 1, \; a &gt; 0$ </p>
<p>â€‹            è‡³æ­¤ï¼Œå½“ $a &gt; 0$ æ—¶ï¼Œ$\forall r \in \Q, \; a^{r}$ æœ‰å®šä¹‰ã€‚</p>
<p>â€‹    â‘¢ åŸºäºåˆ†æ•°æ¬¡æ–¹å¹‚ï¼ˆåŠæœ¬ç« ç¬¬å››èŠ‚ã®å¼•ç† 1 &amp; å¼•ç† 2ï¼‰å®šä¹‰<strong>å®æ•°æ¬¡æ–¹å¹‚</strong>å¦‚ä¸‹ï¼š</p>
<p>â€‹            è®¾ $a&gt;0, \; \forall x \in \R$ï¼Œå®šä¹‰ $a^{x} = \lim\limits_{n \to \infty}a^{q_{n}}$ï¼Œå…¶ä¸­ $\{q_{n}\}$ ä¸ºæ”¶æ•›äº $x$ çš„ä»»æ„<strong>æœ‰ç†</strong>åºåˆ—ã€‚</p>
<p>â€‹            å¦æœ‰å®æ•°æ¬¡æ–¹å¹‚ã®æ€§è´¨å¦‚ä¸‹ï¼š</p>
<p>â€‹            <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s4thm2.png" width="50%" align="center"> </p>
</li>
</ol>
<ol>
<li><p>åŸºæœ¬åˆç­‰å‡½æ•°ï¼š</p>
<p>â€‹    â‘  å¹‚å‡½æ•°ï¼ˆä»¥åŠå¤šé¡¹å¼å‡½æ•°ï¼‰ </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_powerx_3.png" width="60%" align="center"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_powerx_4.png" width="60%" align="center"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_powerx_5.png" width="60%" align="center"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_powerx_2.png" width="90%" align="center"></p>
</li>
</ol>
<p>   â€‹    â‘¡ ä¸‰è§’å‡½æ•°ã€åä¸‰è§’å‡½æ•°</p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_sinx.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_cosx.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_tanx_cotx.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_tanx_cotx_2.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_arcsinx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_arccosx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_arctanx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_arccotx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_secx_cscx.png" width="90%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_secx_cscx_2.png" width="90%" align="center"></p>
<p>   â€‹    â‘¢ å¯¹æ•°å‡½æ•°ã€æŒ‡æ•°å‡½æ•°</p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_expx.png" width="70%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_expx_2.png" width="70%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_logx.png" width="30%" align="center"></p>
<p>   <img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\func_logx_2.png" width="30%" align="center"></p>
<ol>
<li><p>ç¬¦å· $O()ã€o()ã€ï½$ çš„å®šä¹‰ï¼š</p>
<script type="math/tex; mode=display">
\text{å½“} \; x \to a \; \text{æ—¶}
    \begin{cases}
        f(x) = O\Big(g(x)\Big): & \Big\lvert \frac{f(x)}{g(x)} \Big\lvert \leq M, \; x                     \in \check{U}(a) \\
        f(x) = o\Big(g(x)\Big): & \lim\limits_{x \to a} \frac{f(x)}{g(x)} = 0 \\
        f(x) \; ï½ \; g(x): & \lim\limits_{x \to a} \frac{f(x)}{g(x)} = 1 \\ 
    \end{cases}</script></li>
<li><p><strong>é‡è¦æé™</strong>æ€»ç»“ï¼š</p>
<p>â€‹    â‘   $\lim\limits_{x \to 0}\dfrac{\sin x}{x} = 1$ </p>
<p>â€‹    â‘¡  $\lim\limits_{x \to \infty} \Big( 1 + \dfrac{1}{x} \Big) ^ x = e \; \Longleftrightarrow \; \lim\limits_{a \to 0} \big( 1 + a \big) ^ \frac{1}{a} = e$ </p>
<p>â€‹    â‘¢  å¯¹ â‘¡ å³è¾¹å–å¯¹æ•°ï¼Œå¾—ï¼š$\lim\limits_{a \to 0}\dfrac{\ln(1+a)}{a} = 1$ ã€ç±»ä¼¼åœ°æœ‰ï¼š $\lim\limits_{a \to 0}\dfrac{\log_{b}(1+a)}{a} = \dfrac{1}{\ln b}$ã€‘</p>
<p>â€‹    â‘£  $\lim\limits_{a \to 0}\dfrac{e^{a} - 1}{a} = 1$ ã€ä»¤ $\beta = e^{a} - 1$ å³å¯è½¬åŒ–ä¸º â‘¢ ã®å€’æ•°å½¢å¼ï¼Œç±»ä¼¼åœ°æœ‰ï¼š$\lim\limits_{a \to 0}\dfrac{b^{a} - 1}{a} = \ln b$ã€‘</p>
<p>â€‹    â‘¤  $\lim \limits_{a \to 0} \dfrac{(1 + a)^{\mu}-1}{a} = \mu$ ã€ç”± $(1+a)^{\mu} = e^{\mu \times \ln (1+a)}$ï¼ŒåŸæé™å¯åŒ–ä¸º â‘¢ å’Œ â‘£ ä¹‹ç§¯ã®å½¢å¼ã€‘</p>
</li>
</ol>
<h2 id="â—-æœ¬ç« ã®æ³¨è®°-1"><a href="#â—-æœ¬ç« ã®æ³¨è®°-1" class="headerlink" title="â— æœ¬ç« ã®æ³¨è®°"></a>â— æœ¬ç« ã®æ³¨è®°</h2><ol>
<li>æœ¬ç« ä»‹ç»äº† <strong>å‡½æ•°äºä¸€ç‚¹ã®è¿ç»­æ€§</strong> ä»¥åŠ <strong>é—­åŒºé—´ä¸Šè¿ç»­å‡½æ•°ã®æ€§è´¨</strong></li>
<li>å‡½æ•°äºä¸€ç‚¹çš„è¿ç»­æ€§æœ¬è´¨ä¸Šæ˜¯é€šè¿‡æé™å®šä¹‰çš„ï¼Œå› è€Œæœ‰ä¸¤ç§ç­‰ä»·å®šä¹‰ï¼š<strong>åºåˆ—å¼å®šä¹‰</strong> å’Œ <strong>$\epsilon-\delta$ å®šä¹‰</strong>ï¼›ç”±äºè¿™ç§å®šä¹‰æ–¹å¼ï¼Œå‡½æ•°åœ¨è¿ç»­ç‚¹é™„è¿‘æ˜¯<strong>å±€éƒ¨æœ‰ç•Œ</strong>ã€<strong>å±€éƒ¨ä¿åº</strong>çš„ï¼ŒåŒæ—¶<strong>å››åˆ™è¿ç®—ã€å‡½æ•°å¤åˆä¹Ÿä¿æŒè¯¥ç‚¹çš„è¿ç»­æ€§</strong></li>
<li>é—´æ–­ç‚¹æ˜¯å¯¹åº”äºè¿ç»­ç‚¹çš„æ¦‚å¿µï¼Œå®ƒå¯ç”±è¯¥ç‚¹å¤„å•ä¾§æé™çš„æƒ…å†µåŒºåˆ†ä¸º<strong>ä¸¤ç±»é—´æ–­ç‚¹</strong></li>
<li>ç”±å‡½æ•°åœ¨ä¸€ç‚¹è¿ç»­ï¼ˆåŠå•ä¾§è¿ç»­ï¼‰å³å¯å®šä¹‰å‡½æ•°åœ¨ä¸€ä¸ªé—­åŒºé—´ä¸Šè¿ç»­ï¼Œæ­¤æ—¶å‡½æ•°å…·æœ‰å¦‚ä¸‹<strong>æ•´ä½“æ€§è´¨</strong>ï¼š<ul>
<li>é›¶å€¼å®šç†ã€ä»‹å€¼å®šç†</li>
<li>æœ‰ç•Œæ€§ã€æœ€å€¼å®šç†</li>
<li>ä¸€è‡´è¿ç»­æ€§</li>
</ul>
</li>
<li>ä»å•è°ƒå‡½æ•°å‡ºå‘å®šä¹‰äº†<strong>åå‡½æ•°</strong>ã€ä»ç®—æœ¯æ ¹å‡ºå‘å®šä¹‰äº†<strong>æŒ‡æ•°å‡½æ•°</strong>ã€è¿›è€Œå®šä¹‰äº†å…¶åå‡½æ•°â€”â€”<strong>å¯¹æ•°å‡½æ•°</strong>ï¼Œå¹¶ä»‹ç»äº†å®ƒä»¬çš„åŸºæœ¬æ€§è´¨</li>
<li>æŒ‡å‡ºäº†<strong>åˆç­‰å‡½æ•°ã®è¿ç»­æ€§</strong></li>
<li>ä»‹ç»äº†äº”ç±»<strong>é‡è¦æé™</strong></li>
<li>ä»‹ç»äº†<strong>ç­‰ä»·é‡</strong> &amp; å®ƒä»¬åœ¨å‡½æ•°æé™è®¡ç®—ä¸­çš„ä½œç”¨</li>
</ol>
<h2 id="â—-å…¸å‹é—®é¢˜è§£æ-1"><a href="#â—-å…¸å‹é—®é¢˜è§£æ-1" class="headerlink" title="â— å…¸å‹é—®é¢˜è§£æ"></a>â— å…¸å‹é—®é¢˜è§£æ</h2><h3 id="ç¬¬ä¸€èŠ‚-1"><a href="#ç¬¬ä¸€èŠ‚-1" class="headerlink" title="@ ç¬¬ä¸€èŠ‚"></a><strong>@ ç¬¬ä¸€èŠ‚</strong></h3><p>1ã€åˆ©ç”¨å·¦å³æé™åˆ¤æ–­å‡½æ•°ã®é—´æ–­ç‚¹ç±»å‹ï¼ˆåŒ…æ‹¬ç‰¹æ®Šã®<strong>ç‹„åˆ©å…‹é›·å‡½æ•°å’Œé»æ›¼å‡½æ•°</strong>ï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s1e4e5.png" style="width:400px" align="left"><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s1e6e7.png" style="width:400px" align="left"></p>
<h3 id="ç¬¬äºŒèŠ‚-1"><a href="#ç¬¬äºŒèŠ‚-1" class="headerlink" title="@ ç¬¬äºŒèŠ‚"></a><strong>@ ç¬¬äºŒèŠ‚</strong></h3><p>1ã€é›¶å€¼å®šç†ã®åº”ç”¨ï¼š<strong>Brouwer ä¸åŠ¨ç‚¹å®šç† &amp; å¯¹åˆ†åŒºé—´æ³•æ±‚æ–¹ç¨‹æ ¹çš„è¿‘ä¼¼å€¼</strong> </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s2e1.png" style="width:400px" align="left"><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s2e2.png" style="width:400px" align="left"></p>
<p>2ã€ä»å®šä¹‰å‡ºå‘åˆ¤æ–­ä¸€è‡´è¿ç»­æ€§</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s2e5e6.png" style="width:400px" align="left"> </p>
<p>3ã€åˆ©ç”¨ä¸€è‡´è¿ç»­æ€§ã®ç­‰ä»·æ¡ä»¶ï¼ˆåºåˆ—å¼æè¿°ï¼‰è¯æ˜å‡½æ•°ä¸ä¸€è‡´è¿ç»­</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s2e7.png" style="width:400px" align="left"> </p>
<h3 id="ç¬¬ä¸‰èŠ‚-1"><a href="#ç¬¬ä¸‰èŠ‚-1" class="headerlink" title="@ ç¬¬ä¸‰èŠ‚"></a><strong>@ ç¬¬ä¸‰èŠ‚</strong></h3><p>1ã€ è™½ç„¶è¿ç»­å‡½æ•°å¯å°†åŒºé—´æ˜ å°„ä¸ºåŒºé—´ï¼Œä½†å°†åŒºé—´æ˜ å°„ä¸ºåŒºé—´çš„æœªå¿…æ˜¯è¿ç»­å‡½æ•°</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s3e1.png" style="width:400px" align="left"></p>
<p>2ã€ç®—æœ¯æ ¹ã®å®šä¹‰ä¸å­˜åœ¨å”¯ä¸€æ€§</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s3e5.png" style="width:400px" align="left"></p>
<h3 id="ç¬¬å››èŠ‚-1"><a href="#ç¬¬å››èŠ‚-1" class="headerlink" title="@ ç¬¬å››èŠ‚"></a><strong>@ ç¬¬å››èŠ‚</strong></h3><p>1ã€<strong>å¹‚å‡½æ•°</strong>ä¸<strong>å¹‚æŒ‡å‡½æ•°</strong>çš„è¿ç»­æ€§ï¼ˆå°†ä»–ä»¬è§†ä¸ºè¿ç»­å‡½æ•°çš„å¤åˆï¼‰</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s4e1e2.png" style="width:400px" align="left"></p>
<h3 id="ç¬¬äº”èŠ‚-1"><a href="#ç¬¬äº”èŠ‚-1" class="headerlink" title="@ ç¬¬äº”èŠ‚"></a><strong>@ ç¬¬äº”èŠ‚</strong></h3><p>1ã€ $(x-a)^{k} \;\; (x \to a)$ å‹æ— ç©·å°ã€$\dfrac{1}{(x-a)^{k}} \;\; (x \to a)$ å‹æ— ç©·å¤§ã€$x^{k} \;\; (x \to \infty)$ å‹æ— ç©·å¤§</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s5e6e7e8.png" style="width:400px" align="left"></p>
<p>2ã€$x \to +\infty$ æ—¶ï¼Œæ— ç©·å¤§ã®é˜¶<strong>é™åºæ’åˆ—</strong>ä¸ºï¼š$a^{x}$ã€$x^{\mu}$ã€$\log_{a}x$ï¼Œå…¶ä¸­ $a &gt; 1, \mu &gt; 0$  </p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s5e9.png" style="width:400px" align="left"></p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s5e10.png" style="width:400px" align="left"></p>
<p>3ã€åˆ©ç”¨ç­‰ä»·å› å¼æ›¿æ¢æ±‚å‡½æ•°æé™</p>
<p><img src="/2023/05/14/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E6%96%B0%E8%AE%B2-%E7%AC%AC1%E7%AF%87/Users/Gozen Sanji\Desktop\STUDY\å­¦ä¹ ç¬”è®°-Typora\c3s5e11e12e13e14.png" style="width:400px" align="left"> </p>
<h3 id="Final-Remark-1"><a href="#Final-Remark-1" class="headerlink" title="Final Remark"></a><strong>Final Remark</strong></h3><p><strong>â‘  å‡½æ•°é—´æ–­ç‚¹ï¼ˆåŠå…¶ç±»å‹ï¼‰ã®åˆ¤å®š</strong> </p>
<ol>
<li>$f(a-), \; f(a+)$ ã®å­˜åœ¨æ€§ã€å–å€¼ &amp; $f(a)$ ã®å–å€¼</li>
</ol>
<p><strong>â‘¡ æ±‚æ–¹ç¨‹æ ¹ã®è¿‘ä¼¼å€¼</strong></p>
<ol>
<li>åŒºé—´å¯¹åˆ†æ³•ï¼ˆç†è®ºä¾æ®ä¸ºé›¶å€¼å®šç†ï¼‰</li>
</ol>
<p><strong>â‘¢ åˆ¤æ–­å‡½æ•°ã®ä¸€è‡´è¿ç»­æ€§</strong></p>
<ol>
<li>$\epsilon-\delta$ å®šä¹‰</li>
<li>ä¸€è‡´è¿ç»­æ€§ã®ç­‰ä»·æ¡ä»¶ï¼ˆåºåˆ—å¼æè¿°ï¼‰å¸¸ç”¨äºè¯æ˜å‡½æ•°ä¸ä¸€è‡´è¿ç»­</li>
</ol>
<p><strong>â‘£ åˆ¤å®šé«˜é˜¶æ— ç©·å°ï¼ˆå¤§ï¼‰</strong></p>
<ol>
<li>æé™å®šä¹‰</li>
</ol>
<p><strong>â‘¤ æ±‚å‡½æ•°æé™</strong></p>
<ol>
<li>ç­‰ä»·å› å¼æ›¿æ¢</li>
</ol>
<hr>
<h1 id="â˜€ç¬¬ä¸€ç¯‡ã®æ€»ç»“"><a href="#â˜€ç¬¬ä¸€ç¯‡ã®æ€»ç»“" class="headerlink" title="â˜€ç¬¬ä¸€ç¯‡ã®æ€»ç»“"></a>â˜€ç¬¬ä¸€ç¯‡ã®æ€»ç»“</h1><p>æœ¬ç¯‡ä»‹ç»äº†æ•°å­¦åˆ†æä¸­ä¸€äº›<strong>æœ€åŸºæœ¬çš„æ¦‚å¿µ</strong>ï¼š</p>
<ol>
<li>é›†åˆ &amp; æ˜ å°„ã€å®æ•°ç³» $\R$ï¼›</li>
<li>åºåˆ—æé™ &amp; å‡½æ•°æé™ ã® æ¦‚å¿µä¸åŸºæœ¬æ€§è´¨ï¼›</li>
<li>å‡½æ•°åœ¨ä¸€ç‚¹çš„è¿ç»­æ€§ &amp; å±€éƒ¨æ€§è´¨ã€å•ä¾§è¿ç»­ &amp; é—´æ–­ç‚¹ï¼›</li>
<li>å‡½æ•°åœ¨é—­åŒºé—´ä¸Šçš„è¿ç»­æ€§ &amp; <strong>æ•´ä½“æ€§è´¨</strong>ã€åˆç­‰å‡½æ•°çš„è¿ç»­æ€§ï¼›</li>
</ol>
<p>åŒæ—¶ä¹Ÿä»‹ç»äº†è®¸å¤šå¸¸ç”¨ä¸ç­‰å¼å’Œé‡è¦æé™ã€‚</p>
<p>æ¶‰åŠçš„<strong>å…¸å‹é—®é¢˜</strong>åŒ…æ‹¬ï¼š</p>
<ol>
<li>åºåˆ—æ”¶æ•›æ€§ &amp; æé™è®¡ç®—ï¼›</li>
<li>å‡½æ•°æ”¶æ•›æ€§ &amp; æé™è®¡ç®—ï¼›</li>
<li>å‡½æ•°é—´æ–­ç‚¹çš„åˆ¤å®šã€ä¸€è‡´è¿ç»­æ€§çš„åˆ¤å®š</li>
</ol>
<p>ç›¸åº”ä¾‹é¢˜å’Œæ€»ç»“å¯åœ¨ç¬¬äºŒã€ä¸‰ç« çš„å…¸å‹é—®é¢˜è§£æä¸­æ‰¾åˆ°ã€‚</p>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>Mathematics</tag>
        <tag>Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>å˜åˆ†è‡ªç¼–ç å™¨ (Variational Autoencoder)</title>
    <url>/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/</url>
    <content><![CDATA[<p><strong>Variational Autoencoder</strong>:</p>
<ol>
<li><p>æ˜¯ä¸€ç§æ¦‚ç‡è‡ªç¼–ç å™¨, å…¶è¾“å‡ºæœ‰ä¸€å®šçš„éšæœºæ€§;</p>
</li>
<li><p>æ˜¯ä¸€ç§ç”Ÿæˆå¼è‡ªç¼–ç å™¨, èƒ½å¤Ÿç”Ÿæˆç±»ä¼¼è®­ç»ƒé›†ä¸­æ•°æ®çš„æ–°å®ä¾‹.</p>
<span id="more"></span>
</li>
</ol>
<hr>
<p>ä¸å¸¸è§„è‡ªç¼–ç å™¨ä¸åŒ, å˜åˆ†è‡ªç¼–ç å™¨ä¸ç›´æ¥å¯¹ç»™å®š input è¿›è¡Œç¼–ç , è€Œæ˜¯ä½¿ encoder ç”Ÿæˆå‡å€¼ $Î¼$ å’Œæ ‡å‡†å·® $Ïƒ$, ç„¶åä»å¯¹åº”é«˜æ–¯åˆ†å¸ƒ $N(Î¼, Ïƒ^2)$ ä¸­éšæœºé‡‡æ ·å¾—åˆ° input å¯¹åº”çš„ç¼–ç , ç„¶å decoder å†å¯¹æ­¤ç¼–ç è¿›è¡Œè§£ç , å°±å¾—åˆ°ä¸è®­ç»ƒå®ä¾‹ç±»ä¼¼çš„æ•°æ®.</p>
<hr>
<p>å˜åˆ†è‡ªç¼–ç å™¨çš„ cost func ç”±ä¸¤éƒ¨åˆ†ç»„æˆ: </p>
<ol>
<li>å¸¸è§„çš„é‡æ„æŸå¤±(å¯ç”¨ cross entropy è¡¨ç¤º) </li>
<li>latent loss(ç”±é«˜æ–¯åˆ†å¸ƒä¸å®é™…åˆ†å¸ƒä¹‹é—´çš„ $KL$æ•£åº¦ è¡¨ç¤º)</li>
</ol>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/variational_autoencoder.PNG" width="70%"></p>
<p>ä¸‹é¢åŸºäº Fashion MNIST æ„å»ºä¸€ä¸ªå˜åˆ†è‡ªç¼–ç å™¨.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<h2 id="åŠ è½½-amp-åˆ’åˆ†æ•°æ®é›†"><a href="#åŠ è½½-amp-åˆ’åˆ†æ•°æ®é›†" class="headerlink" title="åŠ è½½ &amp; åˆ’åˆ†æ•°æ®é›†"></a>åŠ è½½ &amp; åˆ’åˆ†æ•°æ®é›†</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line">X_train_full = X_train_full.astype(np.float32) / <span class="number">255</span></span><br><span class="line">X_test = X_test.astype(np.float32) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">X_train, X_valid = X_train_full[:-<span class="number">5000</span>], X_train_full[-<span class="number">5000</span>:]</span><br><span class="line">y_train, y_valid = y_train_full[:-<span class="number">5000</span>], y_train_full[-<span class="number">5000</span>:]</span><br></pre></td></tr></table></figure>
<h2 id="è‡ªå®šä¹‰é‡‡æ ·å±‚"><a href="#è‡ªå®šä¹‰é‡‡æ ·å±‚" class="headerlink" title="è‡ªå®šä¹‰é‡‡æ ·å±‚"></a>è‡ªå®šä¹‰é‡‡æ ·å±‚</h2><p>ç»™å®šå‚æ•° $Î¼, Ïƒ$, ä¸‹é¢çš„å±‚å¯¹ codings é‡‡æ ·.<br>[ä¸‹é¢å‡å®š latent loss æ˜¯åŸºäº $Î³ = log(Ïƒ^2)$ çš„ $KL$ æ•£åº¦]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">K = keras.backend</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sampling</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        mean, log_var = inputs</span><br><span class="line">        <span class="keyword">return</span> K.random_normal(tf.shape(log_var)) * K.exp(log_var/<span class="number">2</span>) + mean</span><br></pre></td></tr></table></figure>
<p>æœ‰äº†è¿™ä¸ªå±‚, å°±å¯ä»¥ä»é«˜æ–¯åˆ†å¸ƒ $N(Î¼, Ïƒ^2)$ ä¸­å¯¹ codings å‘é‡é‡‡æ ·äº†.</p>
<h2 id="æ„å»º-encoder"><a href="#æ„å»º-encoder" class="headerlink" title="æ„å»º encoder"></a>æ„å»º encoder</h2><p>codings_size, dense layer ä¸­çš„ neurons å’Œ activation éƒ½å¯ä»¥ä¿®æ”¹.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">codings_size = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">inputs = keras.layers.Input(shape=[<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">z = keras.layers.Flatten()(inputs)</span><br><span class="line">z = keras.layers.Dense(<span class="number">14</span> * <span class="number">14</span>, activation=<span class="string">&quot;elu&quot;</span>)(z)</span><br><span class="line">z = keras.layers.Dense(<span class="number">7</span> * <span class="number">7</span>, activation=<span class="string">&quot;elu&quot;</span>)(z)</span><br><span class="line"><span class="comment"># è®¡ç®— Î¼ &amp; Î³</span></span><br><span class="line">codings_mean = keras.layers.Dense(codings_size)(z)</span><br><span class="line">codings_log_var = keras.layers.Dense(codings_size)(z)</span><br><span class="line"><span class="comment"># å¯¹ç¼–ç é‡‡æ ·</span></span><br><span class="line">codings = Sampling()([codings_mean, codings_log_var])</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç»„åˆä¸º encoder</span></span><br><span class="line">variational_encoder = keras.models.Model(</span><br><span class="line">    inputs=[inputs], </span><br><span class="line">    outputs=[codings_mean, codings_log_var, codings]    <span class="comment"># å‰ä¸¤ä¸ªè¾“å‡ºä»…ä½œæ£€æŸ¥ç”¨</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="æ„å»º-decoder"><a href="#æ„å»º-decoder" class="headerlink" title="æ„å»º decoder"></a>æ„å»º decoder</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">decoder_inputs = keras.layers.Input(shape=[codings_size])    <span class="comment"># è€ƒè™‘ä¸€ä¸‹ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ª shape ~</span></span><br><span class="line"><span class="comment"># å…³äºç¼–ç å±‚å¯¹ç§°çš„ä¸¤ä¸ª Dense å±‚</span></span><br><span class="line">x = keras.layers.Dense(<span class="number">7</span> * <span class="number">7</span>, activation=<span class="string">&quot;elu&quot;</span>)(decoder_inputs)</span><br><span class="line">x = keras.layers.Dense(<span class="number">14</span> * <span class="number">14</span>, activation=<span class="string">&quot;elu&quot;</span>)(x)</span><br><span class="line"><span class="comment"># è¿˜åŸè¾“å…¥çš„ç¥ç»å…ƒæ•°</span></span><br><span class="line"><span class="comment"># sigmoid æ¿€æ´»å°†æ¯ä¸ªåƒç´ å€¼çœ‹ä½œåˆ†ç±»ä»»åŠ¡ (å¯¹åº”ç¼–è¯‘æ—¶çš„é‡æ„æŸå¤±)</span></span><br><span class="line">x = keras.layers.Dense(<span class="number">28</span> * <span class="number">28</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)(x)</span><br><span class="line">outputs = keras.layers.Reshape([<span class="number">28</span>, <span class="number">28</span>])(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç»„åˆä¸º decoder</span></span><br><span class="line">variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])</span><br></pre></td></tr></table></figure>
<h2 id="ç»„åˆ-encoder-amp-decoder"><a href="#ç»„åˆ-encoder-amp-decoder" class="headerlink" title="ç»„åˆ encoder &amp; decoder"></a>ç»„åˆ encoder &amp; decoder</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_, _, codings = variational_encoder(inputs)</span><br><span class="line">reconstructions = variational_decoder(codings)</span><br><span class="line"></span><br><span class="line">variational_autoencoder = keras.models.Model(inputs=[inputs], outputs=[reconstructions])</span><br></pre></td></tr></table></figure>
<h2 id="ç¼–è¯‘"><a href="#ç¼–è¯‘" class="headerlink" title="ç¼–è¯‘"></a>ç¼–è¯‘</h2><p>è¿™é‡Œçš„ loss åˆ†ä¸º latent loss å’Œ reconstruction loss ä¸¤éƒ¨åˆ†.<br>[å¦å¤–, ä¸å¦¨è¯•è¯•å…¶ä»–ä¼˜åŒ–å™¨~]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è‡ªå®šä¹‰çš„ metrics</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rounded_accuracy</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> keras.metrics.binary_accuracy(tf.<span class="built_in">round</span>(y_true), tf.<span class="built_in">round</span>(y_pred))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è®¡ç®— loss</span></span><br><span class="line">latent_loss = -<span class="number">0.5</span> * K.<span class="built_in">sum</span>(</span><br><span class="line">    <span class="number">1</span> + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean), </span><br><span class="line">    axis=-<span class="number">1</span>)</span><br><span class="line">variational_autoencoder.add_loss(K.mean(latent_loss) / <span class="number">784.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.RMSprop(learning_rate=<span class="number">0.003</span>)</span><br><span class="line"></span><br><span class="line">variational_autoencoder.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, </span><br><span class="line">                                optimizer=optimizer, </span><br><span class="line">                                metrics=[rounded_accuracy])</span><br></pre></td></tr></table></figure>
<h2 id="è®­ç»ƒ"><a href="#è®­ç»ƒ" class="headerlink" title="è®­ç»ƒ"></a>è®­ç»ƒ</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">performance_lr_scheduler = keras.callbacks.ReduceLROnPlateau(patience=<span class="number">7</span>, factor=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">history = variational_autoencoder.fit(X_train, X_train, </span><br><span class="line">                                      epochs=<span class="number">100</span>, batch_size=<span class="number">64</span>, </span><br><span class="line">                                      validation_data=(X_valid, X_valid), </span><br><span class="line">                                      callbacks=[early_stopping_cb, performance_lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3663 - rounded_accuracy: 0.8768 - val_loss: 0.3566 - val_rounded_accuracy: 0.8752 - lr: 0.0030
Epoch 2/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3336 - rounded_accuracy: 0.9013 - val_loss: 0.3351 - val_rounded_accuracy: 0.9037 - lr: 0.0030
Epoch 3/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3275 - rounded_accuracy: 0.9065 - val_loss: 0.3298 - val_rounded_accuracy: 0.9024 - lr: 0.0030
Epoch 4/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3245 - rounded_accuracy: 0.9089 - val_loss: 0.3288 - val_rounded_accuracy: 0.9057 - lr: 0.0030
Epoch 5/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3223 - rounded_accuracy: 0.9108 - val_loss: 0.3267 - val_rounded_accuracy: 0.9102 - lr: 0.0030
Epoch 6/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3208 - rounded_accuracy: 0.9121 - val_loss: 0.3250 - val_rounded_accuracy: 0.9070 - lr: 0.0030
Epoch 7/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3196 - rounded_accuracy: 0.9130 - val_loss: 0.3237 - val_rounded_accuracy: 0.9138 - lr: 0.0030
Epoch 8/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3188 - rounded_accuracy: 0.9138 - val_loss: 0.3233 - val_rounded_accuracy: 0.9115 - lr: 0.0030
Epoch 9/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3181 - rounded_accuracy: 0.9143 - val_loss: 0.3259 - val_rounded_accuracy: 0.9118 - lr: 0.0030
Epoch 10/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3175 - rounded_accuracy: 0.9148 - val_loss: 0.3270 - val_rounded_accuracy: 0.9098 - lr: 0.0030
Epoch 11/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3171 - rounded_accuracy: 0.9152 - val_loss: 0.3200 - val_rounded_accuracy: 0.9167 - lr: 0.0030
Epoch 12/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3167 - rounded_accuracy: 0.9155 - val_loss: 0.3205 - val_rounded_accuracy: 0.9138 - lr: 0.0030
Epoch 13/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3164 - rounded_accuracy: 0.9157 - val_loss: 0.3219 - val_rounded_accuracy: 0.9112 - lr: 0.0030
Epoch 14/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3160 - rounded_accuracy: 0.9160 - val_loss: 0.3242 - val_rounded_accuracy: 0.9137 - lr: 0.0030
Epoch 15/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3159 - rounded_accuracy: 0.9161 - val_loss: 0.3160 - val_rounded_accuracy: 0.9178 - lr: 0.0030
Epoch 16/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3156 - rounded_accuracy: 0.9162 - val_loss: 0.3217 - val_rounded_accuracy: 0.9097 - lr: 0.0030
Epoch 17/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3154 - rounded_accuracy: 0.9164 - val_loss: 0.3207 - val_rounded_accuracy: 0.9105 - lr: 0.0030
Epoch 18/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3154 - rounded_accuracy: 0.9164 - val_loss: 0.3236 - val_rounded_accuracy: 0.9114 - lr: 0.0030
Epoch 19/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3152 - rounded_accuracy: 0.9168 - val_loss: 0.3173 - val_rounded_accuracy: 0.9166 - lr: 0.0030
Epoch 20/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3150 - rounded_accuracy: 0.9168 - val_loss: 0.3196 - val_rounded_accuracy: 0.9127 - lr: 0.0030
Epoch 21/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3149 - rounded_accuracy: 0.9169 - val_loss: 0.3235 - val_rounded_accuracy: 0.9120 - lr: 0.0030
Epoch 22/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3147 - rounded_accuracy: 0.9170 - val_loss: 0.3213 - val_rounded_accuracy: 0.9144 - lr: 0.0030
Epoch 23/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3071 - rounded_accuracy: 0.9248 - val_loss: 0.3098 - val_rounded_accuracy: 0.9238 - lr: 6.0000e-04
Epoch 24/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3066 - rounded_accuracy: 0.9253 - val_loss: 0.3093 - val_rounded_accuracy: 0.9240 - lr: 6.0000e-04
Epoch 25/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3064 - rounded_accuracy: 0.9255 - val_loss: 0.3095 - val_rounded_accuracy: 0.9232 - lr: 6.0000e-04
Epoch 26/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3062 - rounded_accuracy: 0.9257 - val_loss: 0.3093 - val_rounded_accuracy: 0.9239 - lr: 6.0000e-04
Epoch 27/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3061 - rounded_accuracy: 0.9258 - val_loss: 0.3088 - val_rounded_accuracy: 0.9242 - lr: 6.0000e-04
Epoch 28/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3060 - rounded_accuracy: 0.9259 - val_loss: 0.3090 - val_rounded_accuracy: 0.9245 - lr: 6.0000e-04
Epoch 29/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3060 - rounded_accuracy: 0.9260 - val_loss: 0.3090 - val_rounded_accuracy: 0.9247 - lr: 6.0000e-04
Epoch 30/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3059 - rounded_accuracy: 0.9260 - val_loss: 0.3088 - val_rounded_accuracy: 0.9243 - lr: 6.0000e-04
Epoch 31/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3058 - rounded_accuracy: 0.9261 - val_loss: 0.3085 - val_rounded_accuracy: 0.9249 - lr: 6.0000e-04
Epoch 32/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3057 - rounded_accuracy: 0.9262 - val_loss: 0.3085 - val_rounded_accuracy: 0.9247 - lr: 6.0000e-04
Epoch 33/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3057 - rounded_accuracy: 0.9262 - val_loss: 0.3088 - val_rounded_accuracy: 0.9246 - lr: 6.0000e-04
Epoch 34/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3056 - rounded_accuracy: 0.9263 - val_loss: 0.3083 - val_rounded_accuracy: 0.9247 - lr: 6.0000e-04
Epoch 35/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3056 - rounded_accuracy: 0.9264 - val_loss: 0.3086 - val_rounded_accuracy: 0.9249 - lr: 6.0000e-04
Epoch 36/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3055 - rounded_accuracy: 0.9265 - val_loss: 0.3086 - val_rounded_accuracy: 0.9252 - lr: 6.0000e-04
Epoch 37/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3054 - rounded_accuracy: 0.9265 - val_loss: 0.3086 - val_rounded_accuracy: 0.9242 - lr: 6.0000e-04
Epoch 38/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3054 - rounded_accuracy: 0.9266 - val_loss: 0.3085 - val_rounded_accuracy: 0.9250 - lr: 6.0000e-04
Epoch 39/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3054 - rounded_accuracy: 0.9266 - val_loss: 0.3085 - val_rounded_accuracy: 0.9250 - lr: 6.0000e-04
Epoch 40/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3053 - rounded_accuracy: 0.9266 - val_loss: 0.3080 - val_rounded_accuracy: 0.9255 - lr: 6.0000e-04
Epoch 41/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3053 - rounded_accuracy: 0.9267 - val_loss: 0.3080 - val_rounded_accuracy: 0.9253 - lr: 6.0000e-04
Epoch 42/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3052 - rounded_accuracy: 0.9267 - val_loss: 0.3084 - val_rounded_accuracy: 0.9251 - lr: 6.0000e-04
Epoch 43/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3052 - rounded_accuracy: 0.9268 - val_loss: 0.3082 - val_rounded_accuracy: 0.9252 - lr: 6.0000e-04
Epoch 44/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3052 - rounded_accuracy: 0.9269 - val_loss: 0.3081 - val_rounded_accuracy: 0.9257 - lr: 6.0000e-04
Epoch 45/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3051 - rounded_accuracy: 0.9268 - val_loss: 0.3084 - val_rounded_accuracy: 0.9248 - lr: 6.0000e-04
Epoch 46/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3051 - rounded_accuracy: 0.9270 - val_loss: 0.3080 - val_rounded_accuracy: 0.9252 - lr: 6.0000e-04
Epoch 47/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3050 - rounded_accuracy: 0.9270 - val_loss: 0.3080 - val_rounded_accuracy: 0.9257 - lr: 6.0000e-04
Epoch 48/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9280 - val_loss: 0.3068 - val_rounded_accuracy: 0.9270 - lr: 1.2000e-04
Epoch 49/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9281 - val_loss: 0.3069 - val_rounded_accuracy: 0.9266 - lr: 1.2000e-04
Epoch 50/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9280 - val_loss: 0.3067 - val_rounded_accuracy: 0.9268 - lr: 1.2000e-04
Epoch 51/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9280 - val_loss: 0.3069 - val_rounded_accuracy: 0.9267 - lr: 1.2000e-04
Epoch 52/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9280 - val_loss: 0.3068 - val_rounded_accuracy: 0.9267 - lr: 1.2000e-04
Epoch 53/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3040 - rounded_accuracy: 0.9282 - val_loss: 0.3068 - val_rounded_accuracy: 0.9270 - lr: 1.2000e-04
Epoch 54/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3067 - val_rounded_accuracy: 0.9269 - lr: 1.2000e-04
Epoch 55/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3068 - val_rounded_accuracy: 0.9266 - lr: 1.2000e-04
Epoch 56/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3068 - val_rounded_accuracy: 0.9268 - lr: 1.2000e-04
Epoch 57/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3069 - val_rounded_accuracy: 0.9266 - lr: 1.2000e-04
Epoch 58/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9269 - lr: 2.4000e-05
Epoch 59/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3065 - val_rounded_accuracy: 0.9271 - lr: 2.4000e-05
Epoch 60/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9270 - lr: 2.4000e-05
Epoch 61/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9269 - lr: 2.4000e-05
Epoch 62/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9268 - lr: 2.4000e-05
Epoch 63/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9268 - lr: 2.4000e-05
Epoch 64/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9268 - lr: 2.4000e-05
Epoch 65/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 2.4000e-05
Epoch 66/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9270 - lr: 4.8000e-06
Epoch 67/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3064 - val_rounded_accuracy: 0.9270 - lr: 4.8000e-06
Epoch 68/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3064 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 69/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3064 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 70/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 4.8000e-06
Epoch 71/100
860/860 [==============================] - 11s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 4.8000e-06
Epoch 72/100
860/860 [==============================] - 12s 13ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 73/100
860/860 [==============================] - 12s 14ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3065 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 74/100
860/860 [==============================] - 11s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9273 - lr: 4.8000e-06
Epoch 75/100
860/860 [==============================] - 11s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9269 - lr: 9.6000e-07
Epoch 76/100
860/860 [==============================] - 12s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 9.6000e-07
Epoch 77/100
860/860 [==============================] - 12s 14ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9272 - lr: 9.6000e-07
Epoch 78/100
860/860 [==============================] - 12s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9273 - lr: 9.6000e-07
Epoch 79/100
860/860 [==============================] - 12s 14ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3065 - val_rounded_accuracy: 0.9272 - lr: 9.6000e-07
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">variational_autoencoder.evaluate(X_valid, X_valid)</span><br></pre></td></tr></table></figure>
<pre><code>157/157 [==============================] - 1s 5ms/step - loss: 0.3066 - rounded_accuracy: 0.9273

[0.306575208902359, 0.9272765517234802]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.DataFrame(history.history).plot(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_24_0.png" alt="png"></p>
<h2 id="æ•ˆæœå±•ç¤º"><a href="#æ•ˆæœå±•ç¤º" class="headerlink" title="æ•ˆæœå±•ç¤º"></a>æ•ˆæœå±•ç¤º</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span>(<span class="params">image</span>):</span></span><br><span class="line">    plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_reconstructions</span>(<span class="params">model, images=X_valid, n_images=<span class="number">12</span></span>):</span></span><br><span class="line">    <span class="comment"># ä½¿ç”¨è‡ªç¼–ç å™¨è·å¾—é‡æ„å›¾åƒ</span></span><br><span class="line">    reconstructions = model.predict(images[:n_images])</span><br><span class="line">    <span class="comment"># ç»˜åˆ¶ &lt;åŸå›¾åƒ&gt; ä¸ &lt;é‡æ„å›¾åƒ&gt; ã® å¯¹æ¯”</span></span><br><span class="line">    fig = plt.figure(figsize=(n_images * <span class="number">1.5</span>, <span class="number">3</span>))</span><br><span class="line">    <span class="keyword">for</span> image_index <span class="keyword">in</span> <span class="built_in">range</span>(n_images):</span><br><span class="line">        plt.subplot(<span class="number">2</span>, n_images, <span class="number">1</span> + image_index)</span><br><span class="line">        plot_image(images[image_index])</span><br><span class="line">        plt.subplot(<span class="number">2</span>, n_images, <span class="number">1</span> + n_images + image_index)</span><br><span class="line">        plot_image(reconstructions[image_index])</span><br><span class="line"></span><br><span class="line">show_reconstructions(variational_autoencoder)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_26_0.png" alt="png"></p>
<h2 id="ç”Ÿæˆæ–°å®ä¾‹"><a href="#ç”Ÿæˆæ–°å®ä¾‹" class="headerlink" title="ç”Ÿæˆæ–°å®ä¾‹"></a>ç”Ÿæˆæ–°å®ä¾‹</h2><p>ä¸‹é¢ä½¿ç”¨åˆšæ‰è®­ç»ƒå¥½çš„å˜åˆ†è‡ªç¼–ç å™¨, å°è¯•ç”Ÿæˆä¸€äº›æ–°å®ä¾‹.</p>
<p>åªéœ€è¦ä»(å¯¹åº”)é«˜æ–¯åˆ†å¸ƒä¸­éšæœºé‡‡æ · codings å‘é‡, å†å°†å®ƒä»¬è§£ç å³å¯.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># éšæœºé‡‡æ · 12 ä¸ª codings å‘é‡å¹¶è§£ç </span></span><br><span class="line">codings = tf.random.normal(shape=[<span class="number">12</span>, codings_size])</span><br><span class="line">images = variational_decoder(codings).numpy()</span><br></pre></td></tr></table></figure>
<p>å±•ç¤ºç”Ÿæˆçš„æ–°å®ä¾‹.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiple_images</span>(<span class="params">images, n_cols=<span class="literal">None</span></span>):</span></span><br><span class="line">    n_cols = n_cols <span class="keyword">or</span> <span class="built_in">len</span>(images)</span><br><span class="line">    n_rows = (<span class="built_in">len</span>(images) - <span class="number">1</span>) // n_cols + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> images.shape[-<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># å¯¹ç°åº¦å›¾åƒ, å‹ç¼© channels ç»´åº¦</span></span><br><span class="line">        images = np.squeeze(images, axis=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    plt.figure(figsize=(<span class="number">2</span> * n_cols, <span class="number">2</span> * n_rows))</span><br><span class="line">    <span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        plt.subplot(n_rows, n_cols, index + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">plot_multiple_images(images, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_31_0.png" alt="png"></p>
<p>å¯ä»¥çœ‹å‡ºç”Ÿæˆçš„æ–°å®ä¾‹ä¸­å¤šæ•°ç¡®å®å½¢å¦‚ Fashion MNIST ä¸­çš„å›¾åƒ, ä½†æ˜¯å®ƒä»¬è¿˜æ˜¯æ¯”è¾ƒæ¨¡ç³Šçš„.</p>
<h2 id="è¯­ä¹‰æ’å€¼"><a href="#è¯­ä¹‰æ’å€¼" class="headerlink" title="è¯­ä¹‰æ’å€¼"></a>è¯­ä¹‰æ’å€¼</h2><p>ä¾é å˜åˆ†è‡ªç¼–ç å™¨, å¯<strong>åœ¨ codings çº§åˆ«è¿›è¡Œæ’å€¼</strong>, è€Œä¸æ˜¯åœ¨åƒç´ çº§åˆ«è¿›è¡Œæ’å€¼(è¿™å¯¼è‡´ç»“æœçœ‹èµ·æ¥åƒä¸¤å¼ å›¾è±¡é‡å åœ¨ä¸€èµ·).</p>
<p>åªéœ€è®©ä¸¤å¼ å›¾åƒé€šè¿‡ encoder, å¯¹æ‰€å¾— codings è¿›è¡Œæ’å€¼, å†å°†ç»“æœé€šè¿‡ decoder å³å¯å¾—åˆ°æœ€ç»ˆçš„æ’å€¼å›¾åƒ.</p>
<p>ä¸‹é¢é€šè¿‡å®è·µæ„Ÿå—ä¸€ä¸‹è¯­ä¹‰æ’å€¼çš„æ•ˆæœ.(å…¶ä¸­çš„ codings æ˜¯[9]ä¸­ç”Ÿæˆçš„æ–°å®ä¾‹çš„ codings)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.å¯¹ codings è¿›è¡Œæ’å€¼åè§£ç </span></span><br><span class="line">codings_grid = tf.reshape(codings, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, codings_size])</span><br><span class="line"><span class="comment"># è¿™é‡Œ tf.image.resize() é»˜è®¤æ‰§è¡ŒåŒçº¿æ€§æ’å€¼</span></span><br><span class="line">larger_grid = tf.image.resize(codings_grid, size=[<span class="number">5</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">interpolated_codings = tf.reshape(larger_grid, [-<span class="number">1</span>, codings_size])</span><br><span class="line">images = variational_decoder(interpolated_codings).numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.å±•ç¤ºè§£ç åçš„å›¾åƒ</span></span><br><span class="line">plt.figure(figsize=(<span class="number">7</span> * <span class="number">1.5</span>, <span class="number">5</span> * <span class="number">1.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">7</span>, index + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> index%<span class="number">7</span>%<span class="number">2</span>==<span class="number">0</span> <span class="keyword">and</span> index//<span class="number">7</span>%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        plt.gca().get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        plt.gca().get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_35_0.png" alt="png"></p>
<p>è§‚å¯Ÿç¬¬1è¡Œ, ç¬¬2åˆ—çš„é‹å­å›¾åƒ, å®ƒå¾ˆè‡ªç„¶åœ°è¿‡æ¸¡äºå…¶å·¦å³çš„ä¸¤å¼ é‹å­çš„å›¾åƒä¹‹é—´~</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>å„ç§å­¦ä¹ ç‡è°ƒåº¦ä¹‹é—´çš„å¯¹æ¯”</title>
    <url>/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>â“ å¦‚ä½•é€‰æ‹©ä½¿ç”¨ä»€ä¹ˆå­¦ä¹ ç‡è°ƒåº¦? é€‰é¡¹æœ‰: </p>
<ol>
<li>å¹‚è°ƒåº¦  </li>
<li>æŒ‡æ•°è°ƒåº¦</li>
<li>åˆ†æ®µå¸¸æ•°è°ƒåº¦</li>
<li>æ€§èƒ½è°ƒåº¦</li>
<li>1Cycle è°ƒåº¦</li>
</ol>
<p>å¦å¤–, å­¦ä¹ ç‡è°ƒåº¦æœ‰<strong>ä¸¤ç§ä¸åŒçš„å®ç°æ–¹å¼</strong>, åˆ†åˆ«å¯¹åº” &lt;åœ¨æ¯ä¸ª epoch å¼€å§‹å‰æ›´æ–°å­¦ä¹ ç‡&gt; å’Œ &lt;åœ¨æ¯ä¸ª step å¼€å§‹å‰æ›´æ–°å­¦ä¹ ç‡&gt;. å®ƒä»¬åœ¨æ€§èƒ½ä¸Šè¡¨ç°å¦‚ä½•?</p>
<span id="more"></span>
<p>â€» ä¸‹é¢é’ˆå¯¹ç›¸åŒçš„ &lt;ç½‘ç»œæ¶æ„ &amp; ä¼˜åŒ–å™¨&gt;, å¯¹æ¯”ä¸åŒå­¦ä¹ ç‡è°ƒåº¦çš„<strong>æ”¶æ•›é€Ÿåº¦</strong>å’Œ<strong>æ¨¡å‹æ€§èƒ½</strong>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>ğŸ”º é’ˆå¯¹ Fashion MNIST æ•°æ®é›†, å¼€å±•ä¸‹é¢çš„æµ‹è¯•.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å‡†å¤‡æ•°æ®é›† (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist ä¸­æ•°å­—æ ‡ç­¾å¯¹åº”çš„ç±»åˆ«åç§°</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å±•ç¤ºéƒ¨åˆ†è®­ç»ƒé›†å®ä¾‹</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m è¡Œ n åˆ—</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_6_0.png" alt="png"></p>
<h1 id="å®šä¹‰æ„å»ºæ¨¡å‹çš„å‡½æ•°"><a href="#å®šä¹‰æ„å»ºæ¨¡å‹çš„å‡½æ•°" class="headerlink" title="å®šä¹‰æ„å»ºæ¨¡å‹çš„å‡½æ•°"></a>å®šä¹‰æ„å»ºæ¨¡å‹çš„å‡½æ•°</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">optimizer, activation=<span class="string">&quot;elu&quot;</span>, n_layers=<span class="number">4</span>, n_neurons=<span class="number">80</span></span>):</span></span><br><span class="line">    model = keras.models.Sequential([</span><br><span class="line">        keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">        </span><br><span class="line">        keras.layers.Dense(<span class="number">400</span>, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">        </span><br><span class="line">        keras.layers.Dense(<span class="number">200</span>, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers):</span><br><span class="line">        model.add(keras.layers.Dense(n_neurons, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">        model.add(keras.layers.BatchNormalization())</span><br><span class="line">        </span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                  optimizer=optimizer, </span><br><span class="line">                  metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>Remark: åŸºäºè®­ç»ƒé€Ÿåº¦è€ƒè™‘, ä¸‹é¢é€‰ç”¨äº† batch_size=128 ä»¥åŠ é€Ÿè®­ç»ƒ, ä½†ä½¿ç”¨æ›´å°çš„ batch_size (å¦‚ 32) å¯èƒ½è·å¾—æ›´å¥½çš„æ¨¡å‹.</p>
<h1 id="å¹‚è°ƒåº¦"><a href="#å¹‚è°ƒåº¦" class="headerlink" title="å¹‚è°ƒåº¦"></a>å¹‚è°ƒåº¦</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰(å¸¦å¹‚è°ƒåº¦çš„)ä¼˜åŒ–å™¨, æ„å»ºæ¨¡å‹</span></span><br><span class="line"><span class="comment"># decay value è¡¨ç¤ºæ¯ 5000 ä¸ª step æ›´æ–°ä¸€æ¬¡ learning rate</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>, decay=<span class="number">1</span>/<span class="number">5000</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 13s 24ms/step - loss: 0.4903 - accuracy: 0.8215 - val_loss: 0.3833 - val_accuracy: 0.8706
Epoch 2/100
430/430 [==============================] - 10s 23ms/step - loss: 0.3652 - accuracy: 0.8666 - val_loss: 0.3747 - val_accuracy: 0.8706
Epoch 3/100
430/430 [==============================] - 10s 23ms/step - loss: 0.3305 - accuracy: 0.8774 - val_loss: 0.4111 - val_accuracy: 0.8530
Epoch 4/100
430/430 [==============================] - 10s 23ms/step - loss: 0.3070 - accuracy: 0.8880 - val_loss: 0.3452 - val_accuracy: 0.8750
Epoch 5/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2825 - accuracy: 0.8951 - val_loss: 0.3383 - val_accuracy: 0.8792
Epoch 6/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2699 - accuracy: 0.9001 - val_loss: 0.3114 - val_accuracy: 0.8920
Epoch 7/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2561 - accuracy: 0.9040 - val_loss: 0.3404 - val_accuracy: 0.8792
Epoch 8/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2477 - accuracy: 0.9082 - val_loss: 0.3086 - val_accuracy: 0.8876
Epoch 9/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2317 - accuracy: 0.9154 - val_loss: 0.3457 - val_accuracy: 0.8740
Epoch 10/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2214 - accuracy: 0.9187 - val_loss: 0.3439 - val_accuracy: 0.8930
Epoch 11/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2117 - accuracy: 0.9207 - val_loss: 0.3006 - val_accuracy: 0.8974
Epoch 12/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2027 - accuracy: 0.9248 - val_loss: 0.3517 - val_accuracy: 0.8904
Epoch 13/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1972 - accuracy: 0.9273 - val_loss: 0.3063 - val_accuracy: 0.8968
Epoch 14/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1875 - accuracy: 0.9302 - val_loss: 0.3425 - val_accuracy: 0.8902
Epoch 15/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1791 - accuracy: 0.9344 - val_loss: 0.3241 - val_accuracy: 0.8928
Epoch 16/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1743 - accuracy: 0.9356 - val_loss: 0.3675 - val_accuracy: 0.8822
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_12_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3343 - accuracy: 0.8893

[0.33429378271102905, 0.8892999887466431]
</code></pre><h1 id="æŒ‡æ•°è°ƒåº¦"><a href="#æŒ‡æ•°è°ƒåº¦" class="headerlink" title="æŒ‡æ•°è°ƒåº¦"></a>æŒ‡æ•°è°ƒåº¦</h1><h2 id="ç¬¬ä¸€ç§å®ç°"><a href="#ç¬¬ä¸€ç§å®ç°" class="headerlink" title="ç¬¬ä¸€ç§å®ç°"></a>ç¬¬ä¸€ç§å®ç°</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.å®šä¹‰æŒ‡æ•°è°ƒåº¦å‡½æ•°</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exp_decay</span>(<span class="params">lr_0, s</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">exp_decay_func</span>(<span class="params">epoch</span>):</span></span><br><span class="line">        <span class="keyword">return</span> lr_0 * <span class="number">0.1</span>**(epoch / s)    <span class="comment"># æ¯ s ä¸ª epoch æ›´æ–°ä¸€æ¬¡ learning rate</span></span><br><span class="line">    <span class="keyword">return</span> exp_decay_func</span><br><span class="line"></span><br><span class="line">exp_decay_func = exp_decay(lr_0=<span class="number">0.009</span>, s=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remark: ä»ä¸‹é¢è®­ç»ƒè¾“å‡ºæ¥çœ‹, å¹¶éæ¯ s ä¸ª epoch æ›´æ–°ä¸€æ¬¡ learning rate......</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.ä½¿ç”¨ä¸Šè¿°è°ƒåº¦å‡½æ•°åˆ›å»º LearningRateScheduler å›è°ƒ</span></span><br><span class="line">exp_lr_scheduler = keras.callbacks.LearningRateScheduler(exp_decay_func)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.å°†æ­¤å›è°ƒä¼ é€’ç»™ fit() ä¸­çš„ callbacks å‚æ•°</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, æ„å»ºæ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb, exp_lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 12s 23ms/step - loss: 0.4903 - accuracy: 0.8211 - val_loss: 0.3804 - val_accuracy: 0.8740 - lr: 0.0090
Epoch 2/100
430/430 [==============================] - 10s 23ms/step - loss: 0.3309 - accuracy: 0.8782 - val_loss: 0.3392 - val_accuracy: 0.8768 - lr: 0.0042
Epoch 3/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2749 - accuracy: 0.8970 - val_loss: 0.3342 - val_accuracy: 0.8770 - lr: 0.0019
Epoch 4/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2371 - accuracy: 0.9122 - val_loss: 0.2884 - val_accuracy: 0.8946 - lr: 9.0000e-04
Epoch 5/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2071 - accuracy: 0.9219 - val_loss: 0.2804 - val_accuracy: 0.9000 - lr: 4.1774e-04
Epoch 6/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1918 - accuracy: 0.9280 - val_loss: 0.2745 - val_accuracy: 0.8990 - lr: 1.9390e-04
Epoch 7/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1816 - accuracy: 0.9314 - val_loss: 0.2703 - val_accuracy: 0.9018 - lr: 9.0000e-05
Epoch 8/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1772 - accuracy: 0.9336 - val_loss: 0.2718 - val_accuracy: 0.9018 - lr: 4.1774e-05
Epoch 9/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1742 - accuracy: 0.9356 - val_loss: 0.2728 - val_accuracy: 0.9018 - lr: 1.9390e-05
Epoch 10/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1726 - accuracy: 0.9359 - val_loss: 0.2729 - val_accuracy: 0.9030 - lr: 9.0000e-06
Epoch 11/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1727 - accuracy: 0.9354 - val_loss: 0.2731 - val_accuracy: 0.9018 - lr: 4.1774e-06
Epoch 12/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1726 - accuracy: 0.9362 - val_loss: 0.2731 - val_accuracy: 0.9024 - lr: 1.9390e-06
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_18_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.2981 - accuracy: 0.8966

[0.29814115166664124, 0.8966000080108643]
</code></pre><h2 id="ç¬¬äºŒç§å®ç°"><a href="#ç¬¬äºŒç§å®ç°" class="headerlink" title="ç¬¬äºŒç§å®ç°"></a>ç¬¬äºŒç§å®ç°</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.ä½¿ç”¨ keras.optimizers.schedule ä¸­çš„è°ƒåº¦æ¥å®šä¹‰å­¦ä¹ ç‡</span></span><br><span class="line">s = <span class="number">15</span> * <span class="built_in">len</span>(x_train) // <span class="number">128</span>    </span><br><span class="line"><span class="comment"># s ä¸ºå‡è®¾è®­ç»ƒ 15 ä¸ªè½®æ¬¡ä¸” batch size=128 æ—¶æ€»çš„è®­ç»ƒ step æ•°</span></span><br><span class="line">learning_rate = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=<span class="number">0.009</span>,</span><br><span class="line">                                                            decay_steps=s, decay_rate=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>è¦ä½¿ç”¨æ­¤æ–¹æ³•å®ç°<strong>å…¶ä»–å­¦ä¹ ç‡è°ƒåº¦</strong>, å¯å‚è€ƒä¸‹é¢çš„é“¾æ¥:<br><a href="https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/schedules?hl=en">https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/schedules?hl=en</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.ä»¥ä¸Šè¿°å­¦ä¹ ç‡ä½œä¸ºä¼˜åŒ–å™¨å­¦ä¹ ç‡ (è¿™é‡Œä¸èƒ½ç”¨ Nadam...) </span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=learning_rate)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.ä½¿ç”¨ä¸Šè¿°ä¼˜åŒ–å™¨è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">model = build_model(optimizer)</span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 14ms/step - loss: 0.4937 - accuracy: 0.8215 - val_loss: 0.3949 - val_accuracy: 0.8542
Epoch 2/100
430/430 [==============================] - 5s 12ms/step - loss: 0.3724 - accuracy: 0.8629 - val_loss: 0.3517 - val_accuracy: 0.8770
Epoch 3/100
430/430 [==============================] - 5s 12ms/step - loss: 0.3303 - accuracy: 0.8783 - val_loss: 0.4212 - val_accuracy: 0.8522
Epoch 4/100
430/430 [==============================] - 5s 12ms/step - loss: 0.3029 - accuracy: 0.8887 - val_loss: 0.3395 - val_accuracy: 0.8770
Epoch 5/100
430/430 [==============================] - 5s 12ms/step - loss: 0.2692 - accuracy: 0.8999 - val_loss: 0.2835 - val_accuracy: 0.8914
Epoch 6/100
430/430 [==============================] - 5s 12ms/step - loss: 0.2477 - accuracy: 0.9080 - val_loss: 0.2935 - val_accuracy: 0.8916
Epoch 7/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2252 - accuracy: 0.9165 - val_loss: 0.3083 - val_accuracy: 0.8902
Epoch 8/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2067 - accuracy: 0.9232 - val_loss: 0.2852 - val_accuracy: 0.8958
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1877 - accuracy: 0.9306 - val_loss: 0.2797 - val_accuracy: 0.9030
Epoch 10/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1701 - accuracy: 0.9356 - val_loss: 0.3188 - val_accuracy: 0.8972
Epoch 11/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1559 - accuracy: 0.9411 - val_loss: 0.3083 - val_accuracy: 0.8968
Epoch 12/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1412 - accuracy: 0.9469 - val_loss: 0.3034 - val_accuracy: 0.8988
Epoch 13/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1277 - accuracy: 0.9523 - val_loss: 0.2980 - val_accuracy: 0.9078
Epoch 14/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1123 - accuracy: 0.9582 - val_loss: 0.3333 - val_accuracy: 0.8972
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_24_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3387 - accuracy: 0.8896

[0.3386974036693573, 0.8895999789237976]
</code></pre><h1 id="åˆ†æ®µå¸¸æ•°è°ƒåº¦"><a href="#åˆ†æ®µå¸¸æ•°è°ƒåº¦" class="headerlink" title="åˆ†æ®µå¸¸æ•°è°ƒåº¦"></a>åˆ†æ®µå¸¸æ•°è°ƒåº¦</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.å®šä¹‰åˆ†æ®µå¸¸æ•°è°ƒåº¦å‡½æ•°</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">piecewise_constant_func</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    <span class="keyword">if</span> epoch &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.009</span></span><br><span class="line">    <span class="keyword">elif</span> epoch &lt; <span class="number">6</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.003</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.001</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.ä½¿ç”¨ä¸Šè¿°è°ƒåº¦å‡½æ•°åˆ›å»º LearningRateScheduler å›è°ƒ</span></span><br><span class="line">piecewise_constant_lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_func)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.å°†æ­¤å›è°ƒä¼ é€’ç»™ fit() ä¸­çš„ callbacks å‚æ•°</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, æ„å»ºæ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb, piecewise_constant_lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 13s 24ms/step - loss: 0.4903 - accuracy: 0.8213 - val_loss: 0.3870 - val_accuracy: 0.8682 - lr: 0.0090
Epoch 2/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3643 - accuracy: 0.8672 - val_loss: 0.3671 - val_accuracy: 0.8720 - lr: 0.0090
Epoch 3/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3302 - accuracy: 0.8775 - val_loss: 0.4405 - val_accuracy: 0.8404 - lr: 0.0090
Epoch 4/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2599 - accuracy: 0.9039 - val_loss: 0.2952 - val_accuracy: 0.8934 - lr: 0.0030
Epoch 5/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2377 - accuracy: 0.9097 - val_loss: 0.2852 - val_accuracy: 0.8972 - lr: 0.0030
Epoch 6/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2253 - accuracy: 0.9159 - val_loss: 0.2886 - val_accuracy: 0.8970 - lr: 0.0030- ETA: 0s - loss: 0.2253 - accuracy
Epoch 7/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1828 - accuracy: 0.9320 - val_loss: 0.2748 - val_accuracy: 0.9040 - lr: 0.0010
Epoch 8/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1668 - accuracy: 0.9373 - val_loss: 0.2692 - val_accuracy: 0.9028 - lr: 0.0010
Epoch 9/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1561 - accuracy: 0.9424 - val_loss: 0.2827 - val_accuracy: 0.9022 - lr: 0.0010
Epoch 10/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1468 - accuracy: 0.9441 - val_loss: 0.3056 - val_accuracy: 0.9016 - lr: 0.0010
Epoch 11/100
430/430 [==============================] - 11s 24ms/step - loss: 0.1372 - accuracy: 0.9484 - val_loss: 0.3081 - val_accuracy: 0.8996 - lr: 0.0010
Epoch 12/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1287 - accuracy: 0.9518 - val_loss: 0.3078 - val_accuracy: 0.9030 - lr: 0.0010
Epoch 13/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1210 - accuracy: 0.9545 - val_loss: 0.3082 - val_accuracy: 0.9010 - lr: 0.0010
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_29_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3019 - accuracy: 0.8990: 0s - los

[0.30188095569610596, 0.8989999890327454]
</code></pre><h1 id="æ€§èƒ½è°ƒåº¦"><a href="#æ€§èƒ½è°ƒåº¦" class="headerlink" title="æ€§èƒ½è°ƒåº¦"></a>æ€§èƒ½è°ƒåº¦</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.åˆ›å»º ReduceLROnPlateau å›è°ƒ</span></span><br><span class="line">performance_lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">0.25</span>, patience=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.å°†æ­¤å›è°ƒä¼ é€’ç»™ fit() ä¸­çš„ callbacks å‚æ•°</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, æ„å»ºæ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">es_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[es_cb, performance_lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 13s 24ms/step - loss: 0.4903 - accuracy: 0.8213 - val_loss: 0.3849 - val_accuracy: 0.8700 - lr: 0.0090
Epoch 2/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3642 - accuracy: 0.8670 - val_loss: 0.3712 - val_accuracy: 0.8716 - lr: 0.0090
Epoch 3/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3305 - accuracy: 0.8787 - val_loss: 0.4373 - val_accuracy: 0.8462 - lr: 0.0090
Epoch 4/100
430/430 [==============================] - 10s 24ms/step - loss: 0.3077 - accuracy: 0.8865 - val_loss: 0.3519 - val_accuracy: 0.8670 - lr: 0.0090
Epoch 5/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2831 - accuracy: 0.8946 - val_loss: 0.3161 - val_accuracy: 0.8792 - lr: 0.0090
Epoch 6/100
430/430 [==============================] - 10s 24ms/step - loss: 0.2691 - accuracy: 0.9006 - val_loss: 0.3206 - val_accuracy: 0.8834 - lr: 0.0090
Epoch 7/100
430/430 [==============================] - 10s 23ms/step - loss: 0.2561 - accuracy: 0.9046 - val_loss: 0.3191 - val_accuracy: 0.8844 - lr: 0.0090
Epoch 8/100
430/430 [==============================] - 10s 23ms/step - loss: 0.1977 - accuracy: 0.9258 - val_loss: 0.2603 - val_accuracy: 0.9028 - lr: 0.0022
Epoch 9/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1764 - accuracy: 0.9343 - val_loss: 0.2731 - val_accuracy: 0.8996 - lr: 0.0022
Epoch 10/100
430/430 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.93 - 10s 24ms/step - loss: 0.1656 - accuracy: 0.9373 - val_loss: 0.2882 - val_accuracy: 0.9024 - lr: 0.0022
Epoch 11/100
430/430 [==============================] - 11s 24ms/step - loss: 0.1377 - accuracy: 0.9487 - val_loss: 0.2825 - val_accuracy: 0.9028 - lr: 5.6250e-04
Epoch 12/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1282 - accuracy: 0.9525 - val_loss: 0.2891 - val_accuracy: 0.9040 - lr: 5.6250e-04
Epoch 13/100
430/430 [==============================] - 10s 24ms/step - loss: 0.1192 - accuracy: 0.9559 - val_loss: 0.2879 - val_accuracy: 0.9088 - lr: 1.4062e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_33_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.2989 - accuracy: 0.8971

[0.2988533079624176, 0.8970999717712402]
</code></pre><h1 id="1Cycle-è°ƒåº¦"><a href="#1Cycle-è°ƒåº¦" class="headerlink" title="1Cycle è°ƒåº¦"></a>1Cycle è°ƒåº¦</h1><h2 id="å¯»æ‰¾æœ€ä½³å­¦ä¹ ç‡"><a href="#å¯»æ‰¾æœ€ä½³å­¦ä¹ ç‡" class="headerlink" title="å¯»æ‰¾æœ€ä½³å­¦ä¹ ç‡"></a>å¯»æ‰¾æœ€ä½³å­¦ä¹ ç‡</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">K = keras.backend</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExponentialLearningRate</span>(<span class="params">keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; ç»§æ‰¿ Callback ä»¥è‡ªå®šä¹‰å›è°ƒ &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, factor</span>):</span></span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.rates = []    </span><br><span class="line">        self.losses = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_batch_end</span>(<span class="params">self, batch, logs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; åœ¨æ¯ä¸ª step ç»“æŸæ—¶ï¼šè®°å½•å½“å‰å­¦ä¹ ç‡å’Œ loss å€¼å¹¶æ›´æ–°å½“å‰å­¦ä¹ ç‡ &quot;&quot;&quot;</span></span><br><span class="line">        self.rates.append(K.get_value(self.model.optimizer.lr))</span><br><span class="line">        self.losses.append(logs[<span class="string">&quot;loss&quot;</span>])</span><br><span class="line">        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_learning_rate</span>(<span class="params">model, X, y, epochs=<span class="number">1</span>, batch_size=<span class="number">32</span>, min_rate=<span class="number">10</span>**-<span class="number">5</span>, max_rate=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; è¯¥å‡½æ•°é€šè¿‡ä¸Šè¿°è‡ªå®šä¹‰å›è°ƒï¼Œè·å–è®­ç»ƒä¸­çš„å­¦ä¹ ç‡ä»¥åŠå¯¹åº”çš„ loss &quot;&quot;&quot;</span></span><br><span class="line">    init_weights = model.get_weights()</span><br><span class="line">    iterations = <span class="built_in">len</span>(X) // batch_size * epochs</span><br><span class="line">    factor = np.exp(np.log(max_rate / min_rate) / iterations)</span><br><span class="line">    init_lr = K.get_value(model.optimizer.lr)</span><br><span class="line">    K.set_value(model.optimizer.lr, min_rate)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># åˆ›å»ºå›è°ƒå®ä¾‹å¹¶è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">    exp_lr = ExponentialLearningRate(factor)</span><br><span class="line">    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,</span><br><span class="line">                        callbacks=[exp_lr])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># è®­ç»ƒå®Œæ¯•åï¼Œé‡ç½®å­¦ä¹ ç‡å’Œæ¨¡å‹çš„è¿æ¥æƒé‡</span></span><br><span class="line">    K.set_value(model.optimizer.lr, init_lr)</span><br><span class="line">    model.set_weights(init_weights)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> exp_lr.rates, exp_lr.losses</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_lr_vs_loss</span>(<span class="params">rates, losses</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; ç»˜åˆ¶ lr vs loss æ›²çº¿ &quot;&quot;&quot;</span></span><br><span class="line">    plt.plot(rates, losses)</span><br><span class="line">    plt.gca().set_xscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    plt.hlines(<span class="built_in">min</span>(losses), <span class="built_in">min</span>(rates), <span class="built_in">max</span>(rates))</span><br><span class="line">    plt.axis([<span class="built_in">min</span>(rates), <span class="built_in">max</span>(rates), <span class="built_in">min</span>(losses), (losses[<span class="number">0</span>] + <span class="built_in">min</span>(losses)) / <span class="number">2</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Learning rate&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, æ„å»ºæ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)    </span><br><span class="line">model = build_model(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒ 1 ä¸ªè½®æ¬¡å¹¶è·å– learning &amp; loss </span></span><br><span class="line">rates, losses = find_learning_rate(model, x_train, y_train, epochs=<span class="number">1</span>, batch_size=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ ¹æ® learning &amp; loss ä½œå›¾</span></span><br><span class="line">plot_lr_vs_loss(rates, losses)</span><br></pre></td></tr></table></figure>
<pre><code>430/430 [==============================] - 13s 25ms/step - loss: 2129.6172 - accuracy: 0.5144
</code></pre><p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_37_1.png" alt="png"></p>
<p>Remark: æ ¹æ®å®é™…æµ‹è¯•, ä»ä¸Šå›¾ä¸­é€‰å–æœ€ä½³å­¦ä¹ ç‡çš„æ•ˆæœå¹¶ä¸å¥½â€¦</p>
<h2 id="æ ¹æ®æ‰€é€‰å­¦ä¹ ç‡-ä½¿ç”¨-1cycle-è°ƒåº¦"><a href="#æ ¹æ®æ‰€é€‰å­¦ä¹ ç‡-ä½¿ç”¨-1cycle-è°ƒåº¦" class="headerlink" title="æ ¹æ®æ‰€é€‰å­¦ä¹ ç‡, ä½¿ç”¨ 1cycle è°ƒåº¦"></a>æ ¹æ®æ‰€é€‰å­¦ä¹ ç‡, ä½¿ç”¨ 1cycle è°ƒåº¦</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneCycleScheduler</span>(<span class="params">keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; ç»§æ‰¿ Callback ç±», å®ç° 1Cycle è°ƒåº¦ &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, iterations, max_rate, start_rate=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 last_iterations=<span class="literal">None</span>, last_rate=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.iterations = iterations    </span><br><span class="line">        self.max_rate = max_rate</span><br><span class="line">        self.start_rate = start_rate <span class="keyword">or</span> max_rate / <span class="number">10</span></span><br><span class="line">        self.last_iterations = last_iterations <span class="keyword">or</span> iterations // <span class="number">10</span> + <span class="number">1</span></span><br><span class="line">        self.half_iteration = (iterations - self.last_iterations) // <span class="number">2</span></span><br><span class="line">        self.last_rate = last_rate <span class="keyword">or</span> self.start_rate / <span class="number">1000</span></span><br><span class="line">        self.iteration = <span class="number">0</span> </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_interpolate</span>(<span class="params">self, iter1, iter2, rate1, rate2</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_batch_begin</span>(<span class="params">self, batch, logs</span>):</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.iteration &lt; self.half_iteration:          <span class="comment"># ç¬¬ä¸€æ®µçš„å­¦ä¹ ç‡</span></span><br><span class="line">            rate = self._interpolate(<span class="number">0</span>, self.half_iteration, self.start_rate, self.max_rate)</span><br><span class="line">        <span class="keyword">elif</span> self.iteration &lt; <span class="number">2</span> * self.half_iteration:    <span class="comment"># ç¬¬äºŒæ®µçš„å­¦ä¹ ç‡</span></span><br><span class="line">            rate = self._interpolate(self.half_iteration, <span class="number">2</span> * self.half_iteration, self.max_rate, self.start_rate)</span><br><span class="line">        <span class="keyword">else</span>:                                             <span class="comment"># æœ€åä¸€æ®µçš„å­¦ä¹ ç‡</span></span><br><span class="line">            rate = self._interpolate(<span class="number">2</span> * self.half_iteration, self.iterations, self.start_rate, self.last_rate)</span><br><span class="line">            rate = <span class="built_in">max</span>(rate, self.last_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># å½“å‰è¿­ä»£æ•°è‡ªå¢ 1ï¼ŒåŒæ—¶æ›´æ–°å­¦ä¹ ç‡</span></span><br><span class="line">        self.iteration += <span class="number">1</span></span><br><span class="line">        K.set_value(self.model.optimizer.lr, rate)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_epochs = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»º 1Cycle è°ƒåº¦å›è°ƒå®ä¾‹, å¹¶ä½¿ç”¨è¯¥å›è°ƒè®­ç»ƒæ¨¡å‹</span></span><br><span class="line">onecycle = OneCycleScheduler(<span class="built_in">len</span>(x_train) // batch_size * n_epochs, max_rate=<span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, epochs=n_epochs,</span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    callbacks=[onecycle])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
430/430 [==============================] - 11s 25ms/step - loss: 0.4601 - accuracy: 0.8402 - val_loss: 0.3513 - val_accuracy: 0.8760
Epoch 2/20
430/430 [==============================] - 11s 25ms/step - loss: 0.3500 - accuracy: 0.8732 - val_loss: 0.3418 - val_accuracy: 0.8762
Epoch 3/20
430/430 [==============================] - 11s 25ms/step - loss: 0.3218 - accuracy: 0.8815 - val_loss: 0.3950 - val_accuracy: 0.8578
Epoch 4/20
430/430 [==============================] - 11s 25ms/step - loss: 0.3073 - accuracy: 0.8866 - val_loss: 0.3414 - val_accuracy: 0.8740
Epoch 5/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2881 - accuracy: 0.8909 - val_loss: 0.2952 - val_accuracy: 0.8892
Epoch 6/20
430/430 [==============================] - 11s 26ms/step - loss: 0.2787 - accuracy: 0.8969 - val_loss: 0.3304 - val_accuracy: 0.8826
Epoch 7/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2713 - accuracy: 0.8986 - val_loss: 0.3386 - val_accuracy: 0.8776
Epoch 8/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2624 - accuracy: 0.9034 - val_loss: 0.3541 - val_accuracy: 0.8758
Epoch 9/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2537 - accuracy: 0.9058 - val_loss: 0.3528 - val_accuracy: 0.8760
Epoch 10/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2419 - accuracy: 0.9105 - val_loss: 0.3156 - val_accuracy: 0.8914
Epoch 11/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2232 - accuracy: 0.9164 - val_loss: 0.3197 - val_accuracy: 0.8896
Epoch 12/20
430/430 [==============================] - 11s 25ms/step - loss: 0.2038 - accuracy: 0.9248 - val_loss: 0.3303 - val_accuracy: 0.8864
Epoch 13/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1909 - accuracy: 0.9282 - val_loss: 0.2927 - val_accuracy: 0.8974
Epoch 14/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1696 - accuracy: 0.9369 - val_loss: 0.3014 - val_accuracy: 0.8966
Epoch 15/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1521 - accuracy: 0.9432 - val_loss: 0.2978 - val_accuracy: 0.8960
Epoch 16/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1325 - accuracy: 0.9504 - val_loss: 0.3239 - val_accuracy: 0.8954
Epoch 17/20
430/430 [==============================] - 11s 25ms/step - loss: 0.1079 - accuracy: 0.9596 - val_loss: 0.3080 - val_accuracy: 0.9064
Epoch 18/20
430/430 [==============================] - 11s 25ms/step - loss: 0.0824 - accuracy: 0.9700 - val_loss: 0.3283 - val_accuracy: 0.9054
Epoch 19/20
430/430 [==============================] - 11s 25ms/step - loss: 0.0620 - accuracy: 0.9776 - val_loss: 0.3511 - val_accuracy: 0.9090
Epoch 20/20
430/430 [==============================] - 11s 25ms/step - loss: 0.0505 - accuracy: 0.9823 - val_loss: 0.3590 - val_accuracy: 0.9068
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_42_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.4023 - accuracy: 0.9012

[0.4022747278213501, 0.901199996471405]
</code></pre><h1 id="ç»“è®º"><a href="#ç»“è®º" class="headerlink" title="ç»“è®º"></a>ç»“è®º</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Scheduler</th>
<th>Initial Learning Rate</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Evaluation on x_test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Power scheduling</td>
<td>0.009</td>
<td>10s/epoch</td>
<td>11 epochs</td>
<td>accuracy=0.8893</td>
</tr>
<tr>
<td>Exponential scheduling 1</td>
<td>0.009</td>
<td>10s/epoch</td>
<td>7 epochs</td>
<td>accuracy=0.8966</td>
</tr>
<tr>
<td>Exponential scheduling 2</td>
<td>0.009</td>
<td>6s/epoch</td>
<td>9 epochs</td>
<td>accuracy=0.8896</td>
</tr>
<tr>
<td>Piecewise constant scheduling</td>
<td>0.009</td>
<td>10s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8990</td>
</tr>
<tr>
<td>Performance scheduling</td>
<td>0.009</td>
<td>10s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8971</td>
</tr>
<tr>
<td>1cycle scheduling</td>
<td>0.001</td>
<td>11s/epoch</td>
<td>/</td>
<td>accuracy=0.9012</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>å¹‚è°ƒåº¦åœ¨å„æ–¹é¢éƒ½ä¸ä¼˜äºæŒ‡æ•°è°ƒåº¦.</li>
<li>æŒ‡æ•°è°ƒåº¦çš„ç¬¬1ç§å®ç°: è®­ç»ƒé€Ÿåº¦ä¸€èˆ¬, æ”¶æ•›é€Ÿåº¦è¾ƒå¿«, æ¨¡å‹æ€§èƒ½æ›´å¥½.</li>
<li>æŒ‡æ•°è°ƒåº¦çš„ç¬¬2ç§å®ç°: <strong>è®­ç»ƒé€Ÿåº¦å¿«</strong>, æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢, æ¨¡å‹æ€§èƒ½ç¨å·®.</li>
<li>åˆ†æ®µå¸¸æ•°è°ƒåº¦çš„è®­ç»ƒ &amp; æ”¶æ•›é€Ÿåº¦éƒ½ä¸é”™, æ¨¡å‹æ€§èƒ½ä¹Ÿè¾ƒå¥½, ä½†<strong>éš¾ä»¥é€‰æ‹©è¶…å‚æ•°</strong>çš„æœ€ä¼˜ç»„åˆ.</li>
<li>æ€§èƒ½è°ƒåº¦çš„è®­ç»ƒ &amp; æ”¶æ•›é€Ÿåº¦éƒ½ä¸é”™, æ¨¡å‹æ€§èƒ½ä¹Ÿè¾ƒå¥½, å¹¶ä¸”<strong>å®ç°éå¸¸ç®€å•</strong>, è¶…å‚æ•°é€‰æ‹©ä¹Ÿä¸å›°éš¾.</li>
<li>1Cycle è°ƒåº¦çš„å­¦ä¹ ç‡å‘ˆåˆ†æ®µçš„çº¿æ€§å‡½æ•°å˜åŒ–, <strong>éœ€è¦æå‰é¢„ä¼° epochs</strong>, å¹¶ä¸”è™½ç„¶è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½, ä½†ä»å…¶è®­ç»ƒè¾“å‡ºæ¥çœ‹, è¿™æ ·çš„ç»“æœä¼¼ä¹æœ‰ç‚¹å„¿éš¾ä»¥ç†è§£â€¦</li>
</ol>
<p>ğŸ’ å¯ä»¥æ”¹è¿›çš„ç‚¹: å›°éš¾è¿˜æ˜¯åœ¨äºéš¾ä»¥å¿«é€Ÿåœ°è·å–æœ€ä½³å­¦ä¹ ç‡â€¦[6.1]ä¸­çš„æ–¹æ³•ä¼¼ä¹ä¹Ÿä¸å¾ˆå¯é .</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>å„ç§ä¼˜åŒ–å™¨ä¹‹é—´çš„å¯¹æ¯”</title>
    <url>/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>â“ å¦‚ä½•é€‰æ‹©ä¼˜åŒ–å™¨? Momentum, Nesterov Momentum, RMSprop, Adam, Nadam?</p>
<span id="more"></span>
<p>â€» ä¸‹é¢åœ¨ç›¸åŒç½‘ç»œæ¶æ„ä¸Š, ä½¿ç”¨ä¸åŒä¼˜åŒ–å™¨, å¯¹æ¯”å®ƒä»¬çš„<strong>æ”¶æ•›é€Ÿåº¦</strong>å’Œ<strong>æ¨¡å‹æ€§èƒ½</strong>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>ğŸ”º é’ˆå¯¹ Fashion MNIST æ•°æ®é›†, å¼€å±•ä¸‹é¢çš„æµ‹è¯•.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å‡†å¤‡æ•°æ®é›† (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist ä¸­æ•°å­—æ ‡ç­¾å¯¹åº”çš„ç±»åˆ«åç§°</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å±•ç¤ºéƒ¨åˆ†è®­ç»ƒé›†å®ä¾‹</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m è¡Œ n åˆ—</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_6_0.png" alt="png"></p>
<h1 id="å®šä¹‰æ„å»º-amp-è®­ç»ƒæ¨¡å‹çš„å‡½æ•°"><a href="#å®šä¹‰æ„å»º-amp-è®­ç»ƒæ¨¡å‹çš„å‡½æ•°" class="headerlink" title="å®šä¹‰æ„å»º &amp; è®­ç»ƒæ¨¡å‹çš„å‡½æ•°"></a>å®šä¹‰æ„å»º &amp; è®­ç»ƒæ¨¡å‹çš„å‡½æ•°</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">optimizer, activation=<span class="string">&quot;elu&quot;</span>, n_layers=<span class="number">4</span>, n_neurons=<span class="number">100</span></span>):</span></span><br><span class="line">    <span class="comment"># 1.æ¨¡å‹æ„å»º</span></span><br><span class="line">    model = keras.models.Sequential([</span><br><span class="line">        keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">        </span><br><span class="line">        keras.layers.Dense(<span class="number">400</span>, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">        </span><br><span class="line">        keras.layers.Dense(<span class="number">200</span>, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">        keras.layers.BatchNormalization(),</span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers):</span><br><span class="line">        model.add(keras.layers.Dense(n_neurons, activation=activation, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">        model.add(keras.layers.BatchNormalization())</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    <span class="comment"># 2.æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                  optimizer=optimizer, </span><br><span class="line">                  metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">model, batch_size=<span class="number">128</span>, patience=<span class="number">5</span></span>):</span></span><br><span class="line">    <span class="comment"># é»˜è®¤ batch_size=128 ä»¥åŠ é€Ÿè®­ç»ƒ, è‹¥æ¨¡å‹æ€§èƒ½å˜å·®, å¯è®¾ç½® batch_size=32</span></span><br><span class="line">    early_stopping_cb = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">    history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                        validation_data=(x_valid, y_valid),</span><br><span class="line">                        batch_size=batch_size,</span><br><span class="line">                        callbacks=[early_stopping_cb])</span><br><span class="line">    <span class="keyword">return</span> history</span><br></pre></td></tr></table></figure>
<h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">model_with_momentum = build_model(momentum)</span><br><span class="line">history = train_model(model_with_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 13ms/step - loss: 0.6790 - accuracy: 0.7709 - val_loss: 0.4564 - val_accuracy: 0.8396
Epoch 2/100
430/430 [==============================] - 6s 15ms/step - loss: 0.4439 - accuracy: 0.8425 - val_loss: 0.4004 - val_accuracy: 0.8566
Epoch 3/100
430/430 [==============================] - 7s 15ms/step - loss: 0.3972 - accuracy: 0.8589 - val_loss: 0.3731 - val_accuracy: 0.8660
Epoch 4/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3691 - accuracy: 0.8685 - val_loss: 0.3591 - val_accuracy: 0.8722
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3443 - accuracy: 0.8772 - val_loss: 0.3512 - val_accuracy: 0.8768
Epoch 6/100
430/430 [==============================] - 6s 13ms/step - loss: 0.3311 - accuracy: 0.8793 - val_loss: 0.3423 - val_accuracy: 0.8766
Epoch 7/100
430/430 [==============================] - 6s 13ms/step - loss: 0.3154 - accuracy: 0.8867 - val_loss: 0.3334 - val_accuracy: 0.8812
Epoch 8/100
430/430 [==============================] - 7s 15ms/step - loss: 0.3037 - accuracy: 0.8893 - val_loss: 0.3271 - val_accuracy: 0.8820
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2911 - accuracy: 0.8926 - val_loss: 0.3231 - val_accuracy: 0.8812
Epoch 10/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2805 - accuracy: 0.8977 - val_loss: 0.3231 - val_accuracy: 0.8800
Epoch 11/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2707 - accuracy: 0.9016 - val_loss: 0.3206 - val_accuracy: 0.8824
Epoch 12/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2627 - accuracy: 0.9039 - val_loss: 0.3182 - val_accuracy: 0.8822
Epoch 13/100
430/430 [==============================] - 6s 13ms/step - loss: 0.2552 - accuracy: 0.9067 - val_loss: 0.3145 - val_accuracy: 0.8840
Epoch 14/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2464 - accuracy: 0.9097 - val_loss: 0.3167 - val_accuracy: 0.8868
Epoch 15/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2374 - accuracy: 0.9141 - val_loss: 0.3174 - val_accuracy: 0.8834
Epoch 16/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2329 - accuracy: 0.9149 - val_loss: 0.3188 - val_accuracy: 0.8856
Epoch 17/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2242 - accuracy: 0.9187 - val_loss: 0.3166 - val_accuracy: 0.8842
Epoch 18/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2188 - accuracy: 0.9204 - val_loss: 0.3149 - val_accuracy: 0.8872
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_11_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3419 - accuracy: 0.8763

[0.34192100167274475, 0.8762999773025513]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.003</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">model_with_momentum = build_model(momentum)</span><br><span class="line">history = train_model(model_with_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 9s 17ms/step - loss: 0.5472 - accuracy: 0.8101 - val_loss: 0.3869 - val_accuracy: 0.8626
Epoch 2/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3755 - accuracy: 0.8643 - val_loss: 0.3510 - val_accuracy: 0.8716
Epoch 3/100
430/430 [==============================] - 7s 17ms/step - loss: 0.3345 - accuracy: 0.8773 - val_loss: 0.3355 - val_accuracy: 0.8802
Epoch 4/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3073 - accuracy: 0.8879 - val_loss: 0.3271 - val_accuracy: 0.8786
Epoch 5/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2814 - accuracy: 0.8966 - val_loss: 0.3202 - val_accuracy: 0.8868
Epoch 6/100
430/430 [==============================] - 8s 20ms/step - loss: 0.2678 - accuracy: 0.9011 - val_loss: 0.3181 - val_accuracy: 0.8812
Epoch 7/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2494 - accuracy: 0.9088 - val_loss: 0.3124 - val_accuracy: 0.8866
Epoch 8/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2354 - accuracy: 0.9132 - val_loss: 0.3114 - val_accuracy: 0.8876
Epoch 9/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2205 - accuracy: 0.9174 - val_loss: 0.3042 - val_accuracy: 0.8892
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2089 - accuracy: 0.9223 - val_loss: 0.3146 - val_accuracy: 0.8898
Epoch 11/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1996 - accuracy: 0.9258 - val_loss: 0.3133 - val_accuracy: 0.8910
Epoch 12/100
430/430 [==============================] - 7s 16ms/step - loss: 0.1881 - accuracy: 0.9310 - val_loss: 0.3129 - val_accuracy: 0.8906
Epoch 13/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1803 - accuracy: 0.9340 - val_loss: 0.3143 - val_accuracy: 0.8912
Epoch 14/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1695 - accuracy: 0.9373 - val_loss: 0.3307 - val_accuracy: 0.8896
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_13_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3337 - accuracy: 0.8836

[0.3336751461029053, 0.8835999965667725]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.009</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">model_with_momentum = build_model(momentum)</span><br><span class="line">history = train_model(model_with_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 8s 16ms/step - loss: 0.4763 - accuracy: 0.8306 - val_loss: 0.3680 - val_accuracy: 0.8668
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3401 - accuracy: 0.8759 - val_loss: 0.3340 - val_accuracy: 0.8764
Epoch 3/100
430/430 [==============================] - 6s 13ms/step - loss: 0.3024 - accuracy: 0.8875 - val_loss: 0.3413 - val_accuracy: 0.8802
Epoch 4/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2754 - accuracy: 0.8982 - val_loss: 0.3193 - val_accuracy: 0.8828
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2497 - accuracy: 0.9059 - val_loss: 0.3031 - val_accuracy: 0.8896
Epoch 6/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2333 - accuracy: 0.9141 - val_loss: 0.3073 - val_accuracy: 0.8860
Epoch 7/100
430/430 [==============================] - 5s 12ms/step - loss: 0.2164 - accuracy: 0.9181 - val_loss: 0.3164 - val_accuracy: 0.8874
Epoch 8/100
430/430 [==============================] - 6s 13ms/step - loss: 0.2020 - accuracy: 0.9241 - val_loss: 0.3006 - val_accuracy: 0.8922
Epoch 9/100
430/430 [==============================] - 7s 15ms/step - loss: 0.1873 - accuracy: 0.9299 - val_loss: 0.3126 - val_accuracy: 0.8906
Epoch 10/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1764 - accuracy: 0.9322 - val_loss: 0.3361 - val_accuracy: 0.8880
Epoch 11/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1689 - accuracy: 0.9363 - val_loss: 0.3185 - val_accuracy: 0.8922
Epoch 12/100
430/430 [==============================] - 7s 16ms/step - loss: 0.1582 - accuracy: 0.9418 - val_loss: 0.3250 - val_accuracy: 0.8908
Epoch 13/100
430/430 [==============================] - 7s 16ms/step - loss: 0.1489 - accuracy: 0.9450 - val_loss: 0.3271 - val_accuracy: 0.8936
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_15_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3313 - accuracy: 0.8851

[0.3312726616859436, 0.8851000070571899]
</code></pre><h1 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">nesterov_momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line">model_with_nesterov_momentum = build_model(nesterov_momentum)</span><br><span class="line">history = train_model(model_with_nesterov_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 8s 16ms/step - loss: 0.6741 - accuracy: 0.7723 - val_loss: 0.4556 - val_accuracy: 0.8396
Epoch 2/100
430/430 [==============================] - 7s 16ms/step - loss: 0.4430 - accuracy: 0.8431 - val_loss: 0.3998 - val_accuracy: 0.8572
Epoch 3/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3965 - accuracy: 0.8588 - val_loss: 0.3732 - val_accuracy: 0.8666
Epoch 4/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3683 - accuracy: 0.8685 - val_loss: 0.3587 - val_accuracy: 0.8720
Epoch 5/100
430/430 [==============================] - 8s 18ms/step - loss: 0.3436 - accuracy: 0.8774 - val_loss: 0.3504 - val_accuracy: 0.8762
Epoch 6/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3303 - accuracy: 0.8792 - val_loss: 0.3417 - val_accuracy: 0.8774
Epoch 7/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3147 - accuracy: 0.8866 - val_loss: 0.3326 - val_accuracy: 0.8810
Epoch 8/100
430/430 [==============================] - 7s 16ms/step - loss: 0.3030 - accuracy: 0.8896 - val_loss: 0.3270 - val_accuracy: 0.8808
Epoch 9/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2903 - accuracy: 0.8928 - val_loss: 0.3225 - val_accuracy: 0.8818
Epoch 10/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2798 - accuracy: 0.8981 - val_loss: 0.3220 - val_accuracy: 0.8806
Epoch 11/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2701 - accuracy: 0.9021 - val_loss: 0.3202 - val_accuracy: 0.8820
Epoch 12/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2618 - accuracy: 0.9040 - val_loss: 0.3179 - val_accuracy: 0.8822
Epoch 13/100
430/430 [==============================] - 6s 13ms/step - loss: 0.2544 - accuracy: 0.9069 - val_loss: 0.3141 - val_accuracy: 0.8836
Epoch 14/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2456 - accuracy: 0.9099 - val_loss: 0.3166 - val_accuracy: 0.8866
Epoch 15/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2366 - accuracy: 0.9145 - val_loss: 0.3172 - val_accuracy: 0.8844
Epoch 16/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2320 - accuracy: 0.9152 - val_loss: 0.3188 - val_accuracy: 0.8852
Epoch 17/100
430/430 [==============================] - 6s 13ms/step - loss: 0.2233 - accuracy: 0.9190 - val_loss: 0.3167 - val_accuracy: 0.8852
Epoch 18/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2177 - accuracy: 0.9209 - val_loss: 0.3146 - val_accuracy: 0.8870
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_nesterov_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_18_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 6ms/step - loss: 0.3416 - accuracy: 0.8767

[0.34159716963768005, 0.8766999840736389]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">nesterov_momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.003</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line">model_with_nesterov_momentum = build_model(nesterov_momentum)</span><br><span class="line">history = train_model(model_with_nesterov_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 8s 15ms/step - loss: 0.5403 - accuracy: 0.8117 - val_loss: 0.3853 - val_accuracy: 0.8630
Epoch 2/100
430/430 [==============================] - 6s 15ms/step - loss: 0.3730 - accuracy: 0.8651 - val_loss: 0.3493 - val_accuracy: 0.8736
Epoch 3/100
430/430 [==============================] - 7s 15ms/step - loss: 0.3319 - accuracy: 0.8785 - val_loss: 0.3349 - val_accuracy: 0.8796
Epoch 4/100
430/430 [==============================] - 8s 19ms/step - loss: 0.3043 - accuracy: 0.8888 - val_loss: 0.3262 - val_accuracy: 0.8786
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2790 - accuracy: 0.8977 - val_loss: 0.3171 - val_accuracy: 0.8876
Epoch 6/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2642 - accuracy: 0.9030 - val_loss: 0.3163 - val_accuracy: 0.8830
Epoch 7/100
430/430 [==============================] - 7s 17ms/step - loss: 0.2465 - accuracy: 0.9098 - val_loss: 0.3131 - val_accuracy: 0.8856
Epoch 8/100
430/430 [==============================] - 8s 20ms/step - loss: 0.2321 - accuracy: 0.9139 - val_loss: 0.3107 - val_accuracy: 0.8886
Epoch 9/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2171 - accuracy: 0.9194 - val_loss: 0.3058 - val_accuracy: 0.8898
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2052 - accuracy: 0.9235 - val_loss: 0.3140 - val_accuracy: 0.8894
Epoch 11/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1953 - accuracy: 0.9275 - val_loss: 0.3126 - val_accuracy: 0.8906
Epoch 12/100
430/430 [==============================] - 8s 19ms/step - loss: 0.1836 - accuracy: 0.9327 - val_loss: 0.3174 - val_accuracy: 0.8902
Epoch 13/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1764 - accuracy: 0.9350 - val_loss: 0.3170 - val_accuracy: 0.8896
Epoch 14/100
430/430 [==============================] - 6s 13ms/step - loss: 0.1651 - accuracy: 0.9396 - val_loss: 0.3289 - val_accuracy: 0.8906
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_nesterov_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_20_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3359 - accuracy: 0.8837

[0.3358677625656128, 0.8837000131607056]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">nesterov_momentum = keras.optimizers.SGD(learning_rate=<span class="number">0.009</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line">model_with_nesterov_momentum = build_model(nesterov_momentum)</span><br><span class="line">history = train_model(model_with_nesterov_momentum)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 10s 19ms/step - loss: 0.4669 - accuracy: 0.8332 - val_loss: 0.3616 - val_accuracy: 0.8674
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3346 - accuracy: 0.8781 - val_loss: 0.3282 - val_accuracy: 0.8820
Epoch 3/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2964 - accuracy: 0.8901 - val_loss: 0.3420 - val_accuracy: 0.8800
Epoch 4/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2682 - accuracy: 0.9017 - val_loss: 0.3156 - val_accuracy: 0.8828
Epoch 5/100
430/430 [==============================] - 9s 21ms/step - loss: 0.2442 - accuracy: 0.9083 - val_loss: 0.2981 - val_accuracy: 0.8892
Epoch 6/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2266 - accuracy: 0.9159 - val_loss: 0.3098 - val_accuracy: 0.8854
Epoch 7/100
430/430 [==============================] - 7s 17ms/step - loss: 0.2102 - accuracy: 0.9207 - val_loss: 0.3190 - val_accuracy: 0.8914
Epoch 8/100
430/430 [==============================] - 8s 18ms/step - loss: 0.1951 - accuracy: 0.9265 - val_loss: 0.3074 - val_accuracy: 0.8900
Epoch 9/100
430/430 [==============================] - 6s 15ms/step - loss: 0.1807 - accuracy: 0.9332 - val_loss: 0.3180 - val_accuracy: 0.8928
Epoch 10/100
430/430 [==============================] - 9s 20ms/step - loss: 0.1696 - accuracy: 0.9360 - val_loss: 0.3292 - val_accuracy: 0.8926
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_nesterov_momentum.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_22_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3270 - accuracy: 0.8803

[0.3269873261451721, 0.880299985408783]
</code></pre><h1 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_rmsprop = build_model(rmsprop)</span><br><span class="line">history = train_model(model_with_rmsprop)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 11s 20ms/step - loss: 0.4623 - accuracy: 0.8330 - val_loss: 0.3530 - val_accuracy: 0.8734
Epoch 2/100
430/430 [==============================] - 8s 19ms/step - loss: 0.3422 - accuracy: 0.8730 - val_loss: 0.3366 - val_accuracy: 0.8786
Epoch 3/100
430/430 [==============================] - 9s 20ms/step - loss: 0.3019 - accuracy: 0.8869 - val_loss: 0.3575 - val_accuracy: 0.8690
Epoch 4/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2734 - accuracy: 0.8982 - val_loss: 0.3104 - val_accuracy: 0.8848
Epoch 5/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2479 - accuracy: 0.9058 - val_loss: 0.2948 - val_accuracy: 0.8924
Epoch 6/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2303 - accuracy: 0.9139 - val_loss: 0.3111 - val_accuracy: 0.8890
Epoch 7/100
430/430 [==============================] - 9s 21ms/step - loss: 0.2133 - accuracy: 0.9207 - val_loss: 0.3307 - val_accuracy: 0.8870
Epoch 8/100
430/430 [==============================] - 9s 20ms/step - loss: 0.1970 - accuracy: 0.9259 - val_loss: 0.3165 - val_accuracy: 0.8876
Epoch 9/100
430/430 [==============================] - 9s 20ms/step - loss: 0.1806 - accuracy: 0.9318 - val_loss: 0.3165 - val_accuracy: 0.8942
Epoch 10/100
430/430 [==============================] - 8s 19ms/step - loss: 0.1691 - accuracy: 0.9363 - val_loss: 0.3131 - val_accuracy: 0.9012
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_rmsprop.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_25_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3321 - accuracy: 0.8803

[0.332133412361145, 0.880299985408783]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_rmsprop = build_model(rmsprop)</span><br><span class="line">history = train_model(model_with_rmsprop)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 10s 20ms/step - loss: 0.5125 - accuracy: 0.8139 - val_loss: 0.4109 - val_accuracy: 0.8638 - loss:
Epoch 2/100
430/430 [==============================] - 8s 19ms/step - loss: 0.3701 - accuracy: 0.8647 - val_loss: 0.3595 - val_accuracy: 0.8742
Epoch 3/100
430/430 [==============================] - 8s 20ms/step - loss: 0.3252 - accuracy: 0.8812 - val_loss: 0.3858 - val_accuracy: 0.8660
Epoch 4/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2951 - accuracy: 0.8920 - val_loss: 0.3519 - val_accuracy: 0.8740
Epoch 5/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2694 - accuracy: 0.9001 - val_loss: 0.3025 - val_accuracy: 0.8908
Epoch 6/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2508 - accuracy: 0.9078 - val_loss: 0.3159 - val_accuracy: 0.8904
Epoch 7/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2349 - accuracy: 0.9139 - val_loss: 0.3265 - val_accuracy: 0.8846
Epoch 8/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2176 - accuracy: 0.9212 - val_loss: 0.3428 - val_accuracy: 0.8858
Epoch 9/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2043 - accuracy: 0.9249 - val_loss: 0.3161 - val_accuracy: 0.8964
Epoch 10/100
430/430 [==============================] - 8s 19ms/step - loss: 0.1905 - accuracy: 0.9296 - val_loss: 0.3248 - val_accuracy: 0.8964
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_rmsprop.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_27_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3396 - accuracy: 0.8794

[0.33956459164619446, 0.8794000148773193]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">0.009</span>)</span><br><span class="line">model_with_rmsprop = build_model(rmsprop)</span><br><span class="line">history = train_model(model_with_rmsprop)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 10s 20ms/step - loss: 0.5813 - accuracy: 0.7944 - val_loss: 0.4772 - val_accuracy: 0.8612
Epoch 2/100
430/430 [==============================] - 9s 20ms/step - loss: 0.3879 - accuracy: 0.8614 - val_loss: 0.4010 - val_accuracy: 0.8702
Epoch 3/100
430/430 [==============================] - 9s 20ms/step - loss: 0.3422 - accuracy: 0.8759 - val_loss: 0.4354 - val_accuracy: 0.8574
Epoch 4/100
430/430 [==============================] - 8s 19ms/step - loss: 0.3121 - accuracy: 0.8870 - val_loss: 0.3847 - val_accuracy: 0.8690
Epoch 5/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2860 - accuracy: 0.8958 - val_loss: 0.3343 - val_accuracy: 0.8824
Epoch 6/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2694 - accuracy: 0.9021 - val_loss: 0.3159 - val_accuracy: 0.8862
Epoch 7/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2546 - accuracy: 0.9061 - val_loss: 0.3193 - val_accuracy: 0.8884
Epoch 8/100
430/430 [==============================] - 8s 20ms/step - loss: 0.2413 - accuracy: 0.9114 - val_loss: 0.3222 - val_accuracy: 0.8860
Epoch 9/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2282 - accuracy: 0.9165 - val_loss: 0.3308 - val_accuracy: 0.8856
Epoch 10/100
430/430 [==============================] - 8s 19ms/step - loss: 0.2149 - accuracy: 0.9213 - val_loss: 0.3389 - val_accuracy: 0.8920
Epoch 11/100
430/430 [==============================] - 9s 20ms/step - loss: 0.2079 - accuracy: 0.9230 - val_loss: 0.3290 - val_accuracy: 0.8928
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_rmsprop.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_29_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3519 - accuracy: 0.8817

[0.3518962860107422, 0.8816999793052673]
</code></pre><h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">adam = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_adam = build_model(adam)</span><br><span class="line">history = train_model(model_with_adam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 14ms/step - loss: 0.4501 - accuracy: 0.8382 - val_loss: 0.3560 - val_accuracy: 0.8696
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3363 - accuracy: 0.8768 - val_loss: 0.3209 - val_accuracy: 0.8856
Epoch 3/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2979 - accuracy: 0.8889 - val_loss: 0.3466 - val_accuracy: 0.8708
Epoch 4/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2736 - accuracy: 0.8976 - val_loss: 0.3208 - val_accuracy: 0.8808
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2500 - accuracy: 0.9059 - val_loss: 0.2769 - val_accuracy: 0.8962
Epoch 6/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2352 - accuracy: 0.9120 - val_loss: 0.2921 - val_accuracy: 0.8916
Epoch 7/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2159 - accuracy: 0.9192 - val_loss: 0.3060 - val_accuracy: 0.8930
Epoch 8/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2000 - accuracy: 0.9256 - val_loss: 0.3018 - val_accuracy: 0.8926
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1862 - accuracy: 0.9296 - val_loss: 0.2977 - val_accuracy: 0.8950
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1744 - accuracy: 0.9339 - val_loss: 0.3132 - val_accuracy: 0.8966
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_adam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_32_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3160 - accuracy: 0.8846

[0.31602275371551514, 0.8845999836921692]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">adam = keras.optimizers.Adam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_adam = build_model(adam)</span><br><span class="line">history = train_model(model_with_adam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 14ms/step - loss: 0.4631 - accuracy: 0.8328 - val_loss: 0.3626 - val_accuracy: 0.8640
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3586 - accuracy: 0.8675 - val_loss: 0.3419 - val_accuracy: 0.8762
Epoch 3/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3208 - accuracy: 0.8804 - val_loss: 0.3707 - val_accuracy: 0.8658
Epoch 4/100
430/430 [==============================] - 6s 15ms/step - loss: 0.2975 - accuracy: 0.8901 - val_loss: 0.3285 - val_accuracy: 0.8802
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2699 - accuracy: 0.8980 - val_loss: 0.2971 - val_accuracy: 0.8898
Epoch 6/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2563 - accuracy: 0.9045 - val_loss: 0.3174 - val_accuracy: 0.8864
Epoch 7/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2437 - accuracy: 0.9086 - val_loss: 0.3319 - val_accuracy: 0.8888
Epoch 8/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2282 - accuracy: 0.9155 - val_loss: 0.2966 - val_accuracy: 0.8944
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2144 - accuracy: 0.9193 - val_loss: 0.3209 - val_accuracy: 0.8946
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2017 - accuracy: 0.9242 - val_loss: 0.3258 - val_accuracy: 0.8946
Epoch 11/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1937 - accuracy: 0.9283 - val_loss: 0.3211 - val_accuracy: 0.8926
Epoch 12/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1821 - accuracy: 0.9323 - val_loss: 0.3170 - val_accuracy: 0.8936
Epoch 13/100
430/430 [==============================] - 6s 14ms/step - loss: 0.1746 - accuracy: 0.9346 - val_loss: 0.3107 - val_accuracy: 0.9008
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_adam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_34_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8778

[0.33767154812812805, 0.8777999877929688]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">adam = keras.optimizers.Adam(learning_rate=<span class="number">0.009</span>)</span><br><span class="line">model_with_adam = build_model(adam)</span><br><span class="line">history = train_model(model_with_adam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 7s 14ms/step - loss: 0.4959 - accuracy: 0.8225 - val_loss: 0.4709 - val_accuracy: 0.8394
Epoch 2/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3831 - accuracy: 0.8603 - val_loss: 0.3787 - val_accuracy: 0.8720
Epoch 3/100
430/430 [==============================] - 6s 14ms/step - loss: 0.3473 - accuracy: 0.8739 - val_loss: 0.4398 - val_accuracy: 0.8482
Epoch 4/100
430/430 [==============================] - 6s 13ms/step - loss: 0.3249 - accuracy: 0.8811 - val_loss: 0.3315 - val_accuracy: 0.8800
Epoch 5/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2948 - accuracy: 0.8922 - val_loss: 0.3096 - val_accuracy: 0.8852
Epoch 6/100
430/430 [==============================] - 7s 16ms/step - loss: 0.2874 - accuracy: 0.8955 - val_loss: 0.3054 - val_accuracy: 0.8894
Epoch 7/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2701 - accuracy: 0.9006 - val_loss: 0.3290 - val_accuracy: 0.8862
Epoch 8/100
430/430 [==============================] - 7s 15ms/step - loss: 0.2578 - accuracy: 0.9050 - val_loss: 0.2897 - val_accuracy: 0.8884
Epoch 9/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2432 - accuracy: 0.9101 - val_loss: 0.3229 - val_accuracy: 0.8896
Epoch 10/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2356 - accuracy: 0.9138 - val_loss: 0.3196 - val_accuracy: 0.8868
Epoch 11/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2280 - accuracy: 0.9151 - val_loss: 0.3303 - val_accuracy: 0.8858
Epoch 12/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2181 - accuracy: 0.9202 - val_loss: 0.3581 - val_accuracy: 0.8856
Epoch 13/100
430/430 [==============================] - 6s 14ms/step - loss: 0.2088 - accuracy: 0.9235 - val_loss: 0.3011 - val_accuracy: 0.9010
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_adam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_36_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.8798

[0.3372490406036377, 0.879800021648407]
</code></pre><h1 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">nadam = keras.optimizers.Nadam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_with_nadam = build_model(nadam)</span><br><span class="line">history = train_model(model_with_nadam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 14s 27ms/step - loss: 0.4456 - accuracy: 0.8412 - val_loss: 0.3369 - val_accuracy: 0.8772
Epoch 2/100
430/430 [==============================] - 11s 27ms/step - loss: 0.3263 - accuracy: 0.8799 - val_loss: 0.3352 - val_accuracy: 0.8774oss: 0.3271 - accuracy: 0.87 - ETA: 0s - loss: 0
Epoch 3/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2903 - accuracy: 0.8909 - val_loss: 0.3364 - val_accuracy: 0.8744
Epoch 4/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2631 - accuracy: 0.9024 - val_loss: 0.3149 - val_accuracy: 0.8882
Epoch 5/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2388 - accuracy: 0.9103 - val_loss: 0.2889 - val_accuracy: 0.8912
Epoch 6/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2219 - accuracy: 0.9165 - val_loss: 0.3054 - val_accuracy: 0.8888
Epoch 7/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2070 - accuracy: 0.9219 - val_loss: 0.3177 - val_accuracy: 0.8852
Epoch 8/100
430/430 [==============================] - 11s 26ms/step - loss: 0.1913 - accuracy: 0.9279 - val_loss: 0.3169 - val_accuracy: 0.8878
Epoch 9/100
430/430 [==============================] - 11s 26ms/step - loss: 0.1759 - accuracy: 0.9336 - val_loss: 0.3141 - val_accuracy: 0.8952
Epoch 10/100
430/430 [==============================] - 11s 26ms/step - loss: 0.1635 - accuracy: 0.9376 - val_loss: 0.3068 - val_accuracy: 0.8994
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_nadam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_39_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8826

[0.3237777650356293, 0.8826000094413757]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">nadam = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_nadam = build_model(nadam)</span><br><span class="line">history = train_model(model_with_nadam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 14s 27ms/step - loss: 0.4557 - accuracy: 0.8346 - val_loss: 0.3408 - val_accuracy: 0.8752
Epoch 2/100
430/430 [==============================] - 11s 26ms/step - loss: 0.3419 - accuracy: 0.8734 - val_loss: 0.3310 - val_accuracy: 0.8770
Epoch 3/100
430/430 [==============================] - 11s 26ms/step - loss: 0.3072 - accuracy: 0.8852 - val_loss: 0.3605 - val_accuracy: 0.8708
Epoch 4/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2821 - accuracy: 0.8950 - val_loss: 0.3365 - val_accuracy: 0.8752
Epoch 5/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2581 - accuracy: 0.9026 - val_loss: 0.2931 - val_accuracy: 0.8932
Epoch 6/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2437 - accuracy: 0.9096 - val_loss: 0.3218 - val_accuracy: 0.8860
Epoch 7/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2304 - accuracy: 0.9137 - val_loss: 0.3155 - val_accuracy: 0.8890
Epoch 8/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2170 - accuracy: 0.9187 - val_loss: 0.3146 - val_accuracy: 0.8848
Epoch 9/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2012 - accuracy: 0.9241 - val_loss: 0.3021 - val_accuracy: 0.8976
Epoch 10/100
430/430 [==============================] - 11s 26ms/step - loss: 0.1881 - accuracy: 0.9287 - val_loss: 0.3103 - val_accuracy: 0.8944
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_nadam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_41_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 5ms/step - loss: 0.3319 - accuracy: 0.8797

[0.3318653404712677, 0.8797000050544739]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¼˜åŒ–å™¨, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">nadam = keras.optimizers.Nadam(learning_rate=<span class="number">0.009</span>)</span><br><span class="line">model_with_nadam = build_model(nadam)</span><br><span class="line">history = train_model(model_with_nadam)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
430/430 [==============================] - 13s 25ms/step - loss: 0.4911 - accuracy: 0.8221 - val_loss: 0.3915 - val_accuracy: 0.8698
Epoch 2/100
430/430 [==============================] - 11s 26ms/step - loss: 0.3658 - accuracy: 0.8668 - val_loss: 0.3957 - val_accuracy: 0.8624
Epoch 3/100
430/430 [==============================] - 11s 25ms/step - loss: 0.3326 - accuracy: 0.8770 - val_loss: 0.4334 - val_accuracy: 0.8420
Epoch 4/100
430/430 [==============================] - 11s 26ms/step - loss: 0.3062 - accuracy: 0.8876 - val_loss: 0.3499 - val_accuracy: 0.8732
Epoch 5/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2841 - accuracy: 0.8940 - val_loss: 0.3356 - val_accuracy: 0.87380 - accuracy: 0.89
Epoch 6/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2701 - accuracy: 0.8994 - val_loss: 0.3503 - val_accuracy: 0.8818
Epoch 7/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2602 - accuracy: 0.9035 - val_loss: 0.3306 - val_accuracy: 0.8786
Epoch 8/100
430/430 [==============================] - 11s 26ms/step - loss: 0.2445 - accuracy: 0.9097 - val_loss: 0.3197 - val_accuracy: 0.8818
Epoch 9/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2328 - accuracy: 0.9140 - val_loss: 0.3531 - val_accuracy: 0.8774
Epoch 10/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2213 - accuracy: 0.9181 - val_loss: 0.3308 - val_accuracy: 0.8920
Epoch 11/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2129 - accuracy: 0.9201 - val_loss: 0.3762 - val_accuracy: 0.8728
Epoch 12/100
430/430 [==============================] - 11s 25ms/step - loss: 0.2044 - accuracy: 0.9247 - val_loss: 0.3339 - val_accuracy: 0.8828
Epoch 13/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1946 - accuracy: 0.9283 - val_loss: 0.3053 - val_accuracy: 0.8970
Epoch 14/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1893 - accuracy: 0.9296 - val_loss: 0.3219 - val_accuracy: 0.8908
Epoch 15/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1775 - accuracy: 0.9345 - val_loss: 0.3630 - val_accuracy: 0.8820
Epoch 16/100
430/430 [==============================] - 11s 24ms/step - loss: 0.1723 - accuracy: 0.9365 - val_loss: 0.3538 - val_accuracy: 0.8884
Epoch 17/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1613 - accuracy: 0.9408 - val_loss: 0.3284 - val_accuracy: 0.8988TA: 3s - -
Epoch 18/100
430/430 [==============================] - 11s 25ms/step - loss: 0.1573 - accuracy: 0.9410 - val_loss: 0.3598 - val_accuracy: 0.8862
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_with_nadam.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_43_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3452 - accuracy: 0.8880

[0.34519216418266296, 0.8880000114440918]
</code></pre><h1 id="ç»“è®º"><a href="#ç»“è®º" class="headerlink" title="ç»“è®º"></a>ç»“è®º</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Optimizer</th>
<th>Learning Rate</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Evaluation on x_test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Momentum</td>
<td>0.001</td>
<td>6s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8763</td>
</tr>
<tr>
<td>Momentum</td>
<td>0.003</td>
<td>6s/epoch</td>
<td>9 epochs</td>
<td>accuracy=0.8836</td>
</tr>
<tr>
<td>Momentum</td>
<td>0.009</td>
<td>6s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8851</td>
</tr>
<tr>
<td>Nesterov Momentum</td>
<td>0.001</td>
<td>7s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8767</td>
</tr>
<tr>
<td>Nesterov Momentum</td>
<td>0.003</td>
<td>7s/epoch</td>
<td>9 epochs</td>
<td>accuracy=0.8837</td>
</tr>
<tr>
<td>Nesterov Momentum</td>
<td>0.009</td>
<td>7s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8803</td>
</tr>
<tr>
<td>RMSprop</td>
<td>0.001</td>
<td>9s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8803</td>
</tr>
<tr>
<td>RMSprop</td>
<td>0.003</td>
<td>9s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8794</td>
</tr>
<tr>
<td>RMSprop</td>
<td>0.009</td>
<td>9s/epoch</td>
<td>6 epochs</td>
<td>accuracy=0.8817</td>
</tr>
<tr>
<td>Adam</td>
<td>0.001</td>
<td>6s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8846</td>
</tr>
<tr>
<td>Adam</td>
<td>0.003</td>
<td>6s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8778</td>
</tr>
<tr>
<td>Adam</td>
<td>0.009</td>
<td>6s/epoch</td>
<td>8 epochs</td>
<td>accuracy=0.8798</td>
</tr>
<tr>
<td>Nadam</td>
<td>0.001</td>
<td>11s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8826</td>
</tr>
<tr>
<td>Nadam</td>
<td>0.003</td>
<td>11s/epoch</td>
<td>5 epochs</td>
<td>accuracy=0.8797</td>
</tr>
<tr>
<td>Nadam</td>
<td>0.009</td>
<td>11s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8880</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>Momentum å’Œ Nesterov Momentum åœ¨è®­ç»ƒé€Ÿåº¦å’Œæ¨¡å‹æ€§èƒ½ä¸Šç›¸å·®ä¸å¤š, åœ¨é€‰å–äº†<strong>æ°å½“çš„å­¦ä¹ ç‡</strong>æ—¶, Nesterov Momentum <strong>æ”¶æ•›å¾ˆå¿«</strong>.</li>
<li>RMSprop åœ¨è®­ç»ƒé€Ÿåº¦ä¸Šç¨å¾®æ…¢äº Nesterov Momentum, ä½†<strong>æ”¶æ•›é€Ÿåº¦å¾ˆå¿«</strong>, æ¨¡å‹æ€§èƒ½ä¹Ÿä¸é”™.</li>
<li>Adam çš„è®­ç»ƒé€Ÿåº¦å’Œæ”¶æ•›é€Ÿåº¦éƒ½ä¸é”™, æ¨¡å‹æ€§èƒ½ä¹Ÿä¸èµ–.</li>
<li>Nadam çš„<strong>è®­ç»ƒé€Ÿåº¦æ˜æ˜¾æ…¢äºå…¶ä»–ä¼˜åŒ–å™¨</strong>, ä½†åœ¨<strong>æ°å½“çš„å­¦ä¹ ç‡ä¸‹, æ”¶æ•›é€Ÿåº¦å¾ˆå¿«</strong>, æ¨¡å‹æ€§èƒ½ä¹Ÿä¸é”™.   </li>
</ol>
<p>ğŸ’ å¯ä»¥æ”¹è¿›çš„ç‚¹: æˆ–è®¸é’ˆå¯¹ä¸åŒä¼˜åŒ–å™¨, åº”è¯¥é€šè¿‡æœç´¢è¶…å‚æ•°ç©ºé—´é€‰å–æœ€ä½³å­¦ä¹ ç‡, å†æ¯”è¾ƒå„ä¼˜åŒ–å™¨çš„æ€§èƒ½å·®å¼‚.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>æ€»ç»“ TensorFlow ä¸­å¸¸ç”¨çš„ resize å›¾åƒçš„æ–¹æ³•</title>
    <url>/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>ä¸‹é¢ä»¥ 2 å¼ æ ·æœ¬å›¾åƒä¸ºä¸€ä¸ª batch, å±•ç¤ºä»¥ä¸‹å››ç§ resize å›¾åƒæ–¹æ³•çš„å¼‚åŒ:</p>
<ol>
<li><p>tf.image.resize()</p>
</li>
<li><p>tf.image.resize_with_pad()</p>
</li>
<li><p>tf.image.resize_with_crop_or_pad()</p>
</li>
<li><p>tf.image.crop_and_resize()</p>
<span id="more"></span>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_sample_image</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åŠ è½½ &amp; å±•ç¤ºæ ·æœ¬å›¾åƒ</span></span><br><span class="line">china = load_sample_image(<span class="string">&quot;china.jpg&quot;</span>) / <span class="number">255</span></span><br><span class="line">flower = load_sample_image(<span class="string">&quot;flower.jpg&quot;</span>) / <span class="number">255</span></span><br><span class="line">images = np.array([china, flower])</span><br><span class="line"></span><br><span class="line">batch_size, height, width, channels = images.shape</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;æ ·æœ¬å›¾åƒçš„å°ºå¯¸å‡ä¸º (<span class="subst">&#123;height&#125;</span>, <span class="subst">&#123;width&#125;</span>)&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_images</span>(<span class="params">images</span>):</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow(images[i])</span><br><span class="line">        plt.axis(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plot_images(images)</span><br></pre></td></tr></table></figure>
<pre><code>æ ·æœ¬å›¾åƒçš„å°ºå¯¸å‡ä¸º (427, 640)
</code></pre><p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_3_1.png" alt="png"></p>
<h1 id="resize"><a href="#resize" class="headerlink" title="resize"></a>resize</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.åŸºæœ¬ç”¨æ³•</span></span><br><span class="line">imgs_resized = tf.image.resize(images, [<span class="number">256</span>, <span class="number">256</span>])</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_5_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.å¼ºåˆ¶ä¿æŒå®½é«˜æ¯”çš„å†™æ³•</span></span><br><span class="line">imgs_resized = tf.image.resize(images, [<span class="number">256</span>, <span class="number">256</span>], preserve_aspect_ratio=<span class="literal">True</span>)    </span><br><span class="line">display(imgs_resized.shape)</span><br><span class="line"></span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<pre><code>TensorShape([2, 171, 256, 3])
</code></pre><p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_6_1.png" alt="png"></p>
<h2 id="Remark"><a href="#Remark" class="headerlink" title="Remark"></a>Remark</h2><ol>
<li>resize() ä¸­æ¥æ”¶çš„ image çš„<strong>åƒç´ å¼ºåº¦</strong>åº”è¯¥åœ¨ [0, 1] ä¸­;</li>
<li>resize() æ— æ³•ä¿æŒ image çš„<strong>å®½é«˜æ¯”</strong>;</li>
<li>è®¾ç½®å‚æ•° preserve_aspect_ratio=True ä»¥<strong>å¼ºåˆ¶ä¿æŒå®½é«˜æ¯”</strong>æ—¶, å¾—åˆ°å›¾åƒçš„ shape ä¸ä¸€å®šèƒ½ä¸ size å‚æ•°ä¸€è‡´.</li>
</ol>
<h1 id="resize-with-pad"><a href="#resize-with-pad" class="headerlink" title="resize_with_pad"></a>resize_with_pad</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imgs_resized = tf.image.resize_with_pad(images, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_10_0.png" alt="png"></p>
<h2 id="Remark-1"><a href="#Remark-1" class="headerlink" title="Remark"></a>Remark</h2><ol>
<li>resize_with_pad() ä¸­è®¾ç½®å›¾åƒ<strong>ç›®æ ‡å®½é«˜çš„å†™æ³•</strong>ä¸ resize() ä¸åŒ;</li>
<li>resize_with_pad() <strong>æ€»æ˜¯ä¿æŒå®½é«˜æ¯”</strong>, ä¸ºæ­¤å¯èƒ½ä¼šåœ¨å›¾åƒå‘¨å›´<strong>æ·»åŠ é»‘è‰²å¸¦å­</strong>.</li>
</ol>
<h1 id="resize-with-crop-or-pad"><a href="#resize-with-crop-or-pad" class="headerlink" title="resize_with_crop_or_pad"></a>resize_with_crop_or_pad</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.ç›®æ ‡å°ºå¯¸ &lt; åŸå§‹å°ºå¯¸ = [ä¸­å¿ƒè£å‰ª]</span></span><br><span class="line">imgs_resized = tf.image.resize_with_crop_or_pad(images, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_14_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.ç›®æ ‡å°ºå¯¸ &gt; åŸå§‹å°ºå¯¸ = [æ·»åŠ é»‘è‰²å¸¦å­]</span></span><br><span class="line">imgs_resized = tf.image.resize_with_crop_or_pad(images, <span class="number">256</span>*<span class="number">3</span>, <span class="number">256</span>*<span class="number">3</span>)</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_15_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.ç›®æ ‡å°ºå¯¸ &amp; åŸå§‹å°ºå¯¸ æ— æ³•ç®€å•æ¯”è¾ƒå¤§å°æ—¶ = [è£å‰ª + é»‘å¸¦]</span></span><br><span class="line">imgs_resized = tf.image.resize_with_crop_or_pad(images, <span class="number">256</span>*<span class="number">2</span>, <span class="number">256</span>*<span class="number">2</span>)</span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_16_0.png" alt="png"></p>
<h2 id="Remark-2"><a href="#Remark-2" class="headerlink" title="Remark"></a>Remark</h2><ol>
<li>resize_with_crop_or_pad() è®¾ç½®<strong>ç›®æ ‡å®½é«˜çš„å†™æ³•</strong>ä¸ resize_with_pad() ç›¸åŒ;</li>
<li>resize_with_crop_or_pad() <strong>æ€»æ˜¯ä¿æŒå®½é«˜æ¯”</strong>: é‡‡å– <strong>ä¸­å¿ƒè£å‰ª</strong> or <strong>æ·»åŠ é»‘è‰²å¸¦å­</strong> or <strong>äºŒè€…ç»“åˆ</strong> çš„åšæ³•. </li>
</ol>
<h1 id="crop-and-resize"><a href="#crop-and-resize" class="headerlink" title="crop_and_resize"></a>crop_and_resize</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.æ°å½“çš„è£å‰ªæ¡†</span></span><br><span class="line">china_box = [<span class="number">0</span>, <span class="number">0.03</span>, <span class="number">1</span>, <span class="number">0.68</span>]    <span class="comment"># è§„èŒƒåŒ–åæ ‡ [y1, x1, y2, x2]</span></span><br><span class="line">flower_box = [<span class="number">0.19</span>, <span class="number">0.26</span>, <span class="number">0.86</span>, <span class="number">0.7</span>]</span><br><span class="line"></span><br><span class="line">imgs_resized = tf.image.crop_and_resize(images, </span><br><span class="line">                                        boxes=[china_box, flower_box], </span><br><span class="line">                                        box_indices=[<span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">                                        crop_size=[<span class="number">256</span>, <span class="number">256</span>])</span><br><span class="line"></span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_20_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.ä¸æ°å½“çš„è£å‰ªæ¡†</span></span><br><span class="line">china_box = [<span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">1.0</span>, <span class="number">0.9</span>]    <span class="comment"># çœ‹å¾—å‡ºæ¥¼&quot;å˜ç˜¦äº†&quot;</span></span><br><span class="line">flower_box = [<span class="number">0.0</span>, <span class="number">0.26</span>, <span class="number">1.0</span>, <span class="number">0.7</span>]   <span class="comment"># çœ‹å¾—å‡ºèŠ±&quot;å˜èƒ–äº†&quot;</span></span><br><span class="line"></span><br><span class="line">imgs_resized = tf.image.crop_and_resize(images, </span><br><span class="line">                                        boxes=[china_box, flower_box], </span><br><span class="line">                                        box_indices=[<span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">                                        crop_size=[<span class="number">256</span>, <span class="number">256</span>])</span><br><span class="line"></span><br><span class="line">plot_images(imgs_resized)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/output_21_0.png" alt="png"></p>
<h2 id="Remark-3"><a href="#Remark-3" class="headerlink" title="Remark"></a>Remark</h2><ol>
<li>crop_and_resize() éœ€è¦è‡ªè¡ŒæŒ‡å®š<strong>è£å‰ªæ¡†</strong>, è‹¥é€‰å–ä¸å½“, åˆ™æ— æ³•ä¿æŒ resize åçš„å®½é«˜æ¯”;</li>
<li>crop_size ç”¨äºæŒ‡å®šè¾“å‡ºå›¾åƒçš„ shape.</li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>å„ç§æ­£åˆ™åŒ–æ–¹æ³•ä¹‹é—´çš„å¯¹æ¯”</title>
    <url>/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>â“ å¦‚ä½•åœ¨å„ç§æ­£åˆ™åŒ–æ–¹æ³•ä¸­åšå‡ºé€‰æ‹©?</p>
<ol>
<li><p>EarlyStopping  </p>
</li>
<li><p>l1 &amp; l2 æ­£åˆ™åŒ–  </p>
</li>
<li><p>Dropout (AlphaDropout)</p>
</li>
<li><p>Batch Normalization (æœ‰ä¸€å®šæ­£åˆ™åŒ–æ•ˆæœ)</p>
<span id="more"></span>
</li>
</ol>
<p>â€» ä¸‹é¢é’ˆå¯¹(åŸºæœ¬)ç›¸åŒçš„ &lt;ç½‘ç»œæ¶æ„ &amp; ä¼˜åŒ–å™¨ &amp; å­¦ä¹ ç‡è°ƒåº¦&gt;, å¯¹æ¯”ä¸åŒæ­£åˆ™åŒ–æ–¹æ³•å¸¦ç»™æ¨¡å‹çš„å½±å“.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>ğŸ”º é’ˆå¯¹ Fashion MNIST æ•°æ®é›†, å¼€å±•ä¸‹é¢çš„æµ‹è¯•.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å‡†å¤‡æ•°æ®é›† (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist ä¸­æ•°å­—æ ‡ç­¾å¯¹åº”çš„ç±»åˆ«åç§°</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å±•ç¤ºéƒ¨åˆ†è®­ç»ƒé›†å®ä¾‹</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m è¡Œ n åˆ—</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_6_0.png" alt="png"></p>
<p>Remark: åŸºäºè®­ç»ƒé€Ÿåº¦è€ƒè™‘, ä¸‹é¢é€‰ç”¨äº† batch_size=128 ä»¥åŠ é€Ÿè®­ç»ƒ, ä½†ä½¿ç”¨æ›´å°çš„ batch_size (å¦‚ 32) å¯èƒ½è·å¾—æ›´å¥½çš„æ¨¡å‹.</p>
<h1 id="No-Regularization"><a href="#No-Regularization" class="headerlink" title="No Regularization"></a>No Regularization</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">model = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">              optimizer=optimizer, </span><br><span class="line">              metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 8s 16ms/step - loss: 0.8197 - accuracy: 0.7210 - val_loss: 0.5812 - val_accuracy: 0.7936 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 6s 15ms/step - loss: 0.4344 - accuracy: 0.8399 - val_loss: 0.4085 - val_accuracy: 0.8498 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 7s 15ms/step - loss: 0.4009 - accuracy: 0.8525 - val_loss: 0.4098 - val_accuracy: 0.8544 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 6s 15ms/step - loss: 0.3585 - accuracy: 0.8691 - val_loss: 0.3402 - val_accuracy: 0.8764 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 7s 15ms/step - loss: 0.3367 - accuracy: 0.8767 - val_loss: 0.3698 - val_accuracy: 0.8662 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 7s 15ms/step - loss: 0.3209 - accuracy: 0.8835 - val_loss: 0.4160 - val_accuracy: 0.8668 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 7s 15ms/step - loss: 0.3122 - accuracy: 0.8874 - val_loss: 0.3409 - val_accuracy: 0.8800 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 7s 15ms/step - loss: 0.3012 - accuracy: 0.8907 - val_loss: 0.3483 - val_accuracy: 0.8762 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 7s 15ms/step - loss: 0.2365 - accuracy: 0.9120 - val_loss: 0.2883 - val_accuracy: 0.8986 - lr: 0.0010
Epoch 10/25
430/430 [==============================] - 7s 16ms/step - loss: 0.2210 - accuracy: 0.9175 - val_loss: 0.2918 - val_accuracy: 0.8984 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 7s 16ms/step - loss: 0.2133 - accuracy: 0.9199 - val_loss: 0.3090 - val_accuracy: 0.8972 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 7s 16ms/step - loss: 0.2070 - accuracy: 0.9219 - val_loss: 0.3103 - val_accuracy: 0.8938 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 7s 16ms/step - loss: 0.2002 - accuracy: 0.9249 - val_loss: 0.2996 - val_accuracy: 0.9030 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1657 - accuracy: 0.9371 - val_loss: 0.2945 - val_accuracy: 0.9042 - lr: 3.3333e-04
Epoch 15/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1547 - accuracy: 0.9411 - val_loss: 0.3044 - val_accuracy: 0.9060 - lr: 3.3333e-04
Epoch 16/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1481 - accuracy: 0.9445 - val_loss: 0.3096 - val_accuracy: 0.9036 - lr: 3.3333e-04
Epoch 17/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1420 - accuracy: 0.9459 - val_loss: 0.3186 - val_accuracy: 0.9050 - lr: 3.3333e-04
Epoch 18/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1235 - accuracy: 0.9539 - val_loss: 0.3218 - val_accuracy: 0.9092 - lr: 1.1111e-04
Epoch 19/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1184 - accuracy: 0.9554 - val_loss: 0.3327 - val_accuracy: 0.9084 - lr: 1.1111e-04
Epoch 20/25
430/430 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.95 - 7s 16ms/step - loss: 0.1143 - accuracy: 0.9576 - val_loss: 0.3411 - val_accuracy: 0.9074 - lr: 1.1111e-04
Epoch 21/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1107 - accuracy: 0.9591 - val_loss: 0.3515 - val_accuracy: 0.9052 - lr: 1.1111e-04
Epoch 22/25
430/430 [==============================] - 7s 16ms/step - loss: 0.1022 - accuracy: 0.9624 - val_loss: 0.3601 - val_accuracy: 0.9072 - lr: 3.7037e-05
Epoch 23/25
430/430 [==============================] - 7s 16ms/step - loss: 0.0999 - accuracy: 0.9629 - val_loss: 0.3624 - val_accuracy: 0.9042 - lr: 3.7037e-05
Epoch 24/25
430/430 [==============================] - 7s 16ms/step - loss: 0.0984 - accuracy: 0.9639 - val_loss: 0.3709 - val_accuracy: 0.9060 - lr: 3.7037e-05
Epoch 25/25
430/430 [==============================] - 7s 16ms/step - loss: 0.0968 - accuracy: 0.9644 - val_loss: 0.3713 - val_accuracy: 0.9066 - lr: 3.7037e-05
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model.evaluate(x_train, y_train))</span><br><span class="line">display(model.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_11_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 5s 3ms/step - loss: 0.0929 - accuracy: 0.9661

[0.09290006756782532, 0.9660909175872803]


313/313 [==============================] - 1s 3ms/step - loss: 0.4360 - accuracy: 0.8949

[0.4359566271305084, 0.8949000239372253]
</code></pre><p>Remark: ä»ä¸Šè¿°å­¦ä¹ æ›²çº¿å¯ä»¥çœ‹å‡º, ä¸å¸¦ä»»ä½•æ­£åˆ™åŒ–çš„æ¨¡å‹ç¡®å®è¿‡æ‹Ÿåˆäº†è®­ç»ƒé›†.</p>
<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">model_with_BN = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_BN.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_with_BN.add(keras.layers.BatchNormalization())</span><br><span class="line"></span><br><span class="line">model_with_BN.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_BN.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_BN = model_with_BN.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                    validation_data=(x_valid, y_valid),</span><br><span class="line">                                    callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 19s 37ms/step - loss: 0.4899 - accuracy: 0.8245 - val_loss: 0.3639 - val_accuracy: 0.8784 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 16s 37ms/step - loss: 0.3666 - accuracy: 0.8671 - val_loss: 0.3484 - val_accuracy: 0.8772 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 15s 36ms/step - loss: 0.3322 - accuracy: 0.8778 - val_loss: 0.4017 - val_accuracy: 0.8600 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 16s 37ms/step - loss: 0.3066 - accuracy: 0.8888 - val_loss: 0.3411 - val_accuracy: 0.8762 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 16s 37ms/step - loss: 0.2821 - accuracy: 0.8945 - val_loss: 0.2950 - val_accuracy: 0.8922 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 16s 37ms/step - loss: 0.2656 - accuracy: 0.9027 - val_loss: 0.3312 - val_accuracy: 0.8888 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 16s 38ms/step - loss: 0.2543 - accuracy: 0.9072 - val_loss: 0.3196 - val_accuracy: 0.8902 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 16s 37ms/step - loss: 0.2390 - accuracy: 0.9133 - val_loss: 0.3031 - val_accuracy: 0.8898 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 16s 37ms/step - loss: 0.2224 - accuracy: 0.9181 - val_loss: 0.3247 - val_accuracy: 0.8894 - lr: 0.0030
Epoch 10/25
430/430 [==============================] - 16s 38ms/step - loss: 0.1692 - accuracy: 0.9375 - val_loss: 0.2805 - val_accuracy: 0.9052 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 16s 38ms/step - loss: 0.1499 - accuracy: 0.9450 - val_loss: 0.2957 - val_accuracy: 0.9022 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 16s 38ms/step - loss: 0.1399 - accuracy: 0.9487 - val_loss: 0.3072 - val_accuracy: 0.8954 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 16s 38ms/step - loss: 0.1311 - accuracy: 0.9515 - val_loss: 0.2964 - val_accuracy: 0.9006 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 17s 40ms/step - loss: 0.1218 - accuracy: 0.9551 - val_loss: 0.3106 - val_accuracy: 0.9034 - lr: 0.0010
Epoch 15/25
430/430 [==============================] - 17s 39ms/step - loss: 0.0939 - accuracy: 0.9671 - val_loss: 0.3114 - val_accuracy: 0.9078 - lr: 3.3333e-04
Epoch 16/25
430/430 [==============================] - 17s 39ms/step - loss: 0.0830 - accuracy: 0.9705 - val_loss: 0.3309 - val_accuracy: 0.9030 - lr: 3.3333e-04
Epoch 17/25
430/430 [==============================] - 17s 39ms/step - loss: 0.0764 - accuracy: 0.9726 - val_loss: 0.3344 - val_accuracy: 0.9040 - lr: 3.3333e-04
Epoch 18/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0726 - accuracy: 0.9738 - val_loss: 0.3462 - val_accuracy: 0.9032 - lr: 3.3333e-04
Epoch 19/25
430/430 [==============================] - 17s 38ms/step - loss: 0.0603 - accuracy: 0.9789 - val_loss: 0.3595 - val_accuracy: 0.9058 - lr: 1.1111e-04
Epoch 20/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0578 - accuracy: 0.9797 - val_loss: 0.3658 - val_accuracy: 0.9062 - lr: 1.1111e-04
Epoch 21/25
430/430 [==============================] - 16s 37ms/step - loss: 0.0544 - accuracy: 0.9810 - val_loss: 0.3720 - val_accuracy: 0.9056 - lr: 1.1111e-04
Epoch 22/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0535 - accuracy: 0.9815 - val_loss: 0.3803 - val_accuracy: 0.9052 - lr: 1.1111e-04
Epoch 23/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0510 - accuracy: 0.9825 - val_loss: 0.3804 - val_accuracy: 0.9052 - lr: 3.7037e-05
Epoch 24/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0485 - accuracy: 0.9833 - val_loss: 0.3842 - val_accuracy: 0.9070 - lr: 3.7037e-05
Epoch 25/25
430/430 [==============================] - 16s 38ms/step - loss: 0.0480 - accuracy: 0.9832 - val_loss: 0.3861 - val_accuracy: 0.9048 - lr: 3.7037e-05ss: 0.0481 -  - ETA: 0s - loss: 0.0480 - accuracy: 0.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_with_BN.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model_with_BN.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_BN.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_16_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 10s 6ms/step - loss: 0.0295 - accuracy: 0.9914

[0.029543157666921616, 0.9914363622665405]


313/313 [==============================] - 2s 6ms/step - loss: 0.4273 - accuracy: 0.9016

[0.4273393452167511, 0.9016000032424927]
</code></pre><h1 id="EarlyStopping"><a href="#EarlyStopping" class="headerlink" title="EarlyStopping"></a>EarlyStopping</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">model_with_ES = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_ES.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_with_ES.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_ES.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">8</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">history_with_ES = model_with_ES.fit(x_train, y_train, epochs=<span class="number">25</span>*<span class="number">2</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                    validation_data=(x_valid, y_valid),</span><br><span class="line">                                    callbacks=[lr_scheduler, early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/50
430/430 [==============================] - 9s 17ms/step - loss: 0.8095 - accuracy: 0.7205 - val_loss: 0.5028 - val_accuracy: 0.8152 - lr: 0.0030
Epoch 2/50
430/430 [==============================] - 7s 17ms/step - loss: 0.4347 - accuracy: 0.8411 - val_loss: 0.4048 - val_accuracy: 0.8596 - lr: 0.0030
Epoch 3/50
430/430 [==============================] - 7s 17ms/step - loss: 0.3897 - accuracy: 0.8570 - val_loss: 0.3748 - val_accuracy: 0.8630 - lr: 0.0030
Epoch 4/50
430/430 [==============================] - 7s 16ms/step - loss: 0.3533 - accuracy: 0.8711 - val_loss: 0.3313 - val_accuracy: 0.8804 - lr: 0.0030
Epoch 5/50
430/430 [==============================] - 7s 16ms/step - loss: 0.3341 - accuracy: 0.8781 - val_loss: 0.3669 - val_accuracy: 0.8662 - lr: 0.0030
Epoch 6/50
430/430 [==============================] - 7s 16ms/step - loss: 0.3198 - accuracy: 0.8835 - val_loss: 0.3579 - val_accuracy: 0.8746 - lr: 0.0030
Epoch 7/50
430/430 [==============================] - 7s 16ms/step - loss: 0.3080 - accuracy: 0.8889 - val_loss: 0.3232 - val_accuracy: 0.8852 - lr: 0.0030
Epoch 8/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2951 - accuracy: 0.8922 - val_loss: 0.3478 - val_accuracy: 0.8726 - lr: 0.0030
Epoch 9/50
430/430 [==============================] - 7s 16ms/step - loss: 0.2905 - accuracy: 0.8959 - val_loss: 0.3328 - val_accuracy: 0.8818 - lr: 0.0030
Epoch 10/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2822 - accuracy: 0.8963 - val_loss: 0.3141 - val_accuracy: 0.8928 - lr: 0.0030
Epoch 11/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2712 - accuracy: 0.9028 - val_loss: 0.3353 - val_accuracy: 0.8810 - lr: 0.0030
Epoch 12/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2678 - accuracy: 0.9029 - val_loss: 0.3128 - val_accuracy: 0.8864 - lr: 0.0030
Epoch 13/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2572 - accuracy: 0.9064 - val_loss: 0.3247 - val_accuracy: 0.8894 - lr: 0.0030
Epoch 14/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2548 - accuracy: 0.9078 - val_loss: 0.3044 - val_accuracy: 0.8902 - lr: 0.0030
Epoch 15/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2513 - accuracy: 0.9106 - val_loss: 0.3109 - val_accuracy: 0.8954 - lr: 0.0030
Epoch 16/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2415 - accuracy: 0.9137 - val_loss: 0.3293 - val_accuracy: 0.8880 - lr: 0.0030
Epoch 17/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2472 - accuracy: 0.9120 - val_loss: 0.3558 - val_accuracy: 0.8794 - lr: 0.0030
Epoch 18/50
430/430 [==============================] - 7s 17ms/step - loss: 0.2340 - accuracy: 0.9164 - val_loss: 0.3173 - val_accuracy: 0.8950 - lr: 0.0030
Epoch 19/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1830 - accuracy: 0.9321 - val_loss: 0.2955 - val_accuracy: 0.9052 - lr: 0.0010
Epoch 20/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1649 - accuracy: 0.9390 - val_loss: 0.3008 - val_accuracy: 0.9050 - lr: 0.0010
Epoch 21/50
430/430 [==============================] - 7s 16ms/step - loss: 0.1560 - accuracy: 0.9419 - val_loss: 0.3085 - val_accuracy: 0.9062 - lr: 0.0010
Epoch 22/50
430/430 [==============================] - 7s 16ms/step - loss: 0.1493 - accuracy: 0.9436 - val_loss: 0.3473 - val_accuracy: 0.9038 - lr: 0.0010
Epoch 23/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1445 - accuracy: 0.9462 - val_loss: 0.3221 - val_accuracy: 0.9048 - lr: 0.0010
Epoch 24/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1197 - accuracy: 0.9547 - val_loss: 0.3485 - val_accuracy: 0.9106 - lr: 3.3333e-04
Epoch 25/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1117 - accuracy: 0.9577 - val_loss: 0.3573 - val_accuracy: 0.9098 - lr: 3.3333e-04
Epoch 26/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1064 - accuracy: 0.9594 - val_loss: 0.3843 - val_accuracy: 0.9062 - lr: 3.3333e-04
Epoch 27/50
430/430 [==============================] - 7s 17ms/step - loss: 0.1010 - accuracy: 0.9622 - val_loss: 0.3881 - val_accuracy: 0.9080 - lr: 3.3333e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_with_ES.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model_with_ES.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_ES.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_20_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 6s 3ms/step - loss: 0.1625 - accuracy: 0.9382

[0.16253036260604858, 0.9381999969482422]


313/313 [==============================] - 1s 4ms/step - loss: 0.3310 - accuracy: 0.8977

[0.3310108780860901, 0.8977000117301941]
</code></pre><h1 id="l1-amp-l2-Regularization"><a href="#l1-amp-l2-Regularization" class="headerlink" title="l1 &amp; l2 Regularization"></a>l1 &amp; l2 Regularization</h1><h2 id="l1"><a href="#l1" class="headerlink" title="l1()"></a>l1()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">model_with_l1_reg = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_l1_reg.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, </span><br><span class="line">                                             kernel_regularizer=keras.regularizers.l1(l1=<span class="number">3e-6</span>)))</span><br><span class="line"></span><br><span class="line">model_with_l1_reg.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_l1_reg.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                          optimizer=optimizer, </span><br><span class="line">                          metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_l1_reg = model_with_l1_reg.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                            validation_data=(x_valid, y_valid),</span><br><span class="line">                                            callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 9s 18ms/step - loss: 0.7871 - accuracy: 0.7362 - val_loss: 0.4988 - val_accuracy: 0.8262 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 8s 18ms/step - loss: 0.4610 - accuracy: 0.8381 - val_loss: 0.3974 - val_accuracy: 0.8626 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 8s 18ms/step - loss: 0.4129 - accuracy: 0.8566 - val_loss: 0.4618 - val_accuracy: 0.8364 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3745 - accuracy: 0.8711 - val_loss: 0.4005 - val_accuracy: 0.8650 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3581 - accuracy: 0.8765 - val_loss: 0.3781 - val_accuracy: 0.8750 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3447 - accuracy: 0.8819 - val_loss: 0.3859 - val_accuracy: 0.8792 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3369 - accuracy: 0.8861 - val_loss: 0.3767 - val_accuracy: 0.8768 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3350 - accuracy: 0.8866 - val_loss: 0.4536 - val_accuracy: 0.8540 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3297 - accuracy: 0.8891 - val_loss: 0.3566 - val_accuracy: 0.8804 - lr: 0.0030
Epoch 10/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3097 - accuracy: 0.8958 - val_loss: 0.3362 - val_accuracy: 0.8928 - lr: 0.0030
Epoch 11/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2984 - accuracy: 0.8985 - val_loss: 0.3772 - val_accuracy: 0.8768 - lr: 0.0030
Epoch 12/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2943 - accuracy: 0.9015 - val_loss: 0.3510 - val_accuracy: 0.8840 - lr: 0.0030
Epoch 13/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2871 - accuracy: 0.9032 - val_loss: 0.3567 - val_accuracy: 0.8878 - lr: 0.0030
Epoch 14/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2776 - accuracy: 0.9068 - val_loss: 0.3540 - val_accuracy: 0.8824 - lr: 0.0030
Epoch 15/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2213 - accuracy: 0.9257 - val_loss: 0.3112 - val_accuracy: 0.8996 - lr: 0.0010
Epoch 16/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2056 - accuracy: 0.9315 - val_loss: 0.3219 - val_accuracy: 0.9006 - lr: 0.0010
Epoch 17/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1982 - accuracy: 0.9326 - val_loss: 0.3345 - val_accuracy: 0.8972 - lr: 0.0010
Epoch 18/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1934 - accuracy: 0.9350 - val_loss: 0.3232 - val_accuracy: 0.8968 - lr: 0.0010
Epoch 19/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1875 - accuracy: 0.9375 - val_loss: 0.3313 - val_accuracy: 0.9028 - lr: 0.0010
Epoch 20/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1584 - accuracy: 0.9486 - val_loss: 0.3293 - val_accuracy: 0.9068 - lr: 3.3333e-04
Epoch 21/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1479 - accuracy: 0.9521 - val_loss: 0.3383 - val_accuracy: 0.9060 - lr: 3.3333e-04
Epoch 22/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1422 - accuracy: 0.9536 - val_loss: 0.3450 - val_accuracy: 0.9058 - lr: 3.3333e-04
Epoch 23/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1371 - accuracy: 0.9558 - val_loss: 0.3588 - val_accuracy: 0.9070 - lr: 3.3333e-04
Epoch 24/25
430/430 [==============================] - 9s 21ms/step - loss: 0.1218 - accuracy: 0.9617 - val_loss: 0.3806 - val_accuracy: 0.9074 - lr: 1.1111e-04
Epoch 25/25
430/430 [==============================] - 8s 19ms/step - loss: 0.1173 - accuracy: 0.9640 - val_loss: 0.3880 - val_accuracy: 0.9064 - lr: 1.1111e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_with_l1_reg.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model_with_l1_reg.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_l1_reg.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_25_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 7s 4ms/step - loss: 0.1113 - accuracy: 0.9666

[0.11131621152162552, 0.9665636420249939]


313/313 [==============================] - 1s 4ms/step - loss: 0.4361 - accuracy: 0.8969

[0.4360505938529968, 0.8968999981880188]
</code></pre><h2 id="l2"><a href="#l2" class="headerlink" title="l2()"></a>l2()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">model_with_l2_reg = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_l2_reg.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, </span><br><span class="line">                                             kernel_regularizer=keras.regularizers.l2(l2=<span class="number">3e-6</span>)))</span><br><span class="line"></span><br><span class="line">model_with_l2_reg.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_l2_reg.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                          optimizer=optimizer, </span><br><span class="line">                          metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_l2_reg = model_with_l2_reg.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                            validation_data=(x_valid, y_valid),</span><br><span class="line">                                            callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 9s 18ms/step - loss: 0.7842 - accuracy: 0.7315 - val_loss: 0.5412 - val_accuracy: 0.8034 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 7s 17ms/step - loss: 0.4384 - accuracy: 0.8405 - val_loss: 0.3967 - val_accuracy: 0.8544 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3980 - accuracy: 0.8554 - val_loss: 0.4102 - val_accuracy: 0.8536 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3568 - accuracy: 0.8709 - val_loss: 0.3396 - val_accuracy: 0.8790 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3413 - accuracy: 0.8748 - val_loss: 0.3545 - val_accuracy: 0.8774 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3269 - accuracy: 0.8816 - val_loss: 0.3683 - val_accuracy: 0.8756 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3185 - accuracy: 0.8860 - val_loss: 0.3836 - val_accuracy: 0.8744 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3023 - accuracy: 0.8915 - val_loss: 0.3632 - val_accuracy: 0.8698 - lr: 0.0030.3022 - accuracy
Epoch 9/25
430/430 [==============================] - 8s 17ms/step - loss: 0.2381 - accuracy: 0.9125 - val_loss: 0.2946 - val_accuracy: 0.8972 - lr: 0.0010
Epoch 10/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2228 - accuracy: 0.9172 - val_loss: 0.3077 - val_accuracy: 0.9002 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2150 - accuracy: 0.9195 - val_loss: 0.3079 - val_accuracy: 0.8986 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2092 - accuracy: 0.9221 - val_loss: 0.3226 - val_accuracy: 0.8910 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2049 - accuracy: 0.9240 - val_loss: 0.3332 - val_accuracy: 0.8926 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1692 - accuracy: 0.9374 - val_loss: 0.3160 - val_accuracy: 0.9040 - lr: 3.3333e-04
Epoch 15/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1574 - accuracy: 0.9415 - val_loss: 0.3203 - val_accuracy: 0.9002 - lr: 3.3333e-04
Epoch 16/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1520 - accuracy: 0.9444 - val_loss: 0.3274 - val_accuracy: 0.9008 - lr: 3.3333e-04
Epoch 17/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1448 - accuracy: 0.9466 - val_loss: 0.3430 - val_accuracy: 0.8996 - lr: 3.3333e-04
Epoch 18/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1267 - accuracy: 0.9539 - val_loss: 0.3502 - val_accuracy: 0.9046 - lr: 1.1111e-04
Epoch 19/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1215 - accuracy: 0.9550 - val_loss: 0.3652 - val_accuracy: 0.9016 - lr: 1.1111e-04
Epoch 20/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1176 - accuracy: 0.9575 - val_loss: 0.3744 - val_accuracy: 0.9002 - lr: 1.1111e-04
Epoch 21/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1138 - accuracy: 0.9583 - val_loss: 0.3721 - val_accuracy: 0.9020 - lr: 1.1111e-04
Epoch 22/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1055 - accuracy: 0.9619 - val_loss: 0.3887 - val_accuracy: 0.9000 - lr: 3.7037e-05
Epoch 23/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1035 - accuracy: 0.9632 - val_loss: 0.3942 - val_accuracy: 0.9012 - lr: 3.7037e-05
Epoch 24/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1019 - accuracy: 0.9634 - val_loss: 0.4113 - val_accuracy: 0.9028 - lr: 3.7037e-05
Epoch 25/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1003 - accuracy: 0.9637 - val_loss: 0.4053 - val_accuracy: 0.8996 - lr: 3.7037e-05
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_with_l2_reg.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model_with_l2_reg.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_l2_reg.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_29_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 7s 4ms/step - loss: 0.0970 - accuracy: 0.9660

[0.09695354849100113, 0.9660181999206543]


313/313 [==============================] - 1s 4ms/step - loss: 0.4260 - accuracy: 0.8957

[0.42597758769989014, 0.8956999778747559]
</code></pre><h2 id="l1-l2"><a href="#l1-l2" class="headerlink" title="l1_l2()"></a>l1_l2()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">model_with_l1l2_reg = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_with_l1l2_reg.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, </span><br><span class="line">                                               kernel_regularizer=keras.regularizers.l1_l2(l1=<span class="number">3e-6</span>, l2=<span class="number">3e-6</span>)))</span><br><span class="line"></span><br><span class="line">model_with_l1l2_reg.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_l1l2_reg.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                            optimizer=optimizer, </span><br><span class="line">                            metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_l1l2_reg = model_with_l1l2_reg.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                                validation_data=(x_valid, y_valid),</span><br><span class="line">                                                callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 10s 19ms/step - loss: 0.8085 - accuracy: 0.7345 - val_loss: 0.5617 - val_accuracy: 0.8048 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 8s 18ms/step - loss: 0.4591 - accuracy: 0.8401 - val_loss: 0.3946 - val_accuracy: 0.8642 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 8s 18ms/step - loss: 0.4229 - accuracy: 0.8565 - val_loss: 0.4137 - val_accuracy: 0.8612 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3774 - accuracy: 0.8725 - val_loss: 0.3887 - val_accuracy: 0.8712 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3602 - accuracy: 0.8767 - val_loss: 0.3529 - val_accuracy: 0.8808 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3462 - accuracy: 0.8836 - val_loss: 0.3782 - val_accuracy: 0.8790 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3335 - accuracy: 0.8872 - val_loss: 0.3859 - val_accuracy: 0.8710 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3223 - accuracy: 0.8920 - val_loss: 0.3978 - val_accuracy: 0.8630 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3145 - accuracy: 0.8946 - val_loss: 0.4028 - val_accuracy: 0.8780 - lr: 0.0030
Epoch 10/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2519 - accuracy: 0.9149 - val_loss: 0.3222 - val_accuracy: 0.8966 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2350 - accuracy: 0.9201 - val_loss: 0.3229 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2277 - accuracy: 0.9229 - val_loss: 0.3259 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2204 - accuracy: 0.9254 - val_loss: 0.3195 - val_accuracy: 0.8978 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2134 - accuracy: 0.9278 - val_loss: 0.3226 - val_accuracy: 0.8994 - lr: 0.0010
Epoch 15/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2083 - accuracy: 0.9297 - val_loss: 0.3433 - val_accuracy: 0.8950 - lr: 0.0010
Epoch 16/25
430/430 [==============================] - 8s 18ms/step - loss: 0.2026 - accuracy: 0.9326 - val_loss: 0.3542 - val_accuracy: 0.8972 - lr: 0.0010
Epoch 17/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1940 - accuracy: 0.9351 - val_loss: 0.3455 - val_accuracy: 0.8970 - lr: 0.0010
Epoch 18/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1590 - accuracy: 0.9479 - val_loss: 0.3532 - val_accuracy: 0.9030 - lr: 3.3333e-04
Epoch 19/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1476 - accuracy: 0.9527 - val_loss: 0.3635 - val_accuracy: 0.9056 - lr: 3.3333e-04
Epoch 20/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1410 - accuracy: 0.9547 - val_loss: 0.3677 - val_accuracy: 0.9030 - lr: 3.3333e-04
Epoch 21/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1349 - accuracy: 0.9567 - val_loss: 0.3878 - val_accuracy: 0.9004 - lr: 3.3333e-04
Epoch 22/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1174 - accuracy: 0.9647 - val_loss: 0.4038 - val_accuracy: 0.8996 - lr: 1.1111e-04
Epoch 23/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1119 - accuracy: 0.9663 - val_loss: 0.4112 - val_accuracy: 0.9028 - lr: 1.1111e-04
Epoch 24/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1082 - accuracy: 0.9682 - val_loss: 0.4321 - val_accuracy: 0.9012 - lr: 1.1111e-04
Epoch 25/25
430/430 [==============================] - 8s 18ms/step - loss: 0.1048 - accuracy: 0.9696 - val_loss: 0.4387 - val_accuracy: 0.8994 - lr: 1.1111e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_with_l1l2_reg.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model_with_l1l2_reg.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_l1l2_reg.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_33_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 8s 5ms/step - loss: 0.0980 - accuracy: 0.9726

[0.09802676737308502, 0.9726181626319885]


313/313 [==============================] - 1s 5ms/step - loss: 0.4885 - accuracy: 0.8947

[0.48848921060562134, 0.8946999907493591]
</code></pre><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><h2 id="Regular-Dropout"><a href="#Regular-Dropout" class="headerlink" title="Regular Dropout"></a>Regular Dropout</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">model_with_dropout = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    model_with_dropout.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    model_with_dropout.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_with_dropout.add(keras.layers.Dropout(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">model_with_dropout.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_dropout.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                           optimizer=optimizer, </span><br><span class="line">                           metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_dropout = model_with_dropout.fit(x_train, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                              validation_data=(x_valid, y_valid),</span><br><span class="line">                                              callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 8s 16ms/step - loss: 0.8331 - accuracy: 0.7204 - val_loss: 0.4820 - val_accuracy: 0.8290 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 7s 17ms/step - loss: 0.4750 - accuracy: 0.8316 - val_loss: 0.4168 - val_accuracy: 0.8586 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 7s 16ms/step - loss: 0.4474 - accuracy: 0.8440 - val_loss: 0.4188 - val_accuracy: 0.8600 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3892 - accuracy: 0.8620 - val_loss: 0.3509 - val_accuracy: 0.8774 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3694 - accuracy: 0.8702 - val_loss: 0.3763 - val_accuracy: 0.8548 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3506 - accuracy: 0.8758 - val_loss: 0.3687 - val_accuracy: 0.8754 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3407 - accuracy: 0.8797 - val_loss: 0.3697 - val_accuracy: 0.8700 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3362 - accuracy: 0.8824 - val_loss: 0.3767 - val_accuracy: 0.8666 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2661 - accuracy: 0.9041 - val_loss: 0.2933 - val_accuracy: 0.8966 - lr: 0.0010
Epoch 10/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2487 - accuracy: 0.9090 - val_loss: 0.2946 - val_accuracy: 0.8970 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2408 - accuracy: 0.9118 - val_loss: 0.3200 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2335 - accuracy: 0.9145 - val_loss: 0.3137 - val_accuracy: 0.8896 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2277 - accuracy: 0.9167 - val_loss: 0.2923 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2213 - accuracy: 0.9182 - val_loss: 0.3001 - val_accuracy: 0.8968 - lr: 0.0010
Epoch 15/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2144 - accuracy: 0.9204 - val_loss: 0.3046 - val_accuracy: 0.8960 - lr: 0.0010
Epoch 16/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2048 - accuracy: 0.9255 - val_loss: 0.3116 - val_accuracy: 0.8984 - lr: 0.0010
Epoch 17/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2021 - accuracy: 0.9252 - val_loss: 0.3191 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 18/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1656 - accuracy: 0.9390 - val_loss: 0.3177 - val_accuracy: 0.9038 - lr: 3.3333e-04
Epoch 19/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1537 - accuracy: 0.9431 - val_loss: 0.3284 - val_accuracy: 0.9050 - lr: 3.3333e-04
Epoch 20/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1476 - accuracy: 0.9449 - val_loss: 0.3322 - val_accuracy: 0.9006 - lr: 3.3333e-04
Epoch 21/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1417 - accuracy: 0.9474 - val_loss: 0.3434 - val_accuracy: 0.8994 - lr: 3.3333e-04
Epoch 22/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1250 - accuracy: 0.9536 - val_loss: 0.3527 - val_accuracy: 0.9050 - lr: 1.1111e-04
Epoch 23/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1190 - accuracy: 0.9560 - val_loss: 0.3574 - val_accuracy: 0.9056 - lr: 1.1111e-04
Epoch 24/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1158 - accuracy: 0.9566 - val_loss: 0.3740 - val_accuracy: 0.9034 - lr: 1.1111e-04
Epoch 25/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1117 - accuracy: 0.9588 - val_loss: 0.3855 - val_accuracy: 0.9034 - lr: 1.1111e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_with_dropout.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model_with_dropout.evaluate(x_train, y_train))</span><br><span class="line">display(model_with_dropout.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_38_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 6s 3ms/step - loss: 0.1015 - accuracy: 0.9624

[0.10153431445360184, 0.9624181985855103]


313/313 [==============================] - 1s 3ms/step - loss: 0.4297 - accuracy: 0.8955

[0.4297321140766144, 0.8955000042915344]
</code></pre><h2 id="AlphaDropout-è‡ªå½’ä¸€åŒ–ç½‘ç»œ"><a href="#AlphaDropout-è‡ªå½’ä¸€åŒ–ç½‘ç»œ" class="headerlink" title="AlphaDropout (è‡ªå½’ä¸€åŒ–ç½‘ç»œ)"></a>AlphaDropout (è‡ªå½’ä¸€åŒ–ç½‘ç»œ)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">model_with_alpha_dropout = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    model_with_alpha_dropout.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    model_with_alpha_dropout.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line">    model_with_alpha_dropout.add(keras.layers.AlphaDropout(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">model_with_alpha_dropout.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">model_with_alpha_dropout.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                                 optimizer=optimizer, </span><br><span class="line">                                 metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¾“å…¥ç‰¹å¾æ ‡å‡†åŒ– (Î¼=0, Ïƒ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">3</span>, patience=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">history_with_alpha_dropout = model_with_alpha_dropout.fit(x_train_scaled, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                                          validation_data=(x_valid_scaled, y_valid),</span><br><span class="line">                                                          callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
430/430 [==============================] - 9s 18ms/step - loss: 0.6672 - accuracy: 0.7771 - val_loss: 0.5470 - val_accuracy: 0.8572 - lr: 0.0030
Epoch 2/25
430/430 [==============================] - 7s 17ms/step - loss: 0.4653 - accuracy: 0.8437 - val_loss: 0.5983 - val_accuracy: 0.8474 - lr: 0.0030
Epoch 3/25
430/430 [==============================] - 7s 17ms/step - loss: 0.4090 - accuracy: 0.8605 - val_loss: 0.5949 - val_accuracy: 0.8572 - lr: 0.0030
Epoch 4/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3917 - accuracy: 0.8655 - val_loss: 0.5147 - val_accuracy: 0.8720 - lr: 0.0030
Epoch 5/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3594 - accuracy: 0.8782 - val_loss: 0.4578 - val_accuracy: 0.8768 - lr: 0.0030
Epoch 6/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3398 - accuracy: 0.8837 - val_loss: 0.5777 - val_accuracy: 0.8672 - lr: 0.0030
Epoch 7/25
430/430 [==============================] - 8s 18ms/step - loss: 0.3399 - accuracy: 0.8849 - val_loss: 0.5091 - val_accuracy: 0.8790 - lr: 0.0030
Epoch 8/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3175 - accuracy: 0.8912 - val_loss: 0.5224 - val_accuracy: 0.8776 - lr: 0.0030
Epoch 9/25
430/430 [==============================] - 7s 17ms/step - loss: 0.3103 - accuracy: 0.8946 - val_loss: 0.5111 - val_accuracy: 0.8818 - lr: 0.0030
Epoch 10/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2435 - accuracy: 0.9147 - val_loss: 0.4527 - val_accuracy: 0.8938 - lr: 0.0010
Epoch 11/25
430/430 [==============================] - 8s 19ms/step - loss: 0.2196 - accuracy: 0.9221 - val_loss: 0.5114 - val_accuracy: 0.8952 - lr: 0.0010
Epoch 12/25
430/430 [==============================] - 7s 17ms/step - loss: 0.2074 - accuracy: 0.9256 - val_loss: 0.4812 - val_accuracy: 0.8868 - lr: 0.0010
Epoch 13/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1983 - accuracy: 0.9283 - val_loss: 0.5023 - val_accuracy: 0.8938 - lr: 0.0010
Epoch 14/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1901 - accuracy: 0.9319 - val_loss: 0.5339 - val_accuracy: 0.8906 - lr: 0.0010
Epoch 15/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1602 - accuracy: 0.9434 - val_loss: 0.5606 - val_accuracy: 0.8988 - lr: 3.3333e-04
Epoch 16/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1466 - accuracy: 0.9483 - val_loss: 0.5909 - val_accuracy: 0.8974 - lr: 3.3333e-04
Epoch 17/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1372 - accuracy: 0.9513 - val_loss: 0.6188 - val_accuracy: 0.8988 - lr: 3.3333e-04
Epoch 18/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1315 - accuracy: 0.9535 - val_loss: 0.6030 - val_accuracy: 0.8964 - lr: 3.3333e-04
Epoch 19/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1150 - accuracy: 0.9596 - val_loss: 0.6832 - val_accuracy: 0.9014 - lr: 1.1111e-04
Epoch 20/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1102 - accuracy: 0.9619 - val_loss: 0.7075 - val_accuracy: 0.8992 - lr: 1.1111e-04
Epoch 21/25
430/430 [==============================] - 7s 17ms/step - loss: 0.1056 - accuracy: 0.9629 - val_loss: 0.7323 - val_accuracy: 0.8990 - lr: 1.1111e-04
Epoch 22/25
430/430 [==============================] - 8s 17ms/step - loss: 0.1027 - accuracy: 0.9641 - val_loss: 0.7475 - val_accuracy: 0.8998 - lr: 1.1111e-04
Epoch 23/25
430/430 [==============================] - 7s 17ms/step - loss: 0.0959 - accuracy: 0.9673 - val_loss: 0.7682 - val_accuracy: 0.9000 - lr: 3.7037e-05
Epoch 24/25
430/430 [==============================] - 7s 17ms/step - loss: 0.0939 - accuracy: 0.9679 - val_loss: 0.7781 - val_accuracy: 0.9014 - lr: 3.7037e-05
Epoch 25/25
430/430 [==============================] - 7s 17ms/step - loss: 0.0921 - accuracy: 0.9686 - val_loss: 0.8001 - val_accuracy: 0.9000 - lr: 3.7037e-05
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_with_alpha_dropout.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model_with_alpha_dropout.evaluate(x_train_scaled, y_train))</span><br><span class="line">display(model_with_alpha_dropout.evaluate(x_test_scaled, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_43_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 6s 4ms/step - loss: 0.1080 - accuracy: 0.9704

[0.10797912627458572, 0.9703999757766724]


313/313 [==============================] - 1s 4ms/step - loss: 0.9345 - accuracy: 0.8960

[0.9344537258148193, 0.8960000276565552]
</code></pre><h1 id="ç»“è®º-1"><a href="#ç»“è®º-1" class="headerlink" title="ç»“è®º 1"></a>ç»“è®º 1</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Regularization</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Accu on train</th>
<th>Accu on test</th>
</tr>
</thead>
<tbody>
<tr>
<td>No Reg</td>
<td>7s/epoch</td>
<td>/</td>
<td>0.9661</td>
<td>0.8949</td>
</tr>
<tr>
<td>Batch Normalization</td>
<td>17s/epoch</td>
<td>/</td>
<td>0.9914</td>
<td>0.9016</td>
</tr>
<tr>
<td>EarlyStopping</td>
<td>7s/epoch</td>
<td>19 epochs</td>
<td>0.9382</td>
<td>0.8977</td>
</tr>
<tr>
<td>L1 Reg</td>
<td>8s/epoch</td>
<td>/</td>
<td>0.9666</td>
<td>0.8969</td>
</tr>
<tr>
<td>L2 Reg</td>
<td>8s/epoch</td>
<td>/</td>
<td>0.9660</td>
<td>0.8957</td>
</tr>
<tr>
<td>L1_L2 Reg</td>
<td>8s/epoch</td>
<td>/</td>
<td>0.9726</td>
<td>0.8947</td>
</tr>
<tr>
<td>Regular Dropout</td>
<td>7s/epoch</td>
<td>/</td>
<td>0.9624</td>
<td>0.8955</td>
</tr>
<tr>
<td>Alpha Dropout</td>
<td>7s/epoch</td>
<td>/</td>
<td>0.9704</td>
<td>0.8960</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>å•ç‹¬ä½¿ç”¨ Batch Normalization ä½œä¸ºæ­£åˆ™åŒ–æ‰‹æ®µ, ä¸å¯é¿å…åœ°<strong>å¤§å¹…é™ä½äº†è®­ç»ƒé€Ÿåº¦</strong>, ä½†ä¹Ÿè·å¾—äº†<strong>æ€§èƒ½æœ€ä½³çš„æ¨¡å‹</strong>.</li>
<li>å•ç‹¬ä½¿ç”¨ EarlyStopping, <strong>æ˜¾è‘—é™ä½äº†è®­ç»ƒé›†ä¸Šçš„ Accuracy</strong>, ä½†æ¨¡å‹æ€§èƒ½æå‡ç”šå¾®.</li>
<li>å•ç‹¬ä½¿ç”¨ L1 / L2 / L1_L2 Reg, ç›¸æ¯”æ— æ­£åˆ™åŒ–åŒºåˆ«ä¸å¤§.</li>
<li>ä»¥ä¸Šæ‰€æœ‰å¸¦æ­£åˆ™åŒ–çš„æ¨¡å‹ä»å­˜åœ¨è¿‡æ‹Ÿåˆç°è±¡.</li>
</ol>
<h1 id="ç»“åˆå¤šç§æ­£åˆ™åŒ–æŠ€æœ¯"><a href="#ç»“åˆå¤šç§æ­£åˆ™åŒ–æŠ€æœ¯" class="headerlink" title="ç»“åˆå¤šç§æ­£åˆ™åŒ–æŠ€æœ¯"></a>ç»“åˆå¤šç§æ­£åˆ™åŒ–æŠ€æœ¯</h1><h2 id="Regular-DNN"><a href="#Regular-DNN" class="headerlink" title="Regular DNN"></a>Regular DNN</h2><p>Early Stopping + Batch Normalization (å¯¹äºè¾ƒæ·±çš„ç½‘ç»œ) + l2 regularization (å¿…è¦æ—¶ä½¿ç”¨)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">regular_dnn = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.BatchNormalization()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    regular_dnn.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>, </span><br><span class="line">                                       kernel_regularizer=keras.regularizers.l2(<span class="number">3e-5</span>)))</span><br><span class="line">    regular_dnn.add(keras.layers.BatchNormalization())</span><br><span class="line"></span><br><span class="line">regular_dnn.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">regular_dnn.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">4</span>, patience=<span class="number">4</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">regular_history = regular_dnn.fit(x_train, y_train, epochs=<span class="number">25</span>*<span class="number">2</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                  validation_data=(x_valid, y_valid),</span><br><span class="line">                                  callbacks=[lr_scheduler, early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/50
430/430 [==============================] - 21s 40ms/step - loss: 0.5284 - accuracy: 0.8247 - val_loss: 0.3989 - val_accuracy: 0.8758 - lr: 0.0030
Epoch 2/50
430/430 [==============================] - 17s 40ms/step - loss: 0.4063 - accuracy: 0.8664 - val_loss: 0.3888 - val_accuracy: 0.8742 - lr: 0.0030
Epoch 3/50
430/430 [==============================] - 18s 41ms/step - loss: 0.3723 - accuracy: 0.8772 - val_loss: 0.4385 - val_accuracy: 0.8558 - lr: 0.0030
Epoch 4/50
430/430 [==============================] - 17s 40ms/step - loss: 0.3454 - accuracy: 0.8876 - val_loss: 0.3718 - val_accuracy: 0.8792 - lr: 0.0030
Epoch 5/50
430/430 [==============================] - 17s 40ms/step - loss: 0.3201 - accuracy: 0.8955 - val_loss: 0.3504 - val_accuracy: 0.8848 - lr: 0.0030
Epoch 6/50
430/430 [==============================] - 17s 40ms/step - loss: 0.3059 - accuracy: 0.8998 - val_loss: 0.3587 - val_accuracy: 0.8874 - lr: 0.0030
Epoch 7/50
430/430 [==============================] - 17s 40ms/step - loss: 0.2921 - accuracy: 0.9046 - val_loss: 0.3691 - val_accuracy: 0.8848 - lr: 0.0030
Epoch 8/50
430/430 [==============================] - 17s 40ms/step - loss: 0.2774 - accuracy: 0.9110 - val_loss: 0.3565 - val_accuracy: 0.8834 - lr: 0.0030
Epoch 9/50
430/430 [==============================] - 17s 40ms/step - loss: 0.2620 - accuracy: 0.9150 - val_loss: 0.3554 - val_accuracy: 0.8844 - lr: 0.0030
Epoch 10/50
430/430 [==============================] - 17s 40ms/step - loss: 0.1988 - accuracy: 0.9384 - val_loss: 0.3103 - val_accuracy: 0.9054 - lr: 7.5000e-04
Epoch 11/50
430/430 [==============================] - 18s 41ms/step - loss: 0.1763 - accuracy: 0.9460 - val_loss: 0.3156 - val_accuracy: 0.9052 - lr: 7.5000e-04
Epoch 12/50
430/430 [==============================] - 18s 42ms/step - loss: 0.1659 - accuracy: 0.9502 - val_loss: 0.3260 - val_accuracy: 0.8986 - lr: 7.5000e-04
Epoch 13/50
430/430 [==============================] - 17s 40ms/step - loss: 0.1558 - accuracy: 0.9528 - val_loss: 0.3301 - val_accuracy: 0.9026 - lr: 7.5000e-04
Epoch 14/50
430/430 [==============================] - 17s 40ms/step - loss: 0.1461 - accuracy: 0.9561 - val_loss: 0.3395 - val_accuracy: 0.9020 - lr: 7.5000e-04
Epoch 15/50
430/430 [==============================] - 17s 40ms/step - loss: 0.1184 - accuracy: 0.9670 - val_loss: 0.3509 - val_accuracy: 0.9022 - lr: 1.8750e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(regular_history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(regular_dnn.evaluate(x_train, y_train))</span><br><span class="line">display(regular_dnn.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_52_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 12s 7ms/step - loss: 0.1546 - accuracy: 0.9559

[0.15455804765224457, 0.9559454321861267]


313/313 [==============================] - 2s 7ms/step - loss: 0.3264 - accuracy: 0.9036

[0.32639697194099426, 0.9035999774932861]
</code></pre><h2 id="Self-Normalize-DNN"><a href="#Self-Normalize-DNN" class="headerlink" title="Self-Normalize DNN"></a>Self-Normalize DNN</h2><p>Early Stopping + Alpha Dropout</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">self_norm_dnn = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    self_norm_dnn.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    self_norm_dnn.add(keras.layers.Dense(<span class="number">80</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line">    self_norm_dnn.add(keras.layers.AlphaDropout(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">self_norm_dnn.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Nadam(learning_rate=<span class="number">0.003</span>)</span><br><span class="line">self_norm_dnn.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¾“å…¥ç‰¹å¾æ ‡å‡†åŒ– (Î¼=0, Ïƒ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=<span class="number">1</span>/<span class="number">4</span>, patience=<span class="number">4</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">self_norm_history = self_norm_dnn.fit(x_train_scaled, y_train, epochs=<span class="number">25</span>*<span class="number">2</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">                                      validation_data=(x_valid_scaled, y_valid),</span><br><span class="line">                                      callbacks=[lr_scheduler, early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/50
430/430 [==============================] - 9s 19ms/step - loss: 0.6619 - accuracy: 0.7798 - val_loss: 0.5637 - val_accuracy: 0.8402 - lr: 0.0030
Epoch 2/50
430/430 [==============================] - 8s 18ms/step - loss: 0.4664 - accuracy: 0.8421 - val_loss: 0.5877 - val_accuracy: 0.8522 - lr: 0.0030
Epoch 3/50
430/430 [==============================] - 8s 18ms/step - loss: 0.4183 - accuracy: 0.8574 - val_loss: 0.5682 - val_accuracy: 0.8634 - lr: 0.0030
Epoch 4/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3946 - accuracy: 0.8657 - val_loss: 0.5031 - val_accuracy: 0.8706 - lr: 0.0030
Epoch 5/50
430/430 [==============================] - 7s 17ms/step - loss: 0.3664 - accuracy: 0.8742 - val_loss: 0.4748 - val_accuracy: 0.8760 - lr: 0.0030
Epoch 6/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3489 - accuracy: 0.8811 - val_loss: 0.5445 - val_accuracy: 0.8758 - lr: 0.0030
Epoch 7/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3360 - accuracy: 0.8865 - val_loss: 0.5468 - val_accuracy: 0.8716 - lr: 0.0030
Epoch 8/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3409 - accuracy: 0.8835 - val_loss: 0.4911 - val_accuracy: 0.8716 - lr: 0.0030
Epoch 9/50
430/430 [==============================] - 8s 18ms/step - loss: 0.3090 - accuracy: 0.8939 - val_loss: 0.5311 - val_accuracy: 0.8780 - lr: 0.0030
Epoch 10/50
430/430 [==============================] - 8s 18ms/step - loss: 0.2407 - accuracy: 0.9152 - val_loss: 0.4742 - val_accuracy: 0.8926 - lr: 7.5000e-04
Epoch 11/50
430/430 [==============================] - 8s 18ms/step - loss: 0.2180 - accuracy: 0.9215 - val_loss: 0.5036 - val_accuracy: 0.8944 - lr: 7.5000e-04
Epoch 12/50
430/430 [==============================] - 8s 18ms/step - loss: 0.2064 - accuracy: 0.9260 - val_loss: 0.4891 - val_accuracy: 0.8920 - lr: 7.5000e-04
Epoch 13/50
430/430 [==============================] - 8s 18ms/step - loss: 0.1953 - accuracy: 0.9291 - val_loss: 0.5147 - val_accuracy: 0.8980 - lr: 7.5000e-04
Epoch 14/50
430/430 [==============================] - 8s 18ms/step - loss: 0.1871 - accuracy: 0.9316 - val_loss: 0.5644 - val_accuracy: 0.8958 - lr: 7.5000e-04
Epoch 15/50
430/430 [==============================] - 8s 18ms/step - loss: 0.1595 - accuracy: 0.9420 - val_loss: 0.5549 - val_accuracy: 0.8994 - lr: 1.8750e-04
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(self_norm_history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(self_norm_dnn.evaluate(x_train_scaled, y_train))</span><br><span class="line">display(self_norm_dnn.evaluate(x_test_scaled, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E5%90%84%E7%A7%8D%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8B%E9%97%B4%E7%9A%84%E5%AF%B9%E6%AF%94/output_58_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 6s 3ms/step - loss: 0.2535 - accuracy: 0.9248

[0.25346049666404724, 0.9247636198997498]


313/313 [==============================] - 1s 3ms/step - loss: 0.5312 - accuracy: 0.8911

[0.5311788320541382, 0.8910999894142151]
</code></pre><h1 id="ç»“è®º-2"><a href="#ç»“è®º-2" class="headerlink" title="ç»“è®º 2"></a>ç»“è®º 2</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Regularization</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Accu on train</th>
<th>Accu on test</th>
</tr>
</thead>
<tbody>
<tr>
<td>No Reg</td>
<td>7s/epoch</td>
<td>/</td>
<td>0.9661</td>
<td>0.8949</td>
</tr>
<tr>
<td>EarlyStopping + Batch Normalization + L2 Reg</td>
<td>18s/epoch</td>
<td>10 epochs</td>
<td>0.9559</td>
<td>0.9036</td>
</tr>
<tr>
<td>EarlyStopping + Alpha Dropout</td>
<td>8s/epoch</td>
<td>10 epochs</td>
<td>0.9248</td>
<td>0.8911</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>å¯¹äºéè‡ªå½’ä¸€åŒ–ç½‘ç»œ, ä½¿ç”¨ EarlyStopping + Batch Normalization + L2 Reg è¿›è¡Œæ­£åˆ™åŒ–, ç”±äº BN å±‚çš„å­˜åœ¨, å¿…ç„¶å¤§å¹…é™ä½è®­ç»ƒé€Ÿåº¦, ä½†<strong>æ”¶æ•›é€Ÿåº¦æå‡</strong>; è€Œ EarlyStopping èŠ‚çº¦äº†ä¸å¿…è¦çš„è®¡ç®—; æœ€ç»ˆ: ç»“åˆå¤šç§æ­£åˆ™åŒ–æŠ€æœ¯, è¾¾åˆ°äº†æ¨¡å‹çš„<strong>æœ€ä½³æ€§èƒ½</strong>.</li>
<li>å¯¹äºè‡ªå½’ä¸€åŒ–ç½‘ç»œ, ä½¿ç”¨ EarlyStopping + Alpha Dropout è¿›è¡Œæ­£åˆ™åŒ–, <strong>è®­ç»ƒé€Ÿåº¦å¾—ä»¥ä¿æŒåœ¨è¾ƒå¿«æ°´å¹³</strong>, ä¸”æ”¶æ•›é€Ÿåº¦æå‡; EarlyStopping èŠ‚çº¦äº†ä¸å¿…è¦çš„è®¡ç®—; <strong>è®­ç»ƒé›†ä¸Šçš„ Accuracy æ˜¾è‘—é™ä½</strong>, ä½†æœ€ç»ˆæ¨¡å‹æ€§èƒ½ä¸ç”šç†æƒ³(å¯èƒ½éœ€è¦å¾®è°ƒå…¶ä»–è¶…å‚æ•°ä»¥å‘æŒ¥è¯¥æ¨¡å‹çš„çœŸæ­£æ€§èƒ½).</li>
</ol>
<p>ğŸ’ å¯ä»¥æ”¹è¿›çš„ç‚¹: ä»¥ä¸Šå„ç§æ¨¡å‹éƒ½æ¶‰åŠå¤§é‡è¶…å‚æ•°, è€Œæµ‹è¯•ä¸­çš„é…ç½®å¹¶éæœ€ä¼˜è¶…å‚æ•°ç»„åˆ, å› æ­¤æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½æ¯”è¾ƒæœªå¿…å‡†ç¡®. å¦å¤–, ä½¿ç”¨ Alpha Dropout çš„ä¸¤ä¸ªæ¨¡å‹çš„éªŒè¯è¯¯å·®éƒ½å¾ˆé«˜, ä½† accuracy å´æ²¡æœ‰å¤§å¹…ä¸‹é™â€¦å…¶åŸå› å°šæœªæ‰¾åˆ°â€¦</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Generative Adversarial Networks)</title>
    <url>/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/</url>
    <content><![CDATA[<p><strong>Generative Adversarial Networks</strong></p>
<p>GAN ç”±ä¸¤ä¸ªç¥ç»ç½‘ç»œç»„æˆ:</p>
<ol>
<li><p><strong>ç”Ÿæˆå™¨</strong>: æ¥å—ä¸€ä¸ªéšæœºåˆ†å¸ƒ(ä¸€èˆ¬ä¸ºé«˜æ–¯åˆ†å¸ƒ), å¹¶è¾“å‡ºä¸€äº›æ•°æ®.</p>
</li>
<li><p><strong>åˆ¤åˆ«å™¨</strong>: åˆ¤æ–­æ¥æ”¶çš„æ•°æ®æ˜¯è®­ç»ƒé›†ä¸­çš„ â€œçœŸå®æ•°æ®â€, è¿˜æ˜¯ç”±ç”Ÿæˆå™¨ç”Ÿæˆçš„ â€œå‡æ•°æ®â€.</p>
<span id="more"></span>
</li>
</ol>
<p>ä¸‹å›¾æ˜¯ä¸€ä¸ª GAN æ¶æ„çš„ç¤ºä¾‹.</p>
<p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/GAN.PNG" width="80%"></p>
<p>è®­ç»ƒæœŸé—´, generator ä¸ discriminator çš„ç›®æ ‡ç›¸å: </p>
<ol>
<li>discriminator è¯•å›¾è¯†åˆ«å‡º â€œå‡å›¾ç‰‡â€.</li>
<li>generator è¯•å›¾äº§ç”Ÿè¶³å¤Ÿé€¼çœŸçš„ â€œå‡å›¾ç‰‡â€ ä»¥éª—è¿‡åˆ¤åˆ«å™¨.</li>
</ol>
<hr>
<p>GAN çš„æ¯ä¸ª training iteration æœ‰ä¸¤ä¸ªé˜¶æ®µ:</p>
<ol>
<li><strong>ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ discriminator</strong>:<br>ä»è®­ç»ƒé›†ä¸­é‡‡æ ·ä¸€æ‰¹ â€œçœŸå›¾åƒâ€(æ ‡ç­¾ä¸º 1), å†ç”± generator äº§ç”ŸåŒæ ·æ•°é‡çš„ â€œå‡å›¾åƒâ€(æ ‡ç­¾ä¸º 0). ç„¶å discriminator ä½¿ç”¨ binary cross-entropy loss åœ¨è¿™ä¸ªçœŸå‡å‚åŠçš„å›¾åƒæ‰¹æ¬¡ä¸Šè®­ç»ƒä¸€ä¸ª step. <strong>è¿™æœŸé—´åå‘ä¼ æ’­ä»…ä¼˜åŒ– discriminator çš„æƒé‡</strong>. </li>
</ol>
<ol>
<li><strong>ç¬¬äºŒé˜¶æ®µè®­ç»ƒ generator</strong>:<br>é¦–å…ˆè®© generator äº§ç”Ÿä¸€æ‰¹ â€œå‡å›¾åƒâ€(<strong>å°†æ ‡ç­¾è®¾ç½®ä¸º 1</strong>), å¹¶è®© discriminator åŒºåˆ†å…¶çœŸå‡ (è¿™æ¬¡æ²¡æœ‰ â€œçœŸå›¾åƒâ€). è¿™é‡Œæ•…æ„å°†æ ‡ç­¾è®¾ç½®ä¸º 1 æ˜¯ä¸ºäº†è®© generator äº§ç”Ÿä½¿ discriminator ä¿¡ä»¥ä¸ºçœŸçš„å›¾åƒ. <strong>è¿™æœŸé—´åå‘ä¼ æ’­ä»…ä¼˜åŒ– generator çš„æƒé‡ (discriminator çš„æƒé‡è¢«å†»ç»“).</strong></li>
</ol>
<p>ä¸‹é¢åŸºäº Fashion MNIST æ¥æ„å»ºä¸€ä¸ªç®€å•çš„ GAN.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<h2 id="åŠ è½½-amp-åˆ’åˆ†æ•°æ®é›†"><a href="#åŠ è½½-amp-åˆ’åˆ†æ•°æ®é›†" class="headerlink" title="åŠ è½½ &amp; åˆ’åˆ†æ•°æ®é›†"></a>åŠ è½½ &amp; åˆ’åˆ†æ•°æ®é›†</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line">X_train_full = X_train_full.astype(np.float32) / <span class="number">255</span></span><br><span class="line">X_test = X_test.astype(np.float32) / <span class="number">255</span></span><br><span class="line">X_train, X_valid = X_train_full[:-<span class="number">5000</span>], X_train_full[-<span class="number">5000</span>:]</span><br><span class="line">y_train, y_valid = y_train_full[:-<span class="number">5000</span>], y_train_full[-<span class="number">5000</span>:]</span><br></pre></td></tr></table></figure>
<h2 id="æ„å»ºç”Ÿæˆå™¨"><a href="#æ„å»ºç”Ÿæˆå™¨" class="headerlink" title="æ„å»ºç”Ÿæˆå™¨"></a>æ„å»ºç”Ÿæˆå™¨</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">codings_size = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">generator = keras.models.Sequential([</span><br><span class="line">    <span class="comment"># generator æ¥æ”¶çš„è¾“å…¥ç±»ä¼¼äº autoencoder ä¸­ decoder éƒ¨åˆ†æ¥æ”¶çš„è¾“å…¥, å³ &quot;æ½œåœ¨ç¼–ç &quot;.</span></span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, input_shape=[codings_size]), </span><br><span class="line">    keras.layers.Dense(<span class="number">150</span>, activation=<span class="string">&quot;elu&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">28</span> * <span class="number">28</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    keras.layers.Reshape([<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 id="æ„å»ºåˆ¤åˆ«å™¨"><a href="#æ„å»ºåˆ¤åˆ«å™¨" class="headerlink" title="æ„å»ºåˆ¤åˆ«å™¨"></a>æ„å»ºåˆ¤åˆ«å™¨</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">discriminator = keras.models.Sequential([</span><br><span class="line">    <span class="comment"># discriminator æ¥æ”¶çš„è¾“å…¥å°±æ˜¯çœŸ/å‡å›¾åƒ</span></span><br><span class="line">    keras.layers.Flatten(input_shape=[<span class="number">28</span>, <span class="number">28</span>]),</span><br><span class="line">    keras.layers.Dense(<span class="number">150</span>, activation=<span class="string">&quot;elu&quot;</span>), </span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)    <span class="comment"># è¯¥æ¿€æ´»å¯¹åº”å›¾åƒäºŒåˆ†ç±»</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 id="ç»„åˆä¸º-GAN"><a href="#ç»„åˆä¸º-GAN" class="headerlink" title="ç»„åˆä¸º GAN"></a>ç»„åˆä¸º GAN</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gan = keras.models.Sequential([generator, discriminator])</span><br></pre></td></tr></table></figure>
<h2 id="ç¼–è¯‘æ¨¡å‹"><a href="#ç¼–è¯‘æ¨¡å‹" class="headerlink" title="ç¼–è¯‘æ¨¡å‹"></a>ç¼–è¯‘æ¨¡å‹</h2><ol>
<li>ç”±äº discriminator æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»å™¨, æ•…å¯ç”¨ binary cross-entropy ä½œä¸ºå…¶æŸå¤±å‡½æ•°.</li>
<li>ğŸ”º generator å°†åªé€šè¿‡æ•´ä¸ª GAN æ¨¡å‹æ¥è®­ç»ƒ, å› æ­¤æ— éœ€å•ç‹¬ç¼–è¯‘å®ƒ. ğŸ”º</li>
<li>æ•´ä¸ª GAN æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»å™¨, æ•…ä¹Ÿå¯ç”¨ binary cross-entropy ä½œä¸ºå…¶æŸå¤±å‡½æ•°.</li>
<li>é‡è¦çš„æ˜¯, ç¬¬äºŒé˜¶æ®µåªè®­ç»ƒ generator çš„æƒé‡, å› æ­¤åœ¨ç¼–è¯‘ GAN æ¨¡å‹å‰å°† discriminator è®¾ç½®ä¸ºä¸å¯è®­ç»ƒçš„.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">discriminator.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&quot;rmsprop&quot;</span>)</span><br><span class="line">discriminator.trainable = <span class="literal">False</span>    <span class="comment"># å¯¹åº”ä¸Šé¢ç¬¬å››æ¡</span></span><br><span class="line">    </span><br><span class="line">gan.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&quot;rmsprop&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Remark</strong> on <code>discriminator.trainable = False</code>: </p>
<ol>
<li>å½“å¯¹ discriminator è°ƒç”¨ fit() æˆ– train_on_batch() æ—¶, discriminator æ˜¯å¯è®­ç»ƒçš„; </li>
<li>å½“å¯¹ gan model     è°ƒç”¨ fit() æˆ– train_on_batch() æ—¶, discriminator æ˜¯ä¸å¯è®­ç»ƒçš„.</li>
</ol>
<h2 id="è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯"><a href="#è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯" class="headerlink" title="è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯"></a>è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯</h2><h3 id="é¦–å…ˆåˆ›å»º-Dataset"><a href="#é¦–å…ˆåˆ›å»º-Dataset" class="headerlink" title="é¦–å…ˆåˆ›å»º Dataset"></a>é¦–å…ˆåˆ›å»º Dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ä» X_train åˆ›å»º Dataset å¹¶ä¹±åº + åˆ†æ‰¹ + é¢„å– </span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(<span class="number">1000</span>)</span><br><span class="line">dataset = dataset.batch(batch_size=batch_size, drop_remainder=<span class="literal">True</span>).prefetch(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="å®šä¹‰è®­ç»ƒå¾ªç¯"><a href="#å®šä¹‰è®­ç»ƒå¾ªç¯" class="headerlink" title="å®šä¹‰è®­ç»ƒå¾ªç¯"></a>å®šä¹‰è®­ç»ƒå¾ªç¯</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiple_images</span>(<span class="params">images, n_cols=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># ç”¨äºç»˜å›¾ã®è¾…åŠ©å‡½æ•°</span></span><br><span class="line">    n_cols = n_cols <span class="keyword">or</span> <span class="built_in">len</span>(images)</span><br><span class="line">    n_rows = (<span class="built_in">len</span>(images) - <span class="number">1</span>) // n_cols + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> images.shape[-<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        images = np.squeeze(images, axis=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    plt.figure(figsize=(n_cols * <span class="number">1.5</span>, n_rows * <span class="number">1.5</span>))</span><br><span class="line">    <span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        plt.subplot(n_rows, n_cols, index + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_gan</span>(<span class="params">gan, dataset, batch_size, codings_size, n_epochs=<span class="number">10</span></span>):</span></span><br><span class="line">    generator, discriminator = gan.layers</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;/&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>, n_epochs))</span><br><span class="line">        <span class="keyword">for</span> X_batch <span class="keyword">in</span> dataset:</span><br><span class="line">            <span class="comment"># é˜¶æ®µ 1: è®­ç»ƒåˆ¤åˆ«å™¨</span></span><br><span class="line">            noise = tf.random.normal(shape=[batch_size, codings_size])</span><br><span class="line">            generated_images = generator(noise)</span><br><span class="line">            X_fake_and_real = tf.concat([generated_images, X_batch], axis=<span class="number">0</span>)</span><br><span class="line">            y1 = tf.constant([[<span class="number">0.</span>]] * batch_size + [[<span class="number">1.</span>]] * batch_size)</span><br><span class="line">            discriminator.trainable = <span class="literal">True</span>     <span class="comment"># é¿å… warnings</span></span><br><span class="line">            discriminator.train_on_batch(X_fake_and_real, y1)       </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># é˜¶æ®µ 2: è®­ç»ƒç”Ÿæˆå™¨</span></span><br><span class="line">            noise = tf.random.normal(shape=[batch_size, codings_size])</span><br><span class="line">            y2 = tf.constant([[<span class="number">1.</span>]] * batch_size)</span><br><span class="line">            discriminator.trainable = <span class="literal">False</span>    <span class="comment"># é¿å… warnings</span></span><br><span class="line">            gan.train_on_batch(noise, y2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># æ¯ä¸ªè½®æ¬¡ç»“æŸæ—¶å±•ç¤ºç”Ÿæˆçš„å›¾åƒ</span></span><br><span class="line">        plot_multiple_images(generated_images, <span class="number">8</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<p><strong>Remark</strong> on previous code:</p>
<ol>
<li>åœ¨è®­ç»ƒçš„ç¬¬ä¸€é˜¶æ®µ, å°†é«˜æ–¯å™ªå£°ä¼ é€’ç»™ generator ä»¥äº§ç”Ÿ â€œå‡å›¾åƒâ€, åŒæ—¶åœ¨è®­ç»ƒé›†ä¸­é‡‡æ ·ç›¸åŒæ•°é‡çš„ â€œçœŸå›¾åƒâ€, å°†çœŸå‡å›¾åƒåˆå¹¶ä¸ºä¸€ä¸ªæ‰¹æ¬¡. è¿™ä¸ªæ‰¹æ¬¡å¯¹åº”çš„æ ‡ç­¾ $y1$ ä¸­ 0 ä»£è¡¨ â€œå‡å›¾åƒâ€, 1 ä»£è¡¨ â€œçœŸå›¾åƒâ€. ç„¶ååœ¨è¿™ä¸ª<strong>çœŸå‡å‚åŠçš„å›¾åƒæ‰¹æ¬¡ä¸Š</strong>è®­ç»ƒ discriminator.</li>
</ol>
<ol>
<li>åœ¨è®­ç»ƒçš„ç¬¬äºŒé˜¶æ®µ, å°†é«˜æ–¯å™ªå£°ä¼ é€’ç»™ gan æ¨¡å‹, å…¶ generator å°†äº§ç”Ÿä¸€äº› â€œå‡å›¾åƒâ€, ç„¶åå…¶ discriminator å°†å°è¯•åˆ†è¾¨è¿™äº›å›¾åƒçš„çœŸå‡. è¿™æ‰¹ â€œå‡å›¾åƒâ€ çš„æ ‡ç­¾ $y2$ è¢«è®¾ç½®ä¸º 1 (å³è¡¨ç¤º â€œçœŸå›¾åƒâ€), è¿™æ˜¯ä¸ºäº†<strong>è®© discriminator è¯¯ä»¥ä¸ºå®ƒä»¬æ˜¯è®­ç»ƒé›†ä¸­é‡‡æ ·çš„ â€œçœŸå›¾åƒâ€</strong>.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_gan(gan, dataset, batch_size, codings_size, n_epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/1
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_24_1.png" alt="png"></p>
<p><strong>Remark</strong> on the figure above:<br>ç”±å›¾å¯è§, ç”Ÿæˆå›¾åƒå·²ç»å¼€å§‹ä¸ Fashion MNIST ä¸­çš„å›¾åƒç›¸ä¼¼äº†, ä½†é—æ†¾çš„æ˜¯, generator å¹¶ä¸èƒ½äº§ç”Ÿæ¯”è¿™æ›´(æ˜æ˜¾åœ°)å¥½çš„å›¾åƒäº†â€¦</p>
<h2 id="è®­ç»ƒ-GAN-æ—¶çš„å›°éš¾-amp-æŒ‘æˆ˜"><a href="#è®­ç»ƒ-GAN-æ—¶çš„å›°éš¾-amp-æŒ‘æˆ˜" class="headerlink" title="è®­ç»ƒ GAN æ—¶çš„å›°éš¾ &amp; æŒ‘æˆ˜"></a>è®­ç»ƒ GAN æ—¶çš„å›°éš¾ &amp; æŒ‘æˆ˜</h2><ol>
<li>äº‹å®ä¸Š, GAN åªèƒ½è¾¾åˆ°å”¯ä¸€çš„çº³ä»€å‡è¡¡, å³å½“ generator äº§ç”Ÿ â€œå®Œå…¨é€¼çœŸâ€ çš„å›¾åƒ, è€Œ discriminator ä¸å¾—ä¸å¯¹æ¯å¼ æ¥æ”¶åˆ°çš„å›¾åƒè¿›è¡Œå•çº¯çš„çŒœæµ‹(50% æ­£ç¡®ç‡)çš„æƒ…å†µ.</li>
</ol>
<ol>
<li>ç„¶è€Œ, è¿™ç§å‡è¡¡çŠ¶æ€èƒ½å¦è¾¾åˆ°æ ¹æœ¬æ²¡æœ‰ä¿è¯.</li>
</ol>
<hr>
<ol>
<li>æœ€å¤§çš„å›°éš¾åœ¨äº <strong>æ¨¡å¼å´©æºƒ(mode collapse)</strong> çš„å‘ç”Ÿ, å³ generator çš„è¾“å‡ºå˜å¾—ä¸å†å¤šæ ·åŒ–. ä¸¾ä¾‹æ¥è¯´, å‡è®¾ generator èƒ½å¤Ÿæ›´å¥½åœ°ç”Ÿæˆé€¼çœŸçš„é‹å­çš„å›¾åƒ, ä¸ºäº†éª—è¿‡ discriminator, å®ƒå°±ä¼šäº§ç”Ÿæ›´å¤šé‹å­çš„å›¾åƒ. æ…¢æ…¢åœ°, generator å°±ä¼šå¿˜è®°å¦‚ä½•ç”Ÿæˆå…¶å®ƒç±»åˆ«çš„å›¾åƒäº†; åŒæ—¶, discriminator åªä¼šæ¥æ”¶åˆ°å‡çš„é‹å­çš„å›¾åƒ, äºæ˜¯å®ƒä¹Ÿä¼šé€æ¸å¿˜è®°å¦‚ä½•åŒºåˆ†å…¶å®ƒç±»åˆ«çš„å‡å›¾åƒ. æœ€ç»ˆ, å½“ discriminator å­¦ä¼šåŒºåˆ†çœŸå‡é‹å­çš„å›¾åƒå, generator åˆè¢«è¿«è½¬è€Œå»ç”Ÿæˆå…¶å®ƒç±»åˆ«çš„å›¾åƒ(æ¯”å¦‚è¯´è¿è¡£è£™)è€Œå¿˜è®°äº†å¦‚ä½•ç”Ÿæˆé€¼çœŸçš„é‹å­çš„å›¾åƒ, ç„¶å discriminator ä¹Ÿè·Ÿç€åªä¼šåŒºåˆ†çœŸå‡è¿è¡£è£™çš„å›¾åƒâ€¦æœ€ç»ˆçš„ç»“æœæ˜¯, GAN é€æ¸åœ¨å°‘æ•°å‡ ä¸ªç±»åˆ«ä¸­å¾ªç¯å¾€å¤, ä½†æ¯ä¸€ä¸ªç±»åˆ«éƒ½ä¸æ“…é•¿.</li>
</ol>
<ol>
<li>å¦å¤–, GAN å¯¹è¶…å‚æ•°éå¸¸æ•æ„Ÿ, å¯èƒ½éœ€è¦èŠ±è´¹å¤§é‡ç²¾åŠ›æ¥è°ƒå‚.</li>
</ol>
<ol>
<li>è§£å†³è¿™äº›éš¾é¢˜çš„æ–¹æ³•æœ‰: <strong>experience replay</strong> å’Œ <strong>mini-batch discrimination</strong> ç­‰.</li>
</ol>
<h2 id="Deep-Convolutional-GANs-DCGANs"><a href="#Deep-Convolutional-GANs-DCGANs" class="headerlink" title="Deep Convolutional GANs (DCGANs)"></a>Deep Convolutional GANs (DCGANs)</h2><p>å¦‚ä½•æ„å»ºç¨³å®šçš„ convolutional GANs? è§ä¸‹è¿°æŒ‡å—:</p>
<ol>
<li><p>å°† discriminator ä¸­çš„æ± åŒ–å±‚æ›¿æ¢ä¸º <strong>strided Convolutions</strong>; å°† generator ä¸­çš„æ± åŒ–å±‚æ›¿æ¢ä¸º <strong>transposed Convolutions</strong>.</p>
</li>
<li><p>åœ¨ generator å’Œ discriminator ä¸­ä½¿ç”¨ <strong>Batch Normalization</strong> (ä½† generator çš„è¾“å‡ºå±‚å’Œ discriminator çš„è¾“å…¥å±‚é™¤å¤–).</p>
</li>
<li><p>å¯¹è¾ƒæ·±çš„ç½‘ç»œæ¶æ„, ç§»é™¤å…¨è¿æ¥çš„éšè—å±‚.</p>
</li>
<li><p>åœ¨ generator çš„æ‰€æœ‰å±‚ä¸­ä½¿ç”¨ <strong>ReLU</strong> æ¿€æ´», ä½†å…¶è¾“å‡ºå±‚ä½¿ç”¨ <strong>tanh</strong> æ¿€æ´».</p>
</li>
<li><p>åœ¨ discriminator çš„æ‰€æœ‰å±‚ä¸­ä½¿ç”¨ <strong>Leaky ReLU</strong> æ¿€æ´», ä½†å…¶è¾“å‡ºå±‚ä½¿ç”¨ <strong>sigmoid</strong> æ¿€æ´».</p>
</li>
</ol>
<p><strong>Remark</strong>: </p>
<ol>
<li>ä¸Šè¿°æŒ‡å—åœ¨å¤šæ•°æƒ…å†µä¸‹èƒ½å¸®åŠ©æ„å»ºä¸€ä¸ªç¨³å®šçš„ DCGAN, ä½†ä¸–äº‹æ— ç»å¯¹, ä½ ä»å¯èƒ½éœ€è¦å°è¯•ä¸åŒçš„è¶…å‚æ•°å€¼; </li>
<li>æœ‰æ—¶å€™, ä»…ä»…æ”¹å˜éšæœºç§å­å¹¶é‡æ–°è®­ç»ƒå®Œå…¨ç›¸åŒçš„æ¨¡å‹ä¹Ÿå¯èƒ½æˆåŠŸ.</li>
</ol>
<p>ä¸‹é¢åŸºäº Fashion MNIST æ„å»ºä¸€ä¸ª DCGAN ä½œä¸ºæ¼”ç¤º:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºç”Ÿæˆå™¨</span></span><br><span class="line">codings_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">generator = keras.models.Sequential([</span><br><span class="line">    keras.layers.Dense(<span class="number">7</span> * <span class="number">7</span> * <span class="number">128</span>, input_shape=[codings_size]), </span><br><span class="line">    keras.layers.Reshape([<span class="number">7</span>, <span class="number">7</span>, <span class="number">128</span>]), </span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    keras.layers.Conv2DTranspose(<span class="number">64</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;selu&quot;</span>), </span><br><span class="line">    keras.layers.BatchNormalization(), </span><br><span class="line">    keras.layers.Conv2DTranspose(<span class="number">1</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;same&quot;</span>, activation=<span class="string">&quot;tanh&quot;</span>) </span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.æ„å»ºåˆ¤åˆ«å™¨</span></span><br><span class="line">discriminator = keras.models.Sequential([</span><br><span class="line">    keras.layers.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;same&quot;</span>, </span><br><span class="line">                        activation=keras.layers.LeakyReLU(<span class="number">0.2</span>), input_shape=[<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.4</span>), </span><br><span class="line">    keras.layers.Conv2D(<span class="number">128</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">&quot;same&quot;</span>, </span><br><span class="line">                        activation=keras.layers.LeakyReLU(<span class="number">0.2</span>)),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.4</span>), </span><br><span class="line">    keras.layers.Flatten(),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)  </span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.ä¸¤è€…ç»“åˆå¾—åˆ° GAN</span></span><br><span class="line">gan = keras.models.Sequential([generator, discriminator])</span><br></pre></td></tr></table></figure>
<p><strong>Remark</strong> on the code above:</p>
<ol>
<li>generator æ¥æ”¶ 100 ç»´çš„ç¼–ç å‘é‡, å¹¶å°†å…¶æ˜ å°„ä¸º (7, 7, 128) çš„ä¸‰é˜¶å¼ é‡, è¯¥å¼ é‡ç»è¿‡æ‰¹é‡å½’ä¸€åŒ–åä¼ é€’è‡³ä¸€ä¸ªè½¬ç½®å·ç§¯å±‚(å°†å›¾åƒçš„ç©ºé—´å°ºåº¦ä¸Šé‡‡æ ·è‡³ 14 <em> 14, åŒæ—¶å°†æ·±åº¦é™è‡³ 64). å…¶ç»“æœå†ç»æ‰¹é‡å½’ä¸€åŒ–åè¢«ä¼ é€’è‡³ç¬¬äºŒä¸ªè½¬ç½®å·ç§¯å±‚(ç©ºé—´å°ºåº¦ä¸Šé‡‡æ ·è‡³ 28 </em> 28, æ·±åº¦é™è‡³ 1), è¯¥å±‚ä½¿ç”¨ tanh æ¿€æ´», æ•…å…¶è¾“å‡ºåœ¨ [-1, 1] ä¸­, å› æ­¤åœ¨è®­ç»ƒ GAN å‰éœ€å°†è®­ç»ƒé›†ç¼©æ”¾è‡³ [-1, 1] å†…. å¦å¤–è¿˜éœ€è¦å¯¹è®­ç»ƒé›†åš reshape ä»¥æ·»åŠ  channel ç»´åº¦.</li>
</ol>
<ol>
<li>discriminator ä¸é€šå¸¸çš„äºŒåˆ†ç±» CNN ç›¸ä¼¼, ä½†å…¶ä¸‹é‡‡æ ·åˆ™é€šè¿‡ strides=2 çš„å·ç§¯å±‚å®ç°(è€Œéæœ€å¤§æ± åŒ–å±‚). å¦å¤–, å·ç§¯å±‚ä½¿ç”¨äº† Leaky ReLU æ¿€æ´».</li>
</ol>
<ol>
<li>ä¸Šé¢çš„ä»£ç æ²¡æœ‰å®Œå…¨éµç…§æŒ‡å—, ä»¥é¿å…è®­ç»ƒä¸ç¨³å®šçš„æƒ…å†µ.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ·»åŠ  channel ç»´åº¦å¹¶ç¼©æ”¾è‡³ [-1, 1]</span></span><br><span class="line">X_train_dcgan = X_train.reshape(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>) * <span class="number">2.</span> - <span class="number">1.</span> </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘ä¸ [5] ä¸­ä¸€è‡´</span></span><br><span class="line">discriminator.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&quot;rmsprop&quot;</span>)</span><br><span class="line">discriminator.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">gan.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&quot;rmsprop&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åˆ›å»º Dataset ä¸ [6.1] ä¸­ä¸€è‡´</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)</span><br><span class="line">dataset = dataset.shuffle(<span class="number">1000</span>)</span><br><span class="line">dataset = dataset.batch(batch_size, drop_remainder=<span class="literal">True</span>).prefetch(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è®­ç»ƒ 50 ä¸ªè½®æ¬¡ (è®­ç»ƒæ–¹æ³•ä¸ [6.2] ä¸­ä¸€è‡´)</span></span><br><span class="line">train_gan(gan, dataset, batch_size, codings_size, n_epochs=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_1.png" alt="png"></p>
<pre><code>Epoch 2/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_3.png" alt="png"></p>
<pre><code>Epoch 3/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_5.png" alt="png"></p>
<p>â€¦â€¦</p>
<pre><code>Epoch 48/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_95.png" alt="png"></p>
<pre><code>Epoch 49/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_97.png" alt="png"></p>
<pre><code>Epoch 50/50
</code></pre><p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/output_37_99.png" alt="png"></p>
<p><strong>Remark</strong>:</p>
<ol>
<li>å¯ä»¥çœ‹åˆ°, ç»è¿‡ 50 ä¸ªè½®æ¬¡çš„è®­ç»ƒ, DCGAN å·²ç»èƒ½ç”Ÿæˆä¸ Fashion MNIST ä¸­è¾ƒä¸ºç›¸ä¼¼çš„å›¾åƒäº†, ä½†ä»æœ‰éƒ¨åˆ†å›¾åƒéš¾ä»¥è¾¨è®¤, æˆ–è€…ä¸å¤Ÿè‡ªç„¶â€¦</li>
</ol>
<ol>
<li>å¯ä»¥å°è¯•æ”¹åŠ¨ä¸Šé¢ DCGAN çš„æ¶æ„, ä½ å°±èƒ½æ˜ç™½å®ƒå¯¹è¶…å‚æ•°å€¼æœ‰å¤šæ•æ„Ÿäº†~</li>
</ol>
<ol>
<li>DCGANs ä»ä¸å®Œç¾, è‹¥å°è¯•ä½¿ç”¨å®ƒç”Ÿæˆå¾ˆå¤§çš„å›¾åƒ, åˆ™å¯èƒ½å¾—åˆ°å±€éƒ¨çœŸå®çš„ç‰¹å¾, ä½†æ€»ä½“ä¸Šå´æœ‰è¿å’Œæ„Ÿ(æ¯”å¦‚ä¸€ä»¶è¡¬è¡«ä¸¤åªè¢–å­é•¿çŸ­ä¸ä¸€).</li>
</ol>
<h2 id="GANs-çš„æ¸è¿›å¼å¢é•¿"><a href="#GANs-çš„æ¸è¿›å¼å¢é•¿" class="headerlink" title="GANs çš„æ¸è¿›å¼å¢é•¿"></a>GANs çš„æ¸è¿›å¼å¢é•¿</h2><p><strong>ä¸€ç§é‡è¦çš„æŠ€æœ¯</strong>: åœ¨è®­ç»ƒåˆæœŸç”Ÿæˆè¾ƒå°çš„å›¾åƒ, ç„¶åé€æ¸åœ¨ generator å’Œ discriminator ä¸­æ·»åŠ å·ç§¯å±‚ä»¥äº§ç”Ÿè¶Šæ¥è¶Šå¤§çš„å›¾åƒ. è¿™äº›é¢å¤–çš„å±‚å°†æ·»åŠ åœ¨ generator çš„æœ«å°¾ä»¥åŠ discriminator çš„å¼€å¤´, å¹¶ä¸”å…ˆå‰è®­ç»ƒå¥½çš„å±‚ä»æ˜¯å¯è®­ç»ƒçš„.</p>
<hr>
<ol>
<li>ä¸¾ä¾‹è€Œè¨€(å‚è€ƒä¸‹å›¾), åœ¨å°† generator çš„è¾“å‡ºä» 4x4 æ‰©å¤§è‡³ 8x8 æ—¶, åœ¨ç°æœ‰å·ç§¯å±‚ä¸Šæ·»åŠ ä¸€ä¸ªä¸Šé‡‡æ ·å±‚ä»¥è¾“å‡º 8x8 çš„ç‰¹å¾å›¾, å…¶è¾“å‡ºå†ä¼ é€’ç»™æ–°çš„å·ç§¯å±‚, è€Œå…¶è¾“å‡ºå†æ¬¡è¢«ä¼ é€’ç»™ä¸€ä¸ªè¾“å‡ºå·ç§¯å±‚, ä»¥è¾“å‡º 8x8x3 çš„å¸¸è§„å›¾åƒ. </li>
</ol>
<ol>
<li>ä¸ºé¿å…ç ´åç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„æƒé‡, æœ€ç»ˆè¾“å‡ºæ˜¯ &lt;åŸå§‹è¾“å‡ºå±‚&gt; ä¸ &lt;æ–°è¾“å‡ºå±‚(ä»¥è™šçº¿æ¡†æ ‡è¯†)&gt; çš„åŠ æƒå’Œ. æ–°è¾“å‡ºçš„æƒé‡ä¸º $Î±$, è€ŒåŸå§‹è¾“å‡ºçš„æƒé‡ä¸º $1-Î±$, ä¸” $Î±$ ä» 0 æ…¢æ…¢å¢åŠ åˆ° 1. ä¹Ÿå°±æ˜¯è¯´: æ–°è¾“å‡ºå±‚å°†<strong>é€æ¸å–ä»£</strong>åŸå§‹è¾“å‡ºå±‚.</li>
</ol>
<ol>
<li>åœ¨å‘ discriminator ä¸­æ·»åŠ æ–°å·ç§¯å±‚æ—¶, ä¹Ÿä½¿ç”¨ç±»ä¼¼çš„é€æ¸å–ä»£çš„ç­–ç•¥.</li>
</ol>
<p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/Progressive Growing of GANs.PNG" width="80%"></p>
<p>å…¶å®ƒæ—¨åœ¨ <strong>å¢åŠ è¾“å‡ºå¤šæ ·æ€§</strong> &amp; <strong>ä½¿è®­ç»ƒæ›´ç¨³å®š</strong> çš„æŠ€æœ¯:</p>
<ol>
<li><strong>Minibatch standard deviation layer</strong>: é¼“åŠ± generator äº§ç”Ÿæ›´å¤šæ ·åŒ–çš„è¾“å‡º, ä»è€Œé™ä½æ¨¡å¼å´©æºƒçš„é£é™©.</li>
</ol>
<ol>
<li><strong>Equalized learning rate</strong>:  æ—¢å¯åŠ å¿«è®­ç»ƒé€Ÿåº¦åˆå¯æé«˜è®­ç»ƒç¨³å®šæ€§. (å½“ä½¿ç”¨ RMSProp, Adam æˆ–å…¶ä»–<strong>è‡ªé€‚åº”æ¢¯åº¦ä¼˜åŒ–å™¨</strong>æ—¶, è¯¥æŠ€æœ¯èƒ½æ˜¾è‘—æé«˜ GAN çš„æ€§èƒ½)</li>
</ol>
<ol>
<li><strong>Pixelwise normalization layer</strong>: å¯é¿å…ç”± generator å’Œ discriminator é—´çš„è¿‡åº¦ç«äº‰è€Œå¯¼è‡´çš„æ¿€æ´»çˆ†ç‚¸.</li>
</ol>
<h2 id="StyleGANs"><a href="#StyleGANs" class="headerlink" title="StyleGANs"></a>StyleGANs</h2><p>åœ¨ generator ä¸­ä½¿ç”¨<strong>é£æ ¼è¿ç§»</strong>æŠ€æœ¯, ç¡®ä¿ç”Ÿæˆå›¾åƒä¸è®­ç»ƒå›¾åƒ(åœ¨æ‰€æœ‰å°ºåº¦ä¸Š)æœ‰ç›¸åŒçš„å±€éƒ¨ç»“æ„, è¿™æå¤§æå‡äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡; è€Œ discriminator ä¸ loss func åˆ™æ²¡æœ‰ä¿®æ”¹.</p>
<p>StyleGAN æœ‰ä¸¤ä¸ªç¥ç»ç½‘ç»œç»„æˆ:</p>
<ol>
<li><strong>Mapping network</strong>: è¿™æ˜¯ä¸€ä¸ª 8 å±‚çš„ MLP, å°†æ½œåœ¨è¡¨å¾å‘é‡ $z$ æ˜ å°„è‡³å‘é‡ $w$, åè€…å†ç»ä»¿å°„å˜æ¢(ä¸‹å›¾ä¸­æ–¹å—A)äº§ç”Ÿäº†å¤šä¸ªå‘é‡, è¿™äº›å‘é‡æ§åˆ¶äº†ç”Ÿæˆå›¾åƒåœ¨ä¸åŒ level ä¸Šçš„é£æ ¼ (ä»ç²¾ç»†çš„çº¹ç†åˆ°é«˜é˜¶çš„ç‰¹å¾). ç®€è€Œè¨€ä¹‹, <strong>Mapping network å°†æ½œåœ¨è¡¨å¾æ˜ å°„è‡³å¤šä¸ªé£æ ¼å‘é‡</strong>.</li>
</ol>
<ol>
<li><strong>Synthesis network</strong>: è´Ÿè´£ç”Ÿæˆå›¾åƒ. å®ƒåŒ…å«ä¸€ä¸ª (åœ¨è®­ç»ƒå) æ’å®šçš„è¾“å…¥, å¹¶ä½¿ç”¨å¤šä¸ªå·ç§¯å±‚ &amp; ä¸Šé‡‡æ ·å±‚å¤„ç†è¯¥è¾“å…¥. ä¸åŒç‚¹åœ¨äº: (1).ä¸€äº›å™ªå£°è¢«æ·»åŠ åˆ°è¾“å…¥å’Œå·ç§¯å±‚çš„æ‰€æœ‰è¾“å‡ºä¸­; (2).æ¯ä¸ªå™ªå£°å±‚åæœ‰ä¸€ä¸ª<strong>è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–å±‚</strong>, å®ƒç‹¬ç«‹åœ°æ ‡å‡†åŒ–æ¯ä¸ªç‰¹å¾å›¾, ç„¶åä½¿ç”¨é£æ ¼å‘é‡æ¥ç¡®å®šæ¯ä¸ªç‰¹å¾å›¾çš„å°ºåº¦å’Œåç§»é‡.</li>
</ol>
<p><img src="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/StyleGAN.PNG" width="80%"></p>
<p>æœªå®Œå¾…ç»­â€¦</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>è’™ç‰¹å¡æ´› Dropout å®è·µ</title>
    <url>/2022/01/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B-Dropout-%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<p>â“ å¦‚ä½•ä½¿ç”¨ MC(è’™ç‰¹å¡æ´›) Dropout? å®ƒèƒ½å¸¦æ¥ä»€ä¹ˆå¥½å¤„?</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>ğŸ”º é’ˆå¯¹ Fashion MNIST æ•°æ®é›†, å¼€å±•ä¸‹é¢çš„æµ‹è¯•.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å‡†å¤‡æ•°æ®é›† (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist ä¸­æ•°å­—æ ‡ç­¾å¯¹åº”çš„ç±»åˆ«åç§°</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å±•ç¤ºéƒ¨åˆ†è®­ç»ƒé›†å®ä¾‹</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m è¡Œ n åˆ—</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B-Dropout-%E5%AE%9E%E8%B7%B5/output_5_0.png" alt="png"></p>
<h1 id="MC-Dropout"><a href="#MC-Dropout" class="headerlink" title="MC Dropout"></a>MC Dropout</h1><h2 id="é¦–å…ˆæ„å»ºä¸€ä¸ªå¸¸è§„çš„-Dropout-ç½‘ç»œ"><a href="#é¦–å…ˆæ„å»ºä¸€ä¸ªå¸¸è§„çš„-Dropout-ç½‘ç»œ" class="headerlink" title="é¦–å…ˆæ„å»ºä¸€ä¸ªå¸¸è§„çš„ Dropout ç½‘ç»œ"></a>é¦–å…ˆæ„å»ºä¸€ä¸ªå¸¸è§„çš„ Dropout ç½‘ç»œ</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.æ„å»ºæ¨¡å‹</span></span><br><span class="line">model = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model.add(keras.layers.Dropout(<span class="number">0.4</span>))</span><br><span class="line"></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.003</span>, decay=<span class="number">1</span>/(<span class="number">430</span>*<span class="number">4</span>))</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                           optimizer=optimizer, </span><br><span class="line">                           metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, batch_size=<span class="number">32</span>,</span><br><span class="line">                    validation_data=(x_valid, y_valid),</span><br><span class="line">                    callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 9s 5ms/step - loss: 0.6985 - accuracy: 0.7602 - val_loss: 0.4135 - val_accuracy: 0.8502
Epoch 2/100
1719/1719 [==============================] - 7s 4ms/step - loss: 0.4509 - accuracy: 0.8418 - val_loss: 0.3817 - val_accuracy: 0.8614
Epoch 3/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.3922 - accuracy: 0.8609 - val_loss: 0.3501 - val_accuracy: 0.8770
Epoch 4/100
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3519 - accuracy: 0.8756 - val_loss: 0.3257 - val_accuracy: 0.8808
Epoch 5/100
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3251 - accuracy: 0.8841 - val_loss: 0.3115 - val_accuracy: 0.8836
Epoch 6/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.3049 - accuracy: 0.8908 - val_loss: 0.3256 - val_accuracy: 0.8862
Epoch 7/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2877 - accuracy: 0.8952 - val_loss: 0.3103 - val_accuracy: 0.8912
Epoch 8/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2707 - accuracy: 0.9030 - val_loss: 0.2940 - val_accuracy: 0.8970
Epoch 9/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2591 - accuracy: 0.9049 - val_loss: 0.2892 - val_accuracy: 0.8970
Epoch 10/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2474 - accuracy: 0.9085 - val_loss: 0.2985 - val_accuracy: 0.8988
Epoch 11/100
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2389 - accuracy: 0.9119 - val_loss: 0.2984 - val_accuracy: 0.8972
Epoch 12/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2291 - accuracy: 0.9157 - val_loss: 0.2889 - val_accuracy: 0.8994
Epoch 13/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2181 - accuracy: 0.9193 - val_loss: 0.2906 - val_accuracy: 0.9040
Epoch 14/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2133 - accuracy: 0.9212 - val_loss: 0.3011 - val_accuracy: 0.9002
Epoch 15/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2067 - accuracy: 0.9235 - val_loss: 0.3074 - val_accuracy: 0.9012
Epoch 16/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1995 - accuracy: 0.9269 - val_loss: 0.3019 - val_accuracy: 0.9034
Epoch 17/100
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1943 - accuracy: 0.9286 - val_loss: 0.2989 - val_accuracy: 0.9048
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model.evaluate(x_train, y_train))</span><br><span class="line">display(model.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B-Dropout-%E5%AE%9E%E8%B7%B5/output_10_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 4s 3ms/step - loss: 0.1939 - accuracy: 0.9266

[0.19393619894981384, 0.9265636205673218]


313/313 [==============================] - 1s 2ms/step - loss: 0.3195 - accuracy: 0.8905

[0.31950753927230835, 0.890500009059906]
</code></pre><h2 id="å®ç°-MC-Dropout"><a href="#å®ç°-MC-Dropout" class="headerlink" title="å®ç° MC Dropout"></a>å®ç° MC Dropout</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¿€æ´» Dropout å±‚å¹¶è¿›è¡Œ sample æ¬¡é¢„æµ‹, å¹¶å¯¹é¢„æµ‹æ±‚å¹³å‡.</span></span><br><span class="line">y_probas = np.stack([model(x_test, training=<span class="literal">True</span>) <span class="keyword">for</span> sample <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>)])</span><br><span class="line">y_proba = y_probas.mean(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="å¯¹æ¯”å¸¸è§„-Dropout-æ¨¡å‹å’Œ-MC-Dropout-çš„é¢„æµ‹ç»“æœ"><a href="#å¯¹æ¯”å¸¸è§„-Dropout-æ¨¡å‹å’Œ-MC-Dropout-çš„é¢„æµ‹ç»“æœ" class="headerlink" title="å¯¹æ¯”å¸¸è§„ Dropout æ¨¡å‹å’Œ MC Dropout çš„é¢„æµ‹ç»“æœ"></a>å¯¹æ¯”å¸¸è§„ Dropout æ¨¡å‹å’Œ MC Dropout çš„é¢„æµ‹ç»“æœ</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.å¸¸è§„ Dropout æ¨¡å‹çš„é¢„æµ‹ç»“æœ</span></span><br><span class="line">display(np.<span class="built_in">round</span>(model.predict(x_test[:<span class="number">1</span>]), <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;~&quot;</span>*<span class="number">72</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.MC Dropout çš„é¢„æµ‹ç»“æœ</span></span><br><span class="line">display(np.<span class="built_in">round</span>(y_probas[:<span class="number">3</span>, :<span class="number">1</span>], <span class="number">2</span>))    <span class="comment"># å¯¹ç¬¬ä¸€ä¸ªå®ä¾‹çš„å‰ä¸‰æ¬¡é¢„æµ‹</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;~&quot;</span>*<span class="number">72</span>)</span><br><span class="line"></span><br><span class="line">display(np.<span class="built_in">round</span>(y_proba[:<span class="number">1</span>], <span class="number">2</span>))         <span class="comment"># å¯¹ç¬¬ä¸€ä¸ªå®ä¾‹çš„å¹³å‡é¢„æµ‹</span></span><br></pre></td></tr></table></figure>
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],
      dtype=float32)

array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]]],
      dtype=float32)

array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],
      dtype=float32)
</code></pre><h2 id="MC-Dropout-é¢„æµ‹çš„-Accuracy"><a href="#MC-Dropout-é¢„æµ‹çš„-Accuracy" class="headerlink" title="MC Dropout é¢„æµ‹çš„ Accuracy"></a>MC Dropout é¢„æµ‹çš„ Accuracy</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracy = np.<span class="built_in">sum</span>(np.argmax(y_proba, axis=<span class="number">1</span>)==y_test) / <span class="built_in">len</span>(y_test)</span><br><span class="line">accuracy</span><br></pre></td></tr></table></figure>
<pre><code>0.892
</code></pre><p>Remark: ç”±æ­¤å¯è§ MC Dropout é¢„æµ‹çš„ Accuracy(=0.892) è¦<strong>ç•¥é«˜äº</strong>å¸¸è§„ Dropout æ¨¡å‹é¢„æµ‹çš„ Accuracy(=0.8905)</p>
<h2 id="ä¸Šè¿°-MC-Dropout-çš„å®ç°ä»æœ‰é™åˆ¶"><a href="#ä¸Šè¿°-MC-Dropout-çš„å®ç°ä»æœ‰é™åˆ¶" class="headerlink" title="ä¸Šè¿° MC Dropout çš„å®ç°ä»æœ‰é™åˆ¶"></a>ä¸Šè¿° MC Dropout çš„å®ç°ä»æœ‰é™åˆ¶</h2><p>è‹¥ model ä¸­åŒ…å«å…¶ä»– &lt;åœ¨è®­ç»ƒæœŸé—´å’Œæ¨æ–­æœŸé—´è¡Œä¸ºä¸åŒ&gt; çš„å±‚ (å¦‚ BatchNormalization å±‚), åˆ™åº”ä½¿ç”¨ä¸‹é¢çš„ç±»å®ç° MC Dropout.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MCDropout</span>(<span class="params">keras.layers.Dropout</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().call(inputs, training=<span class="literal">True</span>)    <span class="comment"># é‡è½½ call æ–¹æ³•, å¼ºåˆ¶ training å‚æ•°ä¸º True</span></span><br></pre></td></tr></table></figure>
<p>å¯ä»¥ä½¿ç”¨ä¸Šè¿° MCDropout å±‚ä»£æ›¿ Dropout å±‚, é‡æ–°è®­ç»ƒä¸€ä¸ªæ¨¡å‹;<br>æˆ–è€…æ„å»ºä¸€ä¸ªä½¿ç”¨ MCDropout å±‚çš„æ–°æ¨¡å‹, å¹¶å°†åŸæ¨¡å‹çš„å‚æ•°æ‹·è´ç»™å®ƒ, åƒä¸‹é¢ä¸€æ ·:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨åŸ model åŸºç¡€ä¸Šï¼Œå°† Dropout å±‚æ›¿æ¢ä¸º MCDropout å±‚, å¾—åˆ°ä¸€ä¸ªæ–°æ¨¡å‹</span></span><br><span class="line">mc_model = keras.models.Sequential([</span><br><span class="line">    MCDropout(layer.rate) <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, keras.layers.Dropout) <span class="keyword">else</span> layer </span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> model.layers</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.003</span>, decay=<span class="number">1</span>/(<span class="number">430</span>*<span class="number">4</span>))</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, optimizer=optimizer, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># é‡ç”¨åŸ model çš„è¿æ¥æƒé‡</span></span><br><span class="line">mc_model.set_weights(model.get_weights())</span><br></pre></td></tr></table></figure>
<p>æ¥ä¸‹æ¥å°±èƒ½ä½¿ç”¨è¿™ä¸ª MC Dropout æ¨¡å‹äº†.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹ 100 æ¬¡åå–å¹³å‡</span></span><br><span class="line">y_proba_1 = np.mean([mc_model.predict(x_test[:<span class="number">1</span>]) <span class="keyword">for</span> sample <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>)], axis=<span class="number">0</span>)</span><br><span class="line">np.<span class="built_in">round</span>(y_proba_1, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],
      dtype=float32)
</code></pre><h1 id="MC-Alpha-Dropout"><a href="#MC-Alpha-Dropout" class="headerlink" title="MC Alpha Dropout"></a>MC Alpha Dropout</h1><p>ä»¿ç…§ä¸Šé¢çš„æµç¨‹, å®¹æ˜“å®ç° MC Alpha Dropout.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.å®šä¹‰ MCAlphaDropout Class (ç»§æ‰¿ AlphaDropout)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MCAlphaDropout</span>(<span class="params">keras.layers.AlphaDropout</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().call(inputs, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.æ„å»ºè‡ªå½’ä¸€åŒ–ç½‘ç»œ</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ„å»ºæ¨¡å‹</span></span><br><span class="line">model_self_norm = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">400</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    model_self_norm.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line">    model_self_norm.add(MCAlphaDropout(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">model_self_norm.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¼–è¯‘æ¨¡å‹</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.003</span>, decay=<span class="number">1</span>/(<span class="number">430</span>*<span class="number">4</span>))</span><br><span class="line">model_self_norm.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è¾“å…¥ç‰¹å¾æ ‡å‡†åŒ– (Î¼=0, Ïƒ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_self_norm = model_self_norm.fit(x_train_scaled, y_train, epochs=<span class="number">25</span>, batch_size=<span class="number">32</span>,</span><br><span class="line">                                        validation_data=(x_valid_scaled, y_valid),</span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25
1719/1719 [==============================] - 9s 5ms/step - loss: 0.5783 - accuracy: 0.8027 - val_loss: 0.4252 - val_accuracy: 0.8522
Epoch 2/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.3962 - accuracy: 0.8610 - val_loss: 0.3962 - val_accuracy: 0.8630
Epoch 3/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.3413 - accuracy: 0.8787 - val_loss: 0.3716 - val_accuracy: 0.8720
Epoch 4/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2987 - accuracy: 0.8921 - val_loss: 0.3438 - val_accuracy: 0.8820
Epoch 5/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2706 - accuracy: 0.9015 - val_loss: 0.3244 - val_accuracy: 0.8900
Epoch 6/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2451 - accuracy: 0.9107 - val_loss: 0.3190 - val_accuracy: 0.8918
Epoch 7/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2254 - accuracy: 0.9161 - val_loss: 0.3248 - val_accuracy: 0.8894
Epoch 8/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2052 - accuracy: 0.9242 - val_loss: 0.3284 - val_accuracy: 0.8930
Epoch 9/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1889 - accuracy: 0.9298 - val_loss: 0.3465 - val_accuracy: 0.8968
Epoch 10/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1764 - accuracy: 0.9351 - val_loss: 0.3469 - val_accuracy: 0.8968
Epoch 11/25
1719/1719 [==============================] - 8s 5ms/step - loss: 0.1618 - accuracy: 0.9390 - val_loss: 0.3632 - val_accuracy: 0.8988
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_self_norm.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">display(model_self_norm.evaluate(x_train_scaled, y_train))</span><br><span class="line">display(model_self_norm.evaluate(x_test_scaled, y_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B-Dropout-%E5%AE%9E%E8%B7%B5/output_31_0.png" alt="png"></p>
<pre><code>1719/1719 [==============================] - 5s 3ms/step - loss: 0.2152 - accuracy: 0.9211

[0.21519158780574799, 0.9211272597312927]


313/313 [==============================] - 1s 3ms/step - loss: 0.3495 - accuracy: 0.8824

[0.3495039939880371, 0.8823999762535095]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 7.ä½¿ç”¨å¸¦æœ‰ (æ¿€æ´»çš„)MCAlphaDropout å±‚çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹</span></span><br><span class="line">y_proba_2 = np.mean([model_self_norm.predict(x_test_scaled[:<span class="number">1</span>]) <span class="keyword">for</span> sample <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>)], axis=<span class="number">0</span>)</span><br><span class="line">np.<span class="built_in">round</span>(y_proba_2, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],
      dtype=float32)
</code></pre><h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><ol>
<li>æ¨èä½¿ç”¨ MCDropout &amp; MCAlphaDropout ç±»æ¥æ„å»ºæ¨¡å‹, è¿™æ ·æ¨¡å‹å¯å…¼å®¹ä¸€äº›åœ¨è®­ç»ƒæœŸé—´å’Œæ¨æ–­æœŸé—´è¡Œä¸ºä¸åŒçš„å±‚.</li>
<li>MCDropout &amp; MCAlphaDropout æ€æƒ³ä¸Šæ˜¯ä¸€è‡´çš„, åªä¸è¿‡åè€…ä¸“é—¨æœåŠ¡äºè‡ªå½’ä¸€åŒ–ç½‘ç»œ.</li>
<li>MC Dropout æŠ€æœ¯èƒ½å¤Ÿå¸®åŠ©è·å¾—æ›´å¥½çš„é¢„æµ‹, ä¹Ÿå°±æ˜¯è¯´: (1).<strong>é¢„æµ‹å‡†ç¡®ç‡æ›´é«˜</strong> (2).é¢„æµ‹å…·æœ‰<strong>æ›´å¥½çš„ä¸ç¡®å®šæ€§è¯„ä¼°</strong>. </li>
</ol>
<p>ğŸ’ å¯ä»¥æ”¹è¿›çš„ç‚¹: MCDropout ä¸­çš„ sample ä¹Ÿæ˜¯ä¸€ä¸ªè¶…å‚æ•°, æœ¬æ–‡ä¸­æ²¡æœ‰æµ‹è¯•ä¸åŒçš„ sample å€¼å¸¦æ¥çš„å½±å“.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>éšè—å±‚ä¸­ä¸åŒ activation çš„å¯¹æ¯”</title>
    <url>/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>â“ å¯¹æ¯”ä¸åŒçš„ activation + å¯¹åº”çš„åˆå§‹åŒ–ç­–ç•¥, å®ƒä»¬å¦‚ä½•å½±å“æ¢¯åº¦æ¶ˆå¤±é—®é¢˜?</p>
<ol>
<li><p>ReLU / Leaky ReLU / Parametric Leaky ReLU + he initialization</p>
</li>
<li><p>ELU + he initialization</p>
</li>
<li><p>SELU + lecun_normal initialization (è‡ªå½’ä¸€åŒ–ç½‘ç»œ)</p>
<span id="more"></span>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># common imports </span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<p>ğŸ”º ä¸‹é¢é’ˆå¯¹ Fashion MNIST æ•°æ®é›†, æµ‹è¯•ä»¥ä¸‹ä¸‰ç§(ç›¸åŒçš„)ç½‘ç»œæ¶æ„å¯¹è¯¥é—®é¢˜èƒ½è¾¾åˆ°æ€æ ·çš„æ€§èƒ½.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å‡†å¤‡æ•°æ®é›† (train, valid, test)</span></span><br><span class="line">(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train_full = x_train_full / <span class="number">255.</span></span><br><span class="line">x_test = x_test / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_full[:<span class="number">5000</span>], x_train_full[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_valid.shape, y_valid.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape, y_test.shape, sep=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(55000, 28, 28)    (55000,)
(5000, 28, 28)    (5000,)
(10000, 28, 28)    (10000,)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fashion_mnist ä¸­æ•°å­—æ ‡ç­¾å¯¹åº”çš„ç±»åˆ«åç§°</span></span><br><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, </span><br><span class="line">               <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankleboot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># å±•ç¤ºéƒ¨åˆ†è®­ç»ƒé›†å®ä¾‹</span></span><br><span class="line">m, n = <span class="number">2</span>, <span class="number">5</span>    <span class="comment"># m è¡Œ n åˆ—</span></span><br><span class="line">rnd_indices = np.random.randint(low=<span class="number">0</span>, high=x_train.shape[<span class="number">0</span>], size=(m * n, ))</span><br><span class="line">x_sample, y_sample = x_train[rnd_indices], y_train[rnd_indices]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n * <span class="number">1.5</span>, m * <span class="number">1.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        idx = (i - <span class="number">1</span>) * n + j</span><br><span class="line">        plt.subplot(m, n, idx)</span><br><span class="line">        plt.imshow(x_sample[idx - <span class="number">1</span>], cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.title(class_names[y_sample[idx - <span class="number">1</span>]])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_5_0.png" alt="png"></p>
<h1 id="åŸå§‹çš„-Sigmoid"><a href="#åŸå§‹çš„-Sigmoid" class="headerlink" title="åŸå§‹çš„ Sigmoid"></a>åŸå§‹çš„ Sigmoid</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹æ„å»º</span></span><br><span class="line">model_ori = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;sigmoid&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_ori.add(keras.layers.Dense(<span class="number">50</span>, activation=<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_ori.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_ori.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                  optimizer=optimizer, </span><br><span class="line">                  metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è®­ç»ƒ</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_ori = model_ori.fit(x_train, y_train, epochs=<span class="number">10</span>, </span><br><span class="line">                            validation_data=(x_valid, y_valid), </span><br><span class="line">                            callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/10
1719/1719 [==============================] - 12s 6ms/step - loss: 1.9044 - accuracy: 0.1749 - val_loss: 1.7585 - val_accuracy: 0.1914
Epoch 2/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7287 - accuracy: 0.2000 - val_loss: 1.7233 - val_accuracy: 0.1954
Epoch 3/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7402 - accuracy: 0.1976 - val_loss: 1.7459 - val_accuracy: 0.1888
Epoch 4/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7295 - accuracy: 0.1999 - val_loss: 1.7693 - val_accuracy: 0.2064
Epoch 5/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7627 - accuracy: 0.1972 - val_loss: 1.7482 - val_accuracy: 0.1938
Epoch 6/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7409 - accuracy: 0.1965 - val_loss: 1.7335 - val_accuracy: 0.2000
Epoch 7/10
1719/1719 [==============================] - 11s 6ms/step - loss: 1.7254 - accuracy: 0.1985 - val_loss: 1.7321 - val_accuracy: 0.2120
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_ori.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_ori.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_9_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 3ms/step - loss: 1.7235 - accuracy: 0.1993

[1.7234939336776733, 0.19930000603199005]
</code></pre><h1 id="ReLU-åŠå…¶å˜ä½“"><a href="#ReLU-åŠå…¶å˜ä½“" class="headerlink" title="ReLU åŠå…¶å˜ä½“"></a>ReLU åŠå…¶å˜ä½“</h1><h2 id="ReLU-he-initialization"><a href="#ReLU-he-initialization" class="headerlink" title="ReLU + he initialization"></a>ReLU + he initialization</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹æ„å»º</span></span><br><span class="line">model = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;relu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">50</span>, activation=<span class="string">&quot;relu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">              optimizer=optimizer, </span><br><span class="line">              metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è®­ç»ƒ</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                    validation_data=(x_valid, y_valid), </span><br><span class="line">                    callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.5692 - accuracy: 0.7916 - val_loss: 0.4004 - val_accuracy: 0.8618
Epoch 2/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.4166 - accuracy: 0.8488 - val_loss: 0.4237 - val_accuracy: 0.8442
Epoch 3/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3775 - accuracy: 0.8631 - val_loss: 0.3526 - val_accuracy: 0.8750
Epoch 4/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3547 - accuracy: 0.8703 - val_loss: 0.3420 - val_accuracy: 0.8804
Epoch 5/100
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3367 - accuracy: 0.8794 - val_loss: 0.3465 - val_accuracy: 0.8736
Epoch 6/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3201 - accuracy: 0.8849 - val_loss: 0.3395 - val_accuracy: 0.8728
Epoch 7/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3109 - accuracy: 0.8885 - val_loss: 0.3303 - val_accuracy: 0.8844
Epoch 8/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2989 - accuracy: 0.8903 - val_loss: 0.3283 - val_accuracy: 0.8876
Epoch 9/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2898 - accuracy: 0.8934 - val_loss: 0.3251 - val_accuracy: 0.8874
Epoch 10/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2795 - accuracy: 0.8984 - val_loss: 0.3163 - val_accuracy: 0.8900
Epoch 11/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2726 - accuracy: 0.9004 - val_loss: 0.3581 - val_accuracy: 0.8870
Epoch 12/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2654 - accuracy: 0.9012 - val_loss: 0.3145 - val_accuracy: 0.8882
Epoch 13/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2575 - accuracy: 0.9063 - val_loss: 0.3099 - val_accuracy: 0.8906
Epoch 14/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2526 - accuracy: 0.9069 - val_loss: 0.3396 - val_accuracy: 0.8834
Epoch 15/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2417 - accuracy: 0.9106 - val_loss: 0.3241 - val_accuracy: 0.8928
Epoch 16/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2405 - accuracy: 0.9128 - val_loss: 0.3205 - val_accuracy: 0.8910
Epoch 17/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2322 - accuracy: 0.9142 - val_loss: 0.3449 - val_accuracy: 0.8892
Epoch 18/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2311 - accuracy: 0.9148 - val_loss: 0.3124 - val_accuracy: 0.8992
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_15_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 3ms/step - loss: 0.3455 - accuracy: 0.8789

[0.3454982042312622, 0.8788999915122986]
</code></pre><h2 id="Leaky-ReLU-he-initialization"><a href="#Leaky-ReLU-he-initialization" class="headerlink" title="Leaky ReLU + he initialization"></a>Leaky ReLU + he initialization</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹æ„å»º</span></span><br><span class="line">model_leaky = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_leaky.add(keras.layers.Dense(<span class="number">50</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_leaky.add(keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>))</span><br><span class="line"></span><br><span class="line">model_leaky.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_leaky.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                    optimizer=optimizer, </span><br><span class="line">                    metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è®­ç»ƒ</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_leaky = model_leaky.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                validation_data=(x_valid, y_valid), </span><br><span class="line">                                callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 12s 6ms/step - loss: 0.5612 - accuracy: 0.7937 - val_loss: 0.4269 - val_accuracy: 0.8432
Epoch 2/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.4181 - accuracy: 0.8481 - val_loss: 0.4394 - val_accuracy: 0.8424
Epoch 3/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3806 - accuracy: 0.8616 - val_loss: 0.3604 - val_accuracy: 0.8698
Epoch 4/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3558 - accuracy: 0.8713 - val_loss: 0.3439 - val_accuracy: 0.8780
Epoch 5/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3381 - accuracy: 0.8770 - val_loss: 0.3917 - val_accuracy: 0.8592
Epoch 6/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3279 - accuracy: 0.8811 - val_loss: 0.3685 - val_accuracy: 0.8720
Epoch 7/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.3173 - accuracy: 0.8852 - val_loss: 0.3416 - val_accuracy: 0.8824
Epoch 8/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3031 - accuracy: 0.8892 - val_loss: 0.3171 - val_accuracy: 0.8828
Epoch 9/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2932 - accuracy: 0.8927 - val_loss: 0.3086 - val_accuracy: 0.8910
Epoch 10/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.2869 - accuracy: 0.8954 - val_loss: 0.3413 - val_accuracy: 0.8900
Epoch 11/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.2762 - accuracy: 0.8988 - val_loss: 0.3319 - val_accuracy: 0.8876
Epoch 12/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2697 - accuracy: 0.9011 - val_loss: 0.3514 - val_accuracy: 0.8730
Epoch 13/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2663 - accuracy: 0.9026 - val_loss: 0.3334 - val_accuracy: 0.8914
Epoch 14/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2588 - accuracy: 0.9036 - val_loss: 0.3192 - val_accuracy: 0.8882
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_leaky.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_leaky.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_19_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3529 - accuracy: 0.8749

[0.3529128432273865, 0.8748999834060669]
</code></pre><h2 id="Parametric-Leaky-ReLU-he-initialization"><a href="#Parametric-Leaky-ReLU-he-initialization" class="headerlink" title="Parametric Leaky ReLU + he initialization"></a>Parametric Leaky ReLU + he initialization</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹æ„å»º</span></span><br><span class="line">model_leaky_p = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.PReLU(),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.PReLU()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_leaky_p.add(keras.layers.Dense(<span class="number">50</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line">    model_leaky_p.add(keras.layers.PReLU())</span><br><span class="line"></span><br><span class="line">model_leaky_p.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_leaky_p.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                      optimizer=optimizer, </span><br><span class="line">                      metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è®­ç»ƒ</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_leaky_p = model_leaky_p.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                    validation_data=(x_valid, y_valid), </span><br><span class="line">                                    callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 19s 10ms/step - loss: 0.5752 - accuracy: 0.7899 - val_loss: 0.3916 - val_accuracy: 0.8602
Epoch 2/100
1719/1719 [==============================] - 17s 10ms/step - loss: 0.4137 - accuracy: 0.8504 - val_loss: 0.4368 - val_accuracy: 0.8450
Epoch 3/100
1719/1719 [==============================] - 17s 10ms/step - loss: 0.3768 - accuracy: 0.8632 - val_loss: 0.3443 - val_accuracy: 0.8730
Epoch 4/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.3514 - accuracy: 0.8729 - val_loss: 0.3302 - val_accuracy: 0.8812
Epoch 5/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.3315 - accuracy: 0.8801 - val_loss: 0.3350 - val_accuracy: 0.8824
Epoch 6/100
1719/1719 [==============================] - 17s 10ms/step - loss: 0.3168 - accuracy: 0.8851 - val_loss: 0.3439 - val_accuracy: 0.8790
Epoch 7/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.3055 - accuracy: 0.8895 - val_loss: 0.3291 - val_accuracy: 0.8878
Epoch 8/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2954 - accuracy: 0.8927 - val_loss: 0.3318 - val_accuracy: 0.8832
Epoch 9/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2857 - accuracy: 0.8964 - val_loss: 0.3485 - val_accuracy: 0.8844
Epoch 10/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2763 - accuracy: 0.8991 - val_loss: 0.3158 - val_accuracy: 0.8922
Epoch 11/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2679 - accuracy: 0.9031 - val_loss: 0.3350 - val_accuracy: 0.8876
Epoch 12/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2625 - accuracy: 0.9042 - val_loss: 0.3068 - val_accuracy: 0.8946
Epoch 13/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2548 - accuracy: 0.9080 - val_loss: 0.3108 - val_accuracy: 0.8912
Epoch 14/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2463 - accuracy: 0.9086 - val_loss: 0.3020 - val_accuracy: 0.8958
Epoch 15/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2395 - accuracy: 0.9113 - val_loss: 0.3358 - val_accuracy: 0.8926
Epoch 16/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2375 - accuracy: 0.9133 - val_loss: 0.3174 - val_accuracy: 0.8868
Epoch 17/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2308 - accuracy: 0.9155 - val_loss: 0.3335 - val_accuracy: 0.8958
Epoch 18/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2262 - accuracy: 0.9177 - val_loss: 0.3179 - val_accuracy: 0.8938
Epoch 19/100
1719/1719 [==============================] - 18s 10ms/step - loss: 0.2202 - accuracy: 0.9197 - val_loss: 0.3548 - val_accuracy: 0.8902
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_leaky_p.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_leaky_p.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_23_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 2s 5ms/step - loss: 0.3284 - accuracy: 0.8881

[0.3283936679363251, 0.8881000280380249]
</code></pre><h1 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹æ„å»º</span></span><br><span class="line">model_elu = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_elu.add(keras.layers.Dense(<span class="number">50</span>, activation=<span class="string">&quot;elu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_elu.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_elu.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                  optimizer=optimizer, </span><br><span class="line">                  metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è®­ç»ƒ</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_elu = model_elu.fit(x_train, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                            validation_data=(x_valid, y_valid), </span><br><span class="line">                            callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.5414 - accuracy: 0.8028 - val_loss: 0.4075 - val_accuracy: 0.8502
Epoch 2/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.4151 - accuracy: 0.8495 - val_loss: 0.4342 - val_accuracy: 0.8462
Epoch 3/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3814 - accuracy: 0.8612 - val_loss: 0.3812 - val_accuracy: 0.8566
Epoch 4/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3545 - accuracy: 0.8722 - val_loss: 0.3407 - val_accuracy: 0.8804
Epoch 5/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3349 - accuracy: 0.8782 - val_loss: 0.3579 - val_accuracy: 0.8732
Epoch 6/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3205 - accuracy: 0.8838 - val_loss: 0.3499 - val_accuracy: 0.8782
Epoch 7/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3114 - accuracy: 0.8874 - val_loss: 0.3223 - val_accuracy: 0.8888
Epoch 8/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2922 - accuracy: 0.8933 - val_loss: 0.3475 - val_accuracy: 0.8760
Epoch 9/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2860 - accuracy: 0.8956 - val_loss: 0.3181 - val_accuracy: 0.8852
Epoch 10/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2766 - accuracy: 0.8986 - val_loss: 0.3325 - val_accuracy: 0.8852
Epoch 11/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2655 - accuracy: 0.9024 - val_loss: 0.3199 - val_accuracy: 0.8880
Epoch 12/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2593 - accuracy: 0.9039 - val_loss: 0.3135 - val_accuracy: 0.8876
Epoch 13/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2522 - accuracy: 0.9080 - val_loss: 0.3080 - val_accuracy: 0.8890
Epoch 14/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2443 - accuracy: 0.9104 - val_loss: 0.3121 - val_accuracy: 0.8892
Epoch 15/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2380 - accuracy: 0.9121 - val_loss: 0.3497 - val_accuracy: 0.8898
Epoch 16/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2332 - accuracy: 0.9140 - val_loss: 0.3162 - val_accuracy: 0.8920
Epoch 17/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.2257 - accuracy: 0.9153 - val_loss: 0.3139 - val_accuracy: 0.8924
Epoch 18/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.2221 - accuracy: 0.9174 - val_loss: 0.3250 - val_accuracy: 0.8894
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_elu.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_elu.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_27_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3421 - accuracy: 0.8789

[0.3421170711517334, 0.8788999915122986]
</code></pre><h1 id="SELU-æ„å»ºçš„è‡ªå½’ä¸€åŒ–ç½‘ç»œ"><a href="#SELU-æ„å»ºçš„è‡ªå½’ä¸€åŒ–ç½‘ç»œ" class="headerlink" title="SELU æ„å»ºçš„è‡ªå½’ä¸€åŒ–ç½‘ç»œ"></a>SELU æ„å»ºçš„è‡ªå½’ä¸€åŒ–ç½‘ç»œ</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.backend.clear_session()</span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹æ„å»º</span></span><br><span class="line">model_self_norm = keras.models.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=x_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    model_self_norm.add(keras.layers.Dense(<span class="number">50</span>, activation=<span class="string">&quot;selu&quot;</span>, kernel_initializer=<span class="string">&quot;lecun_normal&quot;</span>))</span><br><span class="line"></span><br><span class="line">model_self_norm.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹ç¼–è¯‘</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_self_norm.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">                        optimizer=optimizer, </span><br><span class="line">                        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è¾“å…¥ç‰¹å¾æ ‡å‡†åŒ– (Î¼=0, Ïƒ=1)</span></span><br><span class="line">x_means = x_train.mean(axis=<span class="number">0</span>) </span><br><span class="line">x_stds = x_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_train_scaled = (x_train - x_means) / x_stds </span><br><span class="line">x_valid_scaled = (x_valid - x_means) / x_stds</span><br><span class="line">x_test_scaled = (x_test - x_means) / x_stds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è®­ç»ƒ</span></span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">5</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history_self_norm = model_self_norm.fit(x_train_scaled, y_train, epochs=<span class="number">100</span>, </span><br><span class="line">                                        validation_data=(x_valid_scaled, y_valid), </span><br><span class="line">                                        callbacks=[early_stopping_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.5155 - accuracy: 0.8188 - val_loss: 0.4051 - val_accuracy: 0.8532
Epoch 2/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.4026 - accuracy: 0.8584 - val_loss: 0.4220 - val_accuracy: 0.8562
Epoch 3/100
1719/1719 [==============================] - 12s 7ms/step - loss: 0.3683 - accuracy: 0.8692 - val_loss: 0.3650 - val_accuracy: 0.8694
Epoch 4/100
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3436 - accuracy: 0.8798 - val_loss: 0.3531 - val_accuracy: 0.8732
Epoch 5/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3180 - accuracy: 0.8861 - val_loss: 0.3311 - val_accuracy: 0.8786
Epoch 6/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.3016 - accuracy: 0.8924 - val_loss: 0.3715 - val_accuracy: 0.8780
Epoch 7/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2899 - accuracy: 0.8969 - val_loss: 0.3542 - val_accuracy: 0.8738
Epoch 8/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2741 - accuracy: 0.9012 - val_loss: 0.3532 - val_accuracy: 0.8796
Epoch 9/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2634 - accuracy: 0.9064 - val_loss: 0.3524 - val_accuracy: 0.8822
Epoch 10/100
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2500 - accuracy: 0.9102 - val_loss: 0.3428 - val_accuracy: 0.8910
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç»˜åˆ¶å­¦ä¹ æ›²çº¿</span></span><br><span class="line">pd.DataFrame(history_self_norm.history).plot()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹</span></span><br><span class="line">model_self_norm.evaluate(x_test_scaled, y_test)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/18/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%AD%E4%B8%8D%E5%90%8C-activation-%E7%9A%84%E5%AF%B9%E6%AF%94/output_32_0.png" alt="png"></p>
<pre><code>313/313 [==============================] - 1s 4ms/step - loss: 0.3677 - accuracy: 0.8702

[0.3677000403404236, 0.870199978351593]
</code></pre><h1 id="ç»“è®º"><a href="#ç»“è®º" class="headerlink" title="ç»“è®º"></a>ç»“è®º</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Activation</th>
<th>Training speed</th>
<th>Convergence speed</th>
<th>Evaluation on x_test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid</td>
<td>11s/epoch</td>
<td>2 epochs</td>
<td>accuracy=0.1993</td>
</tr>
<tr>
<td>ReLU</td>
<td>11s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8789</td>
</tr>
<tr>
<td>Leaky ReLU</td>
<td>11s/epoch</td>
<td>9 epochs</td>
<td>accuracy=0.8749</td>
</tr>
<tr>
<td>PReLU</td>
<td>18s/epoch</td>
<td>14 epochs</td>
<td>accuracy=0.8881</td>
</tr>
<tr>
<td>ELU</td>
<td>11s/epoch</td>
<td>13 epochs</td>
<td>accuracy=0.8789</td>
</tr>
<tr>
<td>SELU</td>
<td>11s/epoch</td>
<td>15 epochs</td>
<td>accuracy=0.8702</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><p>ä½¿ç”¨ Sigmoid ä½œä¸ºæ¿€æ´»å‡½æ•°, è®­ç»ƒæ—¶ loss å‡ ä¹ä¸ä¸‹é™, æ¨¡å‹æ— æ³•æ”¶æ•›åˆ°ä¸€ä¸ªæœ‰ä»·å€¼çš„è§£. å…¶åŸå› å¾ˆå¯èƒ½æ˜¯è®­ç»ƒæœŸé—´å‡ºç°äº†<strong>æ¢¯åº¦æ¶ˆå¤±</strong>.</p>
</li>
<li><p>ä½¿ç”¨ ReLU ä½œä¸º(é»˜è®¤)æ¿€æ´»å‡½æ•°æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©, æ¨¡å‹è®­ç»ƒé€Ÿåº¦è¾ƒå¿«, æ”¶æ•›é€Ÿåº¦é€‚ä¸­, æœ€ç»ˆçš„æ¨¡å‹æ€§èƒ½ä¹Ÿå¾ˆä¸é”™.</p>
</li>
<li><p>ä½¿ç”¨ PReLU ä½œä¸ºæ¿€æ´»å‡½æ•°, <strong>è®­ç»ƒé€Ÿåº¦æ˜¾è‘—ä¸‹é™</strong>, <strong>æ”¶æ•›ç¨æ…¢</strong>, ä½†æ¨¡å‹<strong>æ€§èƒ½æ›´å¥½</strong>.</p>
</li>
<li><p>ä½¿ç”¨ SELU ä½œä¸ºæ¿€æ´»å‡½æ•°, æ„å»ºè‡ªå½’ä¸€åŒ–ç½‘ç»œ, <strong>æ”¶æ•›é€Ÿåº¦æœ€æ…¢</strong>, ä¸”æ¨¡å‹<strong>æ€§èƒ½ç¨å·®</strong>.</p>
</li>
<li><p>åœ¨è®­ç»ƒ &lt;ä½¿ç”¨ ReLU æ¿€æ´»çš„&gt; æ¨¡å‹æ—¶, ä¼¼ä¹æ²¡æœ‰å‡ºç°ç¥ç»å…ƒâ€æ­»äº¡â€çš„æƒ…å†µ, å¹¶ä¸”ä½œä¸º ReLU æ”¹è¿›çš„ Leaky ReLU &amp; ELU åœ¨è¡¨ç°ä¸Šå’Œ ReLU ç›¸å·®ä¸å¤š.</p>
</li>
</ol>
<p>ğŸ’ å¾…æ”¹è¿›çš„ç‚¹: å¦‚ä½•å®ç°è§‚å¯Ÿè®­ç»ƒæœŸé—´æ¢¯åº¦å‘é‡çš„å˜åŒ–? ä»¥æ˜ç¡®æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ˜¯å¦å‘ç”Ÿ.</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
</search>
