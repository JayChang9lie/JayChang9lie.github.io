<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Variational Autoencoder:  是一种概率自编码器, 其输出有一定的随机性;  是一种生成式自编码器, 能够生成类似训练集中数据的新实例.">
<meta property="og:type" content="article">
<meta property="og:title" content="变分自编码器 (Variational Autoencoder)">
<meta property="og:url" content="http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/index.html">
<meta property="og:site_name" content="Jay&#39;s Blog">
<meta property="og:description" content="Variational Autoencoder:  是一种概率自编码器, 其输出有一定的随机性;  是一种生成式自编码器, 能够生成类似训练集中数据的新实例.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/variational_autoencoder.PNG">
<meta property="og:image" content="http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_24_0.png">
<meta property="og:image" content="http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_26_0.png">
<meta property="og:image" content="http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_31_0.png">
<meta property="og:image" content="http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_35_0.png">
<meta property="article:published_time" content="2022-02-15T12:40:09.000Z">
<meta property="article:modified_time" content="2023-01-06T22:57:27.267Z">
<meta property="article:author" content="Gozen Sanji">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/variational_autoencoder.PNG">


<link rel="canonical" href="http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/","path":"2022/02/15/变分自编码器-Variational-Autoencoder/","title":"变分自编码器 (Variational Autoencoder)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>变分自编码器 (Variational Autoencoder) | Jay's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Jay's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">🐬</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD-amp-%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.</span> <span class="nav-text">加载 &amp; 划分数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E9%87%87%E6%A0%B7%E5%B1%82"><span class="nav-number">2.</span> <span class="nav-text">自定义采样层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA-encoder"><span class="nav-number">3.</span> <span class="nav-text">构建 encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA-decoder"><span class="nav-number">4.</span> <span class="nav-text">构建 decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%84%E5%90%88-encoder-amp-decoder"><span class="nav-number">5.</span> <span class="nav-text">组合 encoder &amp; decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E8%AF%91"><span class="nav-number">6.</span> <span class="nav-text">编译</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">7.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="nav-number">8.</span> <span class="nav-text">效果展示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%96%B0%E5%AE%9E%E4%BE%8B"><span class="nav-number">9.</span> <span class="nav-text">生成新实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E4%B9%89%E6%8F%92%E5%80%BC"><span class="nav-number">10.</span> <span class="nav-text">语义插值</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Gozen Sanji"
      src="/images/my_avatar.jpg">
  <p class="site-author-name" itemprop="name">Gozen Sanji</p>
  <div class="site-description" itemprop="description">目が覚めた抵抗 型落ちの衝動</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_avatar.jpg">
      <meta itemprop="name" content="Gozen Sanji">
      <meta itemprop="description" content="目が覚めた抵抗 型落ちの衝動">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jay's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          变分自编码器 (Variational Autoencoder)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-02-15 20:40:09" itemprop="dateCreated datePublished" datetime="2022-02-15T20:40:09+08:00">2022-02-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-01-07 06:57:27" itemprop="dateModified" datetime="2023-01-07T06:57:27+08:00">2023-01-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><strong>Variational Autoencoder</strong>:</p>
<ol>
<li><p>是一种概率自编码器, 其输出有一定的随机性;</p>
</li>
<li><p>是一种生成式自编码器, 能够生成类似训练集中数据的新实例.</p>
<span id="more"></span>
</li>
</ol>
<hr>
<p>与常规自编码器不同, 变分自编码器不直接对给定 input 进行编码, 而是使 encoder 生成均值 $μ$ 和标准差 $σ$, 然后从对应高斯分布 $N(μ, σ^2)$ 中随机采样得到 input 对应的编码, 然后 decoder 再对此编码进行解码, 就得到与训练实例类似的数据.</p>
<hr>
<p>变分自编码器的 cost func 由两部分组成: </p>
<ol>
<li>常规的重构损失(可用 cross entropy 表示) </li>
<li>latent loss(由高斯分布与实际分布之间的 $KL$散度 表示)</li>
</ol>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/variational_autoencoder.PNG" width="70%"></p>
<p>下面基于 Fashion MNIST 构建一个变分自编码器.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure>
<h2 id="加载-amp-划分数据集"><a href="#加载-amp-划分数据集" class="headerlink" title="加载 &amp; 划分数据集"></a>加载 &amp; 划分数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line">X_train_full = X_train_full.astype(np.float32) / <span class="number">255</span></span><br><span class="line">X_test = X_test.astype(np.float32) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">X_train, X_valid = X_train_full[:-<span class="number">5000</span>], X_train_full[-<span class="number">5000</span>:]</span><br><span class="line">y_train, y_valid = y_train_full[:-<span class="number">5000</span>], y_train_full[-<span class="number">5000</span>:]</span><br></pre></td></tr></table></figure>
<h2 id="自定义采样层"><a href="#自定义采样层" class="headerlink" title="自定义采样层"></a>自定义采样层</h2><p>给定参数 $μ, σ$, 下面的层对 codings 采样.<br>[下面假定 latent loss 是基于 $γ = log(σ^2)$ 的 $KL$ 散度]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">K = keras.backend</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sampling</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        mean, log_var = inputs</span><br><span class="line">        <span class="keyword">return</span> K.random_normal(tf.shape(log_var)) * K.exp(log_var/<span class="number">2</span>) + mean</span><br></pre></td></tr></table></figure>
<p>有了这个层, 就可以从高斯分布 $N(μ, σ^2)$ 中对 codings 向量采样了.</p>
<h2 id="构建-encoder"><a href="#构建-encoder" class="headerlink" title="构建 encoder"></a>构建 encoder</h2><p>codings_size, dense layer 中的 neurons 和 activation 都可以修改.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">codings_size = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">inputs = keras.layers.Input(shape=[<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">z = keras.layers.Flatten()(inputs)</span><br><span class="line">z = keras.layers.Dense(<span class="number">14</span> * <span class="number">14</span>, activation=<span class="string">&quot;elu&quot;</span>)(z)</span><br><span class="line">z = keras.layers.Dense(<span class="number">7</span> * <span class="number">7</span>, activation=<span class="string">&quot;elu&quot;</span>)(z)</span><br><span class="line"><span class="comment"># 计算 μ &amp; γ</span></span><br><span class="line">codings_mean = keras.layers.Dense(codings_size)(z)</span><br><span class="line">codings_log_var = keras.layers.Dense(codings_size)(z)</span><br><span class="line"><span class="comment"># 对编码采样</span></span><br><span class="line">codings = Sampling()([codings_mean, codings_log_var])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合为 encoder</span></span><br><span class="line">variational_encoder = keras.models.Model(</span><br><span class="line">    inputs=[inputs], </span><br><span class="line">    outputs=[codings_mean, codings_log_var, codings]    <span class="comment"># 前两个输出仅作检查用</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="构建-decoder"><a href="#构建-decoder" class="headerlink" title="构建 decoder"></a>构建 decoder</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">decoder_inputs = keras.layers.Input(shape=[codings_size])    <span class="comment"># 考虑一下为什么是这个 shape ~</span></span><br><span class="line"><span class="comment"># 关于编码层对称的两个 Dense 层</span></span><br><span class="line">x = keras.layers.Dense(<span class="number">7</span> * <span class="number">7</span>, activation=<span class="string">&quot;elu&quot;</span>)(decoder_inputs)</span><br><span class="line">x = keras.layers.Dense(<span class="number">14</span> * <span class="number">14</span>, activation=<span class="string">&quot;elu&quot;</span>)(x)</span><br><span class="line"><span class="comment"># 还原输入的神经元数</span></span><br><span class="line"><span class="comment"># sigmoid 激活将每个像素值看作分类任务 (对应编译时的重构损失)</span></span><br><span class="line">x = keras.layers.Dense(<span class="number">28</span> * <span class="number">28</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)(x)</span><br><span class="line">outputs = keras.layers.Reshape([<span class="number">28</span>, <span class="number">28</span>])(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合为 decoder</span></span><br><span class="line">variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])</span><br></pre></td></tr></table></figure>
<h2 id="组合-encoder-amp-decoder"><a href="#组合-encoder-amp-decoder" class="headerlink" title="组合 encoder &amp; decoder"></a>组合 encoder &amp; decoder</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, _, codings = variational_encoder(inputs)</span><br><span class="line">reconstructions = variational_decoder(codings)</span><br><span class="line"></span><br><span class="line">variational_autoencoder = keras.models.Model(inputs=[inputs], outputs=[reconstructions])</span><br></pre></td></tr></table></figure>
<h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p>这里的 loss 分为 latent loss 和 reconstruction loss 两部分.<br>[另外, 不妨试试其他优化器~]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义的 metrics</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rounded_accuracy</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> keras.metrics.binary_accuracy(tf.<span class="built_in">round</span>(y_true), tf.<span class="built_in">round</span>(y_pred))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算 loss</span></span><br><span class="line">latent_loss = -<span class="number">0.5</span> * K.<span class="built_in">sum</span>(</span><br><span class="line">    <span class="number">1</span> + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean), </span><br><span class="line">    axis=-<span class="number">1</span>)</span><br><span class="line">variational_autoencoder.add_loss(K.mean(latent_loss) / <span class="number">784.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译</span></span><br><span class="line">optimizer = keras.optimizers.RMSprop(learning_rate=<span class="number">0.003</span>)</span><br><span class="line"></span><br><span class="line">variational_autoencoder.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, </span><br><span class="line">                                optimizer=optimizer, </span><br><span class="line">                                metrics=[rounded_accuracy])</span><br></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">performance_lr_scheduler = keras.callbacks.ReduceLROnPlateau(patience=<span class="number">7</span>, factor=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">history = variational_autoencoder.fit(X_train, X_train, </span><br><span class="line">                                      epochs=<span class="number">100</span>, batch_size=<span class="number">64</span>, </span><br><span class="line">                                      validation_data=(X_valid, X_valid), </span><br><span class="line">                                      callbacks=[early_stopping_cb, performance_lr_scheduler])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3663 - rounded_accuracy: 0.8768 - val_loss: 0.3566 - val_rounded_accuracy: 0.8752 - lr: 0.0030
Epoch 2/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3336 - rounded_accuracy: 0.9013 - val_loss: 0.3351 - val_rounded_accuracy: 0.9037 - lr: 0.0030
Epoch 3/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3275 - rounded_accuracy: 0.9065 - val_loss: 0.3298 - val_rounded_accuracy: 0.9024 - lr: 0.0030
Epoch 4/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3245 - rounded_accuracy: 0.9089 - val_loss: 0.3288 - val_rounded_accuracy: 0.9057 - lr: 0.0030
Epoch 5/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3223 - rounded_accuracy: 0.9108 - val_loss: 0.3267 - val_rounded_accuracy: 0.9102 - lr: 0.0030
Epoch 6/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3208 - rounded_accuracy: 0.9121 - val_loss: 0.3250 - val_rounded_accuracy: 0.9070 - lr: 0.0030
Epoch 7/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3196 - rounded_accuracy: 0.9130 - val_loss: 0.3237 - val_rounded_accuracy: 0.9138 - lr: 0.0030
Epoch 8/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3188 - rounded_accuracy: 0.9138 - val_loss: 0.3233 - val_rounded_accuracy: 0.9115 - lr: 0.0030
Epoch 9/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3181 - rounded_accuracy: 0.9143 - val_loss: 0.3259 - val_rounded_accuracy: 0.9118 - lr: 0.0030
Epoch 10/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3175 - rounded_accuracy: 0.9148 - val_loss: 0.3270 - val_rounded_accuracy: 0.9098 - lr: 0.0030
Epoch 11/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3171 - rounded_accuracy: 0.9152 - val_loss: 0.3200 - val_rounded_accuracy: 0.9167 - lr: 0.0030
Epoch 12/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3167 - rounded_accuracy: 0.9155 - val_loss: 0.3205 - val_rounded_accuracy: 0.9138 - lr: 0.0030
Epoch 13/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3164 - rounded_accuracy: 0.9157 - val_loss: 0.3219 - val_rounded_accuracy: 0.9112 - lr: 0.0030
Epoch 14/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3160 - rounded_accuracy: 0.9160 - val_loss: 0.3242 - val_rounded_accuracy: 0.9137 - lr: 0.0030
Epoch 15/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3159 - rounded_accuracy: 0.9161 - val_loss: 0.3160 - val_rounded_accuracy: 0.9178 - lr: 0.0030
Epoch 16/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3156 - rounded_accuracy: 0.9162 - val_loss: 0.3217 - val_rounded_accuracy: 0.9097 - lr: 0.0030
Epoch 17/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3154 - rounded_accuracy: 0.9164 - val_loss: 0.3207 - val_rounded_accuracy: 0.9105 - lr: 0.0030
Epoch 18/100
860/860 [==============================] - 9s 10ms/step - loss: 0.3154 - rounded_accuracy: 0.9164 - val_loss: 0.3236 - val_rounded_accuracy: 0.9114 - lr: 0.0030
Epoch 19/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3152 - rounded_accuracy: 0.9168 - val_loss: 0.3173 - val_rounded_accuracy: 0.9166 - lr: 0.0030
Epoch 20/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3150 - rounded_accuracy: 0.9168 - val_loss: 0.3196 - val_rounded_accuracy: 0.9127 - lr: 0.0030
Epoch 21/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3149 - rounded_accuracy: 0.9169 - val_loss: 0.3235 - val_rounded_accuracy: 0.9120 - lr: 0.0030
Epoch 22/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3147 - rounded_accuracy: 0.9170 - val_loss: 0.3213 - val_rounded_accuracy: 0.9144 - lr: 0.0030
Epoch 23/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3071 - rounded_accuracy: 0.9248 - val_loss: 0.3098 - val_rounded_accuracy: 0.9238 - lr: 6.0000e-04
Epoch 24/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3066 - rounded_accuracy: 0.9253 - val_loss: 0.3093 - val_rounded_accuracy: 0.9240 - lr: 6.0000e-04
Epoch 25/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3064 - rounded_accuracy: 0.9255 - val_loss: 0.3095 - val_rounded_accuracy: 0.9232 - lr: 6.0000e-04
Epoch 26/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3062 - rounded_accuracy: 0.9257 - val_loss: 0.3093 - val_rounded_accuracy: 0.9239 - lr: 6.0000e-04
Epoch 27/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3061 - rounded_accuracy: 0.9258 - val_loss: 0.3088 - val_rounded_accuracy: 0.9242 - lr: 6.0000e-04
Epoch 28/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3060 - rounded_accuracy: 0.9259 - val_loss: 0.3090 - val_rounded_accuracy: 0.9245 - lr: 6.0000e-04
Epoch 29/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3060 - rounded_accuracy: 0.9260 - val_loss: 0.3090 - val_rounded_accuracy: 0.9247 - lr: 6.0000e-04
Epoch 30/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3059 - rounded_accuracy: 0.9260 - val_loss: 0.3088 - val_rounded_accuracy: 0.9243 - lr: 6.0000e-04
Epoch 31/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3058 - rounded_accuracy: 0.9261 - val_loss: 0.3085 - val_rounded_accuracy: 0.9249 - lr: 6.0000e-04
Epoch 32/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3057 - rounded_accuracy: 0.9262 - val_loss: 0.3085 - val_rounded_accuracy: 0.9247 - lr: 6.0000e-04
Epoch 33/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3057 - rounded_accuracy: 0.9262 - val_loss: 0.3088 - val_rounded_accuracy: 0.9246 - lr: 6.0000e-04
Epoch 34/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3056 - rounded_accuracy: 0.9263 - val_loss: 0.3083 - val_rounded_accuracy: 0.9247 - lr: 6.0000e-04
Epoch 35/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3056 - rounded_accuracy: 0.9264 - val_loss: 0.3086 - val_rounded_accuracy: 0.9249 - lr: 6.0000e-04
Epoch 36/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3055 - rounded_accuracy: 0.9265 - val_loss: 0.3086 - val_rounded_accuracy: 0.9252 - lr: 6.0000e-04
Epoch 37/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3054 - rounded_accuracy: 0.9265 - val_loss: 0.3086 - val_rounded_accuracy: 0.9242 - lr: 6.0000e-04
Epoch 38/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3054 - rounded_accuracy: 0.9266 - val_loss: 0.3085 - val_rounded_accuracy: 0.9250 - lr: 6.0000e-04
Epoch 39/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3054 - rounded_accuracy: 0.9266 - val_loss: 0.3085 - val_rounded_accuracy: 0.9250 - lr: 6.0000e-04
Epoch 40/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3053 - rounded_accuracy: 0.9266 - val_loss: 0.3080 - val_rounded_accuracy: 0.9255 - lr: 6.0000e-04
Epoch 41/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3053 - rounded_accuracy: 0.9267 - val_loss: 0.3080 - val_rounded_accuracy: 0.9253 - lr: 6.0000e-04
Epoch 42/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3052 - rounded_accuracy: 0.9267 - val_loss: 0.3084 - val_rounded_accuracy: 0.9251 - lr: 6.0000e-04
Epoch 43/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3052 - rounded_accuracy: 0.9268 - val_loss: 0.3082 - val_rounded_accuracy: 0.9252 - lr: 6.0000e-04
Epoch 44/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3052 - rounded_accuracy: 0.9269 - val_loss: 0.3081 - val_rounded_accuracy: 0.9257 - lr: 6.0000e-04
Epoch 45/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3051 - rounded_accuracy: 0.9268 - val_loss: 0.3084 - val_rounded_accuracy: 0.9248 - lr: 6.0000e-04
Epoch 46/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3051 - rounded_accuracy: 0.9270 - val_loss: 0.3080 - val_rounded_accuracy: 0.9252 - lr: 6.0000e-04
Epoch 47/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3050 - rounded_accuracy: 0.9270 - val_loss: 0.3080 - val_rounded_accuracy: 0.9257 - lr: 6.0000e-04
Epoch 48/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9280 - val_loss: 0.3068 - val_rounded_accuracy: 0.9270 - lr: 1.2000e-04
Epoch 49/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9281 - val_loss: 0.3069 - val_rounded_accuracy: 0.9266 - lr: 1.2000e-04
Epoch 50/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9280 - val_loss: 0.3067 - val_rounded_accuracy: 0.9268 - lr: 1.2000e-04
Epoch 51/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3041 - rounded_accuracy: 0.9280 - val_loss: 0.3069 - val_rounded_accuracy: 0.9267 - lr: 1.2000e-04
Epoch 52/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9280 - val_loss: 0.3068 - val_rounded_accuracy: 0.9267 - lr: 1.2000e-04
Epoch 53/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3040 - rounded_accuracy: 0.9282 - val_loss: 0.3068 - val_rounded_accuracy: 0.9270 - lr: 1.2000e-04
Epoch 54/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3067 - val_rounded_accuracy: 0.9269 - lr: 1.2000e-04
Epoch 55/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3068 - val_rounded_accuracy: 0.9266 - lr: 1.2000e-04
Epoch 56/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3068 - val_rounded_accuracy: 0.9268 - lr: 1.2000e-04
Epoch 57/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3040 - rounded_accuracy: 0.9281 - val_loss: 0.3069 - val_rounded_accuracy: 0.9266 - lr: 1.2000e-04
Epoch 58/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9269 - lr: 2.4000e-05
Epoch 59/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3065 - val_rounded_accuracy: 0.9271 - lr: 2.4000e-05
Epoch 60/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9270 - lr: 2.4000e-05
Epoch 61/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9269 - lr: 2.4000e-05
Epoch 62/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9268 - lr: 2.4000e-05
Epoch 63/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9268 - lr: 2.4000e-05
Epoch 64/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9268 - lr: 2.4000e-05
Epoch 65/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3038 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 2.4000e-05
Epoch 66/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9270 - lr: 4.8000e-06
Epoch 67/100
860/860 [==============================] - 10s 12ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3064 - val_rounded_accuracy: 0.9270 - lr: 4.8000e-06
Epoch 68/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3064 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 69/100
860/860 [==============================] - 9s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3064 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 70/100
860/860 [==============================] - 10s 11ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 4.8000e-06
Epoch 71/100
860/860 [==============================] - 11s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 4.8000e-06
Epoch 72/100
860/860 [==============================] - 12s 13ms/step - loss: 0.3038 - rounded_accuracy: 0.9283 - val_loss: 0.3066 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 73/100
860/860 [==============================] - 12s 14ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3065 - val_rounded_accuracy: 0.9272 - lr: 4.8000e-06
Epoch 74/100
860/860 [==============================] - 11s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9273 - lr: 4.8000e-06
Epoch 75/100
860/860 [==============================] - 11s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3066 - val_rounded_accuracy: 0.9269 - lr: 9.6000e-07
Epoch 76/100
860/860 [==============================] - 12s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3066 - val_rounded_accuracy: 0.9271 - lr: 9.6000e-07
Epoch 77/100
860/860 [==============================] - 12s 14ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9272 - lr: 9.6000e-07
Epoch 78/100
860/860 [==============================] - 12s 13ms/step - loss: 0.3037 - rounded_accuracy: 0.9284 - val_loss: 0.3065 - val_rounded_accuracy: 0.9273 - lr: 9.6000e-07
Epoch 79/100
860/860 [==============================] - 12s 14ms/step - loss: 0.3037 - rounded_accuracy: 0.9285 - val_loss: 0.3065 - val_rounded_accuracy: 0.9272 - lr: 9.6000e-07
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">variational_autoencoder.evaluate(X_valid, X_valid)</span><br></pre></td></tr></table></figure>
<pre><code>157/157 [==============================] - 1s 5ms/step - loss: 0.3066 - rounded_accuracy: 0.9273

[0.306575208902359, 0.9272765517234802]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(history.history).plot(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_24_0.png" alt="png"></p>
<h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span>(<span class="params">image</span>):</span></span><br><span class="line">    plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_reconstructions</span>(<span class="params">model, images=X_valid, n_images=<span class="number">12</span></span>):</span></span><br><span class="line">    <span class="comment"># 使用自编码器获得重构图像</span></span><br><span class="line">    reconstructions = model.predict(images[:n_images])</span><br><span class="line">    <span class="comment"># 绘制 &lt;原图像&gt; 与 &lt;重构图像&gt; の 对比</span></span><br><span class="line">    fig = plt.figure(figsize=(n_images * <span class="number">1.5</span>, <span class="number">3</span>))</span><br><span class="line">    <span class="keyword">for</span> image_index <span class="keyword">in</span> <span class="built_in">range</span>(n_images):</span><br><span class="line">        plt.subplot(<span class="number">2</span>, n_images, <span class="number">1</span> + image_index)</span><br><span class="line">        plot_image(images[image_index])</span><br><span class="line">        plt.subplot(<span class="number">2</span>, n_images, <span class="number">1</span> + n_images + image_index)</span><br><span class="line">        plot_image(reconstructions[image_index])</span><br><span class="line"></span><br><span class="line">show_reconstructions(variational_autoencoder)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_26_0.png" alt="png"></p>
<h2 id="生成新实例"><a href="#生成新实例" class="headerlink" title="生成新实例"></a>生成新实例</h2><p>下面使用刚才训练好的变分自编码器, 尝试生成一些新实例.</p>
<p>只需要从(对应)高斯分布中随机采样 codings 向量, 再将它们解码即可.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机采样 12 个 codings 向量并解码</span></span><br><span class="line">codings = tf.random.normal(shape=[<span class="number">12</span>, codings_size])</span><br><span class="line">images = variational_decoder(codings).numpy()</span><br></pre></td></tr></table></figure>
<p>展示生成的新实例.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_multiple_images</span>(<span class="params">images, n_cols=<span class="literal">None</span></span>):</span></span><br><span class="line">    n_cols = n_cols <span class="keyword">or</span> <span class="built_in">len</span>(images)</span><br><span class="line">    n_rows = (<span class="built_in">len</span>(images) - <span class="number">1</span>) // n_cols + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> images.shape[-<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 对灰度图像, 压缩 channels 维度</span></span><br><span class="line">        images = np.squeeze(images, axis=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    plt.figure(figsize=(<span class="number">2</span> * n_cols, <span class="number">2</span> * n_rows))</span><br><span class="line">    <span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        plt.subplot(n_rows, n_cols, index + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">plot_multiple_images(images, <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_31_0.png" alt="png"></p>
<p>可以看出生成的新实例中多数确实形如 Fashion MNIST 中的图像, 但是它们还是比较模糊的.</p>
<h2 id="语义插值"><a href="#语义插值" class="headerlink" title="语义插值"></a>语义插值</h2><p>依靠变分自编码器, 可<strong>在 codings 级别进行插值</strong>, 而不是在像素级别进行插值(这导致结果看起来像两张图象重叠在一起).</p>
<p>只需让两张图像通过 encoder, 对所得 codings 进行插值, 再将结果通过 decoder 即可得到最终的插值图像.</p>
<p>下面通过实践感受一下语义插值的效果.(其中的 codings 是[9]中生成的新实例的 codings)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.对 codings 进行插值后解码</span></span><br><span class="line">codings_grid = tf.reshape(codings, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, codings_size])</span><br><span class="line"><span class="comment"># 这里 tf.image.resize() 默认执行双线性插值</span></span><br><span class="line">larger_grid = tf.image.resize(codings_grid, size=[<span class="number">5</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">interpolated_codings = tf.reshape(larger_grid, [-<span class="number">1</span>, codings_size])</span><br><span class="line">images = variational_decoder(interpolated_codings).numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.展示解码后的图像</span></span><br><span class="line">plt.figure(figsize=(<span class="number">7</span> * <span class="number">1.5</span>, <span class="number">5</span> * <span class="number">1.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">7</span>, index + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> index%<span class="number">7</span>%<span class="number">2</span>==<span class="number">0</span> <span class="keyword">and</span> index//<span class="number">7</span>%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        plt.gca().get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        plt.gca().get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(image, cmap=<span class="string">&quot;binary&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/02/15/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-Variational-Autoencoder/output_35_0.png" alt="png"></p>
<p>观察第1行, 第2列的鞋子图像, 它很自然地过渡于其左右的两张鞋子的图像之间~</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/02/05/%E6%80%BB%E7%BB%93-TensorFlow-%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84-resize-%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B9%E6%B3%95/" rel="prev" title="总结 TensorFlow 中常用的 resize 图像的方法">
                  <i class="fa fa-chevron-left"></i> 总结 TensorFlow 中常用的 resize 图像的方法
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/02/16/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-Adversarial-Networks/" rel="next" title="生成对抗网络 (Generative Adversarial Networks)">
                  生成对抗网络 (Generative Adversarial Networks) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Gozen Sanji</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
